Exploitation techniques to abuse metadata of heap allocators have been widely studied because of their generality (i.e., application independence) and powerfulness (i.e., bypassing modern mitigation).
However, such techniques are commonly considered arts, and thus the ways to discover them remain ad-hoc, manual, and allocator-specific.
In this paper, we present an automatic tool, ARCHEAP, to systematically discover the unexplored heap exploitation primitives, regardless of their underlying implementations.
The key idea of ARCHEAP is to let the computer autonomously explore the spaces, similar in concept to fuzzing, by specifying a set of common designs of modern heap allo-cators and root causes of vulnerabilities as models, and by providing heap operations and attack capabilities as actions.
During the exploration, ARCHEAP checks whether the combinations of these actions can be potentially used to construct exploitation primitives, such as arbitrary write or overlapped chunks.
As a proof, ARCHEAP generates working PoC that demonstrates the discovered exploitation technique.
We evaluated ARCHEAP with ptmalloc2 and 10 other allo-cators, and discovered five previously unknown exploitation techniques in ptmalloc2 as well as several techniques against seven out of 10 allocators including the security-focused allo-cator, DieHarder.
To show the effectiveness of ARCHEAP's approach in other domains, we also studied how security features and exploit primitives evolve across different versions of ptmalloc2.
Heap-related vulnerabilities have been the most common, yet critical source of security problems in systems software [42,64,65,71].
According to Microsoft, heap vulnerabilities accounted for 53% of security problems in their products in 2017 [48].
One way to exploit these vulnerabilities is to use heap exploitation techniques [61], which abuse underlying allocators.
There are two properties that make these techniques preferable for attacks.
First, heap exploitation techniques tend to be application-independent, making it possible to write exploit without a deep understanding of Scriptable: Software accepting a script language (e.g., web browsers or PDF readers).
Table 1: The number of exploitations that lead to code execution from heap vulnerabilities in exploit-db [50].
A heap exploit technique is one of the popular methods used to compromise nonscriptable programs-bugs in scriptable programs typically allow much easier, simpler way for exploitation, requiring no use of the heap exploitation technique.application internals.
Second, heap vulnerabilities are typically so powerful that attackers can bypass modern mitigation schemes by abusing them.
For example, a seemingly benign bug that overwrites one NULL byte to the metadata of ptmalloc2 leads to a privilege escalation on Chrome OS [2].
Heap exploitation techniques have steadily been used in real-world exploits.
To show that, we collected successful exploits for heap vulnerabilities leading to arbitrary code execution from the well-known exploit database, exploit-db [50].
As shown in Table 1, heap exploitation techniques were one of the favorable ways to compromise software when ASLR was not implemented (24 / 52 exploits).
Even after ASLR is deployed, heap bugs in non-scriptable programs are frequently exploited via heap exploitation techniques (10 / 21 exploits).
Not to mention, popular software such as the Exim mail server [47], WhatsApp [6] and VMware ESXi [77] are all hijacked via the heap exploitation technique in 2019.
Note that scriptable programs provide much simpler, flexible exploitation techniques, so using heap exploitation techniques is not yet preferred by an attacker: e.g., corrupting an array-like structure to achieve arbitrary reads and writes.Communities have been studying possible attack techniques against heap vulnerabilities (see, Table 2), but finding such techniques is often considered an art, and thus the approaches used to discover them remain ad-hoc, manual and allocator-specific at best.
Unfortunately, such a trend makes it hard for communities to understand the security implications of various heap allocators (or even across different versions).
For example, when tcache was recently introduced in ptmalloc2 to improve the performance with a per-thread cache, its security was improperly evaluated (i.e., insufficient integrity checks for allocation or free [17,37]), enabling an easier way for exploitation.
Moreover, existing studies for heap exploitation techniques are highly biased; only ptmalloc2 is exhaustively considered (e.g., missing DieHarder [49]).
In this paper, we present an automatic tool, ARCHEAP, to systematically discover the unexplored heap exploitation primitives, regardless of their underlying implementations.
The key idea of ARCHEAP is to let the computer autonomously explore the spaces, similar in concept to fuzzing, which is proven to be practical and effective in discovering software bugs [29,75].
However, it is non-trivial to apply classical fuzzing techniques to discover new heap exploitation primitives for three reasons.
First, to successfully trigger a heap vulnerability, it must generate a particular sequence of steps with exact data, quickly rendering the problem intractable using fuzzing approaches.
Accordingly, researchers attempt to tackle this problem using symbolic execution instead, but stumbled over the well-known state explosion problem, thereby limiting its scope to validating known exploitation techniques [17].
Second, we need to devise a fast way to estimate the possibility of heap exploitation, as fuzzing requires clear signals, such as segmentation faults, to recognize interesting test cases.
Third, the test cases generated by fuzzers are typically redundant and obscure, so users are required to spend non-negligible time and effort analyzing the final results.The key intuition to overcome these challenges (i.e., reducing search space) is to abstract the internals of heap allocators and the root causes of heap vulnerabilities (see §3.1).
In particular, we observed that modern heap allocators share three common design components, namely, binning, in-place metadata, and cardinal data.
On top of these models, we directed ARCHEAP to mutate and synthesize heap operations and attack capabilities.
During the exploration, ARCHEAP checks whether the generated test case can be potentially used to construct exploitation primitives, such as arbitrary writes or overlapped chunks-we devised shadow-memorybased detection for efficient evaluation (see, §5.3).
Whenever ARCHEAP finds a new exploit primitive, it generates a working PoC code using delta-debugging [76] to reduce redundant test cases to a minimal, equivalent class.
We evaluated ARCHEAP with ptmalloc2 and 10 other allocators.
As a result, we discovered five new exploit techniques against Linux's default heap allocator, ptmalloc2.
ARCHEAP's approach can be extended beyond ptmalloc2; ARCHEAP found several exploit primitives against other popular heap allocators, such as tcmalloc and jemalloc.
Moreover, by disclosing unexpected exploit primitives, ARCHEAP identified three implementation bugs in DieHarder, Mesh [56], and mimalloc, respectively.The closest related work to ARCHEAP is HeapHopper [17], which verifies existing heap exploit techniques using symbolic execution.
Compared with HeapHopper, ARCHEAP outperforms it in finding new techniques; none of the new techniques from ARCHEAP are found by HeapHopper.
Moreover, unlike HeapHopper, ARCHEAP is independent on exploit-specific information, which is unavailable in finding new techniques; HeapHopper found only three out of eight known techniques in ptmalloc2 without the prior knowledge, while ARCHEAP found all eight.
This shows that HeapHopper is ineffective for this new task (i.e., finding new exploit techniques), justifying the need for this new tool.To show the effectiveness of the ARCHEAP's approach in other domains, we also studied how exploit primitives evolve across different versions of ptmalloc2, demonstrating the need for an automated method to evaluate the security of heap allocators.
To foster further research, we open-source ARCHEAP at https://github.com/sslab-gatech/ArcHeap.In summary, we make the following contributions:• We show that heap allocators share common designs, and we devise an efficient method to evaluate exploitation techniques using shadow memory.
• We design, implement, and evaluate our prototype, ARCHEAP, a tool that automatically discovers heap exploitation techniques.
against various allocators.
• ARCHEAP found five new techniques in ptmalloc2 and several techniques in various allocators, including tcmalloc, jemalloc, and DieHarder, and it outperforms a state-of-the-art tool, HeapHopper, in finding new exploitation techniques.
Dynamic memory allocation [41] plays an essential role in managing a program's heap space.
The C standard library defines a set of APIs to manage dynamic memory allocations such as malloc() and free() [24].
For example, malloc() allocates the given number of bytes and returns a pointer to the allocated memory, and free() reclaims the memory specified by the given pointer.
A variety of heap allocators [19,26,41,43,45,49,56,59,64,65] have been developed to meet the specific needs of target programs.
Heap allocators have two types of common goals: good performance and small memory footprintminimizing the memory usage as well as reducing fragmentation, which is the unused memory (i.e., hole) among in-use memory blocks.
Unfortunately, these two desirable properties are fundamentally conflicting; an allocator should minimize additional operations to achieve good performance, whereas it requires additional operations to minimize fragmentation.
Therefore, the goal of an allocator is typically to find a good balance between these two goals for its workloads.
Common designs.
In analyzing various heap allocators, we found their common design principles shown in Table 3: binning, in-place metadata, and cardinal data.
Many allocators use size-based classification, known as binning.
In particular, they partition a whole size range into multiple groups to manage memory blocks deliberately according to their size groups; small-size blocks focus on performance, and large-size blocks focus on memory usage of the allocators.
Moreover, by dividing size groups, when they try to find the best-fit block, the smallest but sufficient block for given request, they scan only blocks in the proper size group instead of scanning all memory blocks.Moreover, many dynamic memory allocators place metadata near the payload, called in-place metadata, even though some allocators avoid this because of security problems from corrupted metadata in the presence of memory corruption bugs (see Table 3).
To minimize memory fragmentation, a memory allocator should maintain information about allocated or freed memory in metadata.
Even though the allocator can place metadata and payload in distinct locations, many allocators store the metadata near the payload (i.e., a head or a tail of a chunk) to increase locality.
In particular, by connecting metadata and payload, an allocator can get benefits from the cache, resulting in performance improvement.Further, memory allocators contain only cardinal data that are not encoded and essential for fast lookup and memory usage.
In particular, metadata are mostly pointers or sizerelated values that are used for their data structures.
For example, ptmalloc2 stores a raw pointer for a linked list that is used to maintain freed memory blocks.This observation has been leveraged to devise the universal method to test various allocators regardless of their implementations (see §5.2 Figure 1: Metadata for a chunk in ptmalloc2 and memory layout for the in-use and freed chunks [23].
binning to explore multiple size groups of an allocator.
For example, if we just uniformly pick a size in the 2 64 space, the probability of choosing the smallest size group in ptmalloc2 (< 2 7 ) becomes nearly zero (2 −57 ).
Thus, we need to use a better sampling method considering binning.
Moreover, the other two design principles -in-place and cardinal metadata -limit the locations and domains of metadata, reducing the search space.
Under these design principles, we only need to focus on metadata in the boundary of a chunk with specific forms (i.e., pointers or sizes).
In this section, we discuss ptmalloc2 [22,23,27], the heap allocator used in glibc, whose exploitation techniques have been heavily studied because of its prevalence and its complexity of metadata [1,3,7,18,20,25,36,38,55].
Similar to other work [17,58], we will use ptmalloc2 as our default allocator for further discussions.
Metadata.
A chunk in ptmalloc2 is a memory region containing metadata and payload.
Memory allocation API such as malloc() returns the address of the payload in the chunk.
Figure 1 shows the metadata of a chunk and its memory layout for an in-use and a freed chunk.
prev_size represents the size of a previous chunk if it is freed.
Although the prev_size of a chunk overlaps with the payload of the previous chunk, this is legitimate since prev_size is considered only after the previous chunk is freed, i.e., the payload is no longer used.
size represents the size of a current chunk.
The real size of the chunk is 8-bit aligned, and the 3 LSBs of the size are used for storing the state of the chunk.
The last bit of size, called PREV_IN_USE (P), shows whether the previous chunk is in use.
For example, in Figure 1, after the chunk is freed, the PREV_IN_USE in the next chunk is changed from 1 to 0.
Other metadata, fd, bk, fd_nextsize, and bk_nextsize, are used to maintain linked lists that hold freed chunks.
Binning.
ptmalloc2 has several types of bins: fast bin, small bin, large bin, unsorted bin, and tcache [15].
Each bin has its own characteristics to achieve its goal; a fast bin uses a single-linked list, giving up merging for performance, but a small bin merges its freed chunks to reduce fragmentation.
Moreover, a large bin stores chunks that have different sizes to handle arbitrarily large chunks.
To optimize scanning for the best-fit chunk, a large bin maintains another sorted, doublelinked list.
The unsorted bin is a special bin that serves as a fast staging place for free chunks.
If a chunk is freed, it first moves to the unsorted bin and is used to serve the subsequent allocation.
If the chunk is not suitable for the request, it will move to a regular bin (i.e., a small bin or a large bin).
Using the unsorted bin, ptmalloc2 can increase locality for performance by deferring the decision for the regular bins.
The tcache, per-thread cache, is enabled by default from glibc 2.26.
It works similarly to a fast bin but requires no locking for threads, and therefore it can achieve significant performance improvements for multithread programs [15].
Heap exploit techniques have recently been much subtle and sophisticated to bypass the new security checks introduced in the allocators.
If an attacker found a vulnerability that corrupts heap metadata (e.g., overflow) or improperly uses heap APIs (e.g., double free), the next step is to develop the bug to a more useful exploit primitive such as arbitrary write.
To do so, attackers typically have to modify the heap metadata, craft a fake chunk, or call other heap APIs according to the implementation of the target heap allocator.
This development was trivial in the good old days for attackers; they can use the universal technique for most allocators (e.g., unsafe unlink).
However, it became complicated after many security checks were introduced to respond to such attacks.
Therefore, researchers have studied and shared heap exploitation techniques that are reusable methods to develop vulnerabilities to useful attack primitives [1,3,7,18,18,20,25,36,38,55,66,72].
Table 4 summarizes modern heap exploitation techniques from previous work [17] and new ones that our tool, ARCHEAP, found.
Example: Unsafe unlink.
One of the most famous heap exploitation techniques is the unsafe unlink attack that abuses the unlink mechanism of double-linked lists in heap allocators, as illustrated in Figure 2a.
By modifying a forward pointer (P->fd) into a properly encoded location and a backward pointer (P->bk) into a desired value, attackers can achieve arbitrary writes (see, P->fd->bk = P->bk).
Due to the prevalence of double-linked lists, this technique was used for many allocators, including dlmalloc, ptmalloc2, and even the Win- dows allocator [1].
To mitigate this attack, allocators have added the new security check shown in Figure 2a, which turns out to be insufficient to prevent more advanced attacks.
The check verifies an invariant of a double-linked list that a backward pointer of a forward pointer of a chunk should point to the chunk (i.e., P->fd->bk == P) and vice versa.
Therefore, attackers cannot make the pointer directly refer to arbitrary locations as before since the pointer will not hold the invariant.
Even though the check prevents the aforementioned attack, attackers can avoid this check by making a fake chunk to meet the condition, as in Figure 2b.
Compared to the previous one, the check makes the exploitation more complicated, but still feasible.
In this section, we discuss our heap abstract model, which enables us to describe a heap exploit technique independent from an underlying allocator.
Here, we focus on an adversarial model, omitting obvious heap APIs (i.e., malloc and free) for brevity.
Note that this abstraction is consistent with related work [17,58].
Our model abstracts a heap technique in two aspects: 1) types of bugs (i.e., allowing an attacker to divert the program into unexpected states), and 2) impact of exploitation (i.e., describing what an attacker can achieve as a result).
This section elaborates on each of these aspects.
1) Types of bugs.
Four common types of heap-related bugs instantiate exploitation:• Overflow (OF): Writing beyond an object boundary.
• Write-after-free (WF): Reusing a freed object.
• Arbitrary free (AF): Freeing an arbitrary pointer.
• Double free (DF): Freeing a reclaimed object.
Each of theses mistakes of a developer allows attackers to divert the program into unexpected states in a certain way: overflow allows modification of all the metadata (e.g., struct malloc_chunk in Figure 1) of any consequent chunks; write-after-free allows modification of the free metadata (e.g., fd/bk in Figure 1), which is similar in spirit to use-afterfree; double free allows violation of the operational integrity of the internal heap metadata (e.g., multiple reclaimed pointers linked in the heap structure); and arbitrary free similarly breaks the operational integrity of the heap management but in a highly controlled manner-freeing an object with the crafted metadata.
Since overflow enables a variety of paths for exploitation, we further characterize its types based on common mistakes and errors by developers.
• Off-by-one (O1): Overwriting the last byte of the next consequent chunk (e.g., when making a mistake in size calculation, such as CVE-2016-5180 [31]).
• Off-by-one NULL (O1N): Similar to the previous type, but overwriting the NULL byte (e.g., when using string related libraries such as sprintf).
It is worth noting that, unlike a typical exploit scenario that assumes arbitrary reads and writes, we exclude such primitives for two reasons: They are too specific to applications and execution contexts, hardly meaningful for generalization, and they are so powerful for attackers to launch easier attacks, demotivating use of heap exploitation techniques.
Therefore, such powerful primitives are considered one of the ultimate goals of heap exploitation.
2) Impact of exploitation.
The goal of each heap exploitation technique is to develop common types of heap-related bugs into more powerful exploit primitives for full-fledged attacks.
For the systematization of a heap exploit, we categorize its final impact (i.e., an achieved exploit primitive) into four classes:• Arbitrary-chunk (AC): Hijacking the next malloc to return an arbitrary pointer of choice.
• Overlapping-chunk (OC): Hijacking the next malloc to return a chunk inside a controllable (e.g., overwritable) chunk by an attacker.
• Arbitrary-write (AW): Developing the heap vulnerability into an arbitrary write (a write-where-what primitive).
• Restricted-write (RW): Similar to arbitrary-write, but with various restrictions (e.g., non-controllable "what", such as a pointer to a global heap structure).
Attackers want to hijack control by using these exploit primitives combined with application-specific execution contexts.
For example, in the unsafe unlink (see, Figure 2), attackers can develop heap overflow to arbitrary writes and corrupt code pointers to hijack control.
To commonly describe heap exploitation techniques, we clarify legitimate actions that an attacker can launch.
First, an attacker can allocate an object with an arbitrary size, and free objects in an arbitrary order.
This essentially means that the attacker can invoke an arbitrary number of malloc calls with an arbitrary size parameter and invoke free (or not) in whatever order the attacker wishes.
Second, the attacker can write arbitrary data on legitimate memory regions (i.e., the payload in Figure 1 or global memory).
Although such legitimate behaviors largely depend on applications in theory, assuming this powerful model lets us examine all potential opportunities for abuses.
Third, the attacker can trigger only a single type of bug.
This limits the capabilities of the adversary to the realistic setting.
However, we allow multiple uses of the same type to simulate a re-triggerable bug in practice.
We note that it is always more favorable to an attacker if a heap exploit technique requires fewer capabilities than what are described here, and in such cases, we make a side note for better clarification.
Our goal is to automatically explore new types of heap exploitation techniques given an implementation of any heap allocator-its source code is not required like AFL [75].
Such a capability not only enables to support automatic exploit synthesis but also makes several, unprecedented applications possible: 1) systematically discovering unknown types of heap exploitation schemes; 2) comprehensively evaluating the security of popular heap allocators; and 3) providing insight into what and how to improve their security.
However, achieving this autonomous capability is far from trivial, for the following reasons.
Autonomous reasoning of the heap space.
To find heap exploitation techniques, we should satisfy complicated constraints to bypass security checks (see §2.3) in a large search space consisting of enormous possible orders, arguments for heap APIs, and data in the heap and global buffer.
This space could be greatly reduced using exploit-specific knowledge [17]; however, this is not applicable for finding new exploit techniques.
To resolve this issue, we use a random search algorithm that is effective in exploring a large search space [33].
We also abstract common designs of modern heap allocators to further reduce the search space ( §5.2).
Devising exploitation techniques.While enumerating possible candidates for exploit techniques, a system needs to verify whether the candidates are valuable.
One way to assess the candidates is to synthesize end-to-end exploits automatically (e.g., spawning a shell), but this is extremely difficult and inefficient, especially for heap vulnerabilities [4,11,16,33,58,60].
To resolve this issue, we use the concept of impact of exploitation.
In particular, we estimate the impacts of exploitation (i.e., AC, OC, AW, and RW) during exploration instead of synthesizing a full exploit.
We show that these impacts can be quickly detectable at runtime by utilizing shadow memory ( §5.3).
Normalization.
Even though a random search is effective in exploring a large search space, an exploitation technique found by this algorithm tends to be redundant and inessential, requiring non-trivial time to analyze the result.
To fix this issue, we leverage the delta-debugging technique [76] to minimize the redundant actions and transform the found result into an essential class.
This is so effective that we could reduce actions by 84.3%, drastically helping us to share the Execute actions and detect impacts ( §5.3)Generate PoC exploit ( §5.4)Minimize actions using delta-debugging ( §5.4) Model specification PoC exploit new exploitation techniques with the community ( §5.4).
Exploitation Techniques ARCHEAP follows a common paradigm in classical fuzzingtest generation, crash detection, and test reduction-but is tailored to heap exploitation (see Figure 3).
It first generates a sequence of heap actions based on a user-provided model specification.
This specification is optional; if it is not given, ARCHEAP will generate every possible action.
Heap actions that ARCHEAP can formulate include heap allocation, free, buffer writes, heap writes, and bug invocation ( §5.2).
During execution, ARCHEAP evaluates whether the executed test case results in impacts of exploitation, similar in concept to detecting a crash in fuzzing ( §5.3).
Whenever ARCHEAP finds a new exploit, it minimizes the heap actions and produces PoC code (see Figure 5), which contains only an essential set of actions ( §5.4).
It is worth noting that this minimization is to help post-analysis of a found technique but is irrelevant to false positives; ARCHEAP yields no false positive during our evaluation thanks to its straightforward analysis at runtime.
ARCHEAP randomly generates five types of heap-related actions: allocation, deallocation, buffer writes, heap writes, and bug invocation.
To reduce the search space, ARCHEAP formulates each action on top of an abstract heap model using the common design idioms of modern allocators.
The following explains how each action takes advantage of the designs in reducing the search space.Allocation.
The first action that ARCHEAP can perform is allocating memory through the standardized API, malloc().
After allocating memory, ARCHEAP stores the returned object's address to its internal data structure, called the container.
It also stores a chunk size of the object using another API, malloc_usable_size(), and its status (i.e., allocated) for further use in other actions (Line 15 -23 in Figure 4), e.g., deallocation or bug invocation.
ARCHEAP allocates memory in random size but considering multiple aspects to test an allocator.
First of all, ARCHEAP carefully chooses a size of an object (I1 in Ta- ble 5) to examine different logic in different bins.
In particular, ARCHEAP first randomly selects a group of sizes and then allocates an object whose size is in this group.
This group is separated by approximate boundary values instead of implementation-specific ones to make ARCHEAP compatible with any allocator.
Currently, ARCHEAP uses four boundaries with exponential distance from 2 0 to 2 20 , e.g., the first group is [2 0 , 2 5 ), the second one is [2 5 , 2 1 0), etc.
It makes a small size likely to be chosen.
For instance, the chance of making a fast-bin object in ptmalloc2 becomes more than 1/4 (i.e., chance to select the first group), which was 2 −57 in the uniform sampling.
This division is arbitrary but sufficient for increasing the probability of exploring various bins.
ARCHEAP also attempts to allocate multiple objects in the same bin (I2) since an object interacts with others in the same bin.
For example, in ptmalloc2, a non-fast-bin object merges with a non-fast-bin object, not with a fast bin object.
To cover this interaction, ARCHEAP allocates an object whose size is related to the other object's size.To find techniques induced by common mistakes in an allocator, ARCHEAP also uses specialized sizes (I3, I4).
In particular, ARCHEAP uses the differences between pointers to find integer overflow vulnerabilities in an allocator.
For example, a vulnerable allocator can return a buffer address when claiming a very large chunk whose size is the same as the difference between the buffer and a heap object.
ARCHEAP also utilizes several pre-defined constants, e.g., zero or negative numbers, to evaluate its edge case handling.
This is analogous to classical fuzzing, which uses a fixed set of integers to check corner conditions (e.g., interesting values in AFL [75]).
Deallocation.
ARCHEAP deallocates a randomly selected heap pointer from the heap container using free().
To avoid launching a double free bug, which will be emulated in the bug invocation action, ARCHEAP checks an object's status.
If ARCHEAP chooses an already freed pointer, it simply ignores the deallocation action to avoid the bug ( Line 24 -30 // generate random size using the integer strategies in Table 5 10 // note that it only uses container and buffer, not their shadow 11 } 12 void* random_value() { 13 // similar to random_size(), but use all strategies in Table 5 random generation) infeasible.
To overcome such limitations, ARCHEAP exploits the in-place and cardinal metadata design of heap allocators to prune its search space.
In particular, ARCHEAP writes only a limited number of values -noted as MAX_WRITE in the pseudocode, which is eight in our prototype -from the start or the end of an object (see Line 31 -51 in Figure 4) since an allocator stores its metadata near the boundary for locality (in-place metadata).
Further, ARCHEAP generates random values (see Table 5) that can be used for sizes or pointers in an allocator instead of fully random ones (cardinal data).
To explore various exploit techniques, ARCHEAP introduces systematic noises to generated values.
In particular, ARCHEAP modifies a value using linear (addition and multiplication) or shift transformation (addition only) according to the value's type.
For example, a heap address can be shifted by word granularity (i.e., respecting alignment); however, ❷ p[1] p[2] p[0] p[1] p[2] p[0] p[1] p[2] p[0]Heap container Shadow memoryGlobal buffer Shadow memory❸ ★ p[2] p[0] p[1] p[2] p[0] p[2]Discrepency after free() -Restricted write in the heap container★ = (void*)&p[1] -offsetof(bk) p[0]Heap container Shadow memoryGlobal buffer Shadow memory❹ p[2] p[0] p[2] p[0] p[2] bufDivergence after heap write -Arbitrary write in the heap containerp[0]Heap container Shadow memoryGlobal buffer Shadow memory❺ p[2] p[0] p[2] p[0] p[2] bufDivergence after heap write -Arbitrary write in the global buffer ⃝ off-by-one NULL overflow, 5 ⃝ double free, and 6⃝ arbitrary free.
ARCHEAP performs only one of these bugs for a technique to limit the power of an adversary as described in the threat model (see §3.2).
Also, ARCHEAP allows repetitive execution of the same bug to emulate the situation in which an attacker re-triggers the bug.ARCHEAP deliberately builds a buggy action to ensure its occurrence.
For overflow and off-by-one, ARCHEAP uses the malloc_usable_size API to get the actual heap size to ensure overflow.
This is necessary since the request size could be smaller than the actual size due to alignment or the minimum size constraint.
Particularly for ptmalloc2, ARCHEAP uses a dedicated single-line routine to get the actual size since ptmalloc2's malloc_usable_size() is inaccurate under the presence of memory corruption bugs.
Moreover, in double free and write-after-free bugs, ARCHEAP checks whether a target chunk is already freed.
If it is not freed yet, ARCHEAP ignores this buggy action and waits for the next one.Model specifications.
A user can optionally provide model specification either to direct ARCHEAP to focus on a certain type of exploitation techniques or to restrict the conditions for a target environment.
It accepts five types of a model specification: chunk sizes, bugs, impacts, actions, and knowledge.
The first four types are self-explanatory, and knowledge is about the ability of an attacker to break ASLR (i.e., prior knowledge of certain addresses).
The user can specify three types of addresses that an attacker may know: a heap address, the global buffer address, and the container address.
Such knowledge will affect future data generation by ARCHEAP, as shown in Table 5.
ARCHEAP detects four types of impact of exploitations that are the building blocks of a full chain exploit: arbitrarychunk (AC), overlapping-chunk (OC), arbitrary-write (AW), and restricted-write (RW).
This approach has two benefits, namely, expressiveness and performance.
These types are useful in developing control-hijacking, the ultimate goal of an attacker.
Thus, all existing techniques lead to one of these types, i.e., can be represented by these types.
Also, it causes small performance overheads to detect the existence of these types with a simple data structure shadowing the heap space.1 To detect AC and OC, ARCHEAP determines any overlapping chunks in each allocation (Line 18 in Figure 4).
To make the check safe, it replicates the address and size of a chunk right after malloc since it could be corrupted when a buggy action is executed.
Using the stored addresses and sizes, it can quickly check if a chunk overlaps with its data structure (AC) or other chunks (OC).2 To detect AW and RW, ARCHEAP safely replicates its data structures, the containers and the global buffer, using the technique called shadow memory.
During execution, ARCHEAP synchronizes the state of the shadow memory whenever it performs actions that can modify its internal structures: allocations for the container and buffer writes for the global buffer (Line 21, 49).
Then, ARCHEAP checks the divergence of the shadow memory when performing any action ( Line 17, 29, 43, 50).
Because of the explicit consistency maintained by ARCHEAP, divergence can only occur when previously executed actions modify ARCHEAP's data structures via an internal operation of the heap allocator.
Later, these actions can be reformulated to modify sensitive data of an application instead of the data structure for exploitation.ARCHEAP's fuzzing strategies (Table 5) make this detection efficient by limiting its analysis scope to its data structures.
In general, a heap exploit technique can corrupt any data, leading to scanning of the entire memory space.
However, the technique found by ARCHEAP can only modify heap or the data structures because these are the only visible addresses from its fuzzing strategies.
ARCHEAP checks only modification in its data structures, but ignores one in heap because it is hard to distinguish a legitimate one (e.g., modifying metadata in deallocation) from an abusing one (i.e., a heap exploit technique) without a deep understanding of an allocator.
This is semantically equivalent to monitoring the violence of the implicit invariant of an allocator -it should not modify memory that is not under its control.ARCHEAP distinguishes AW from RW based on the heap actions that introduce divergence.
If a divergence occurs in allocation or deallocation, it concludes RW, otherwise (i.e., in heap or buffer write), it concludes AW.
The underlying intuition is that parameters in the former actions are hard to control arbitrarily, but not in the latter ones.
After detecting divergence, ARCHEAP copies the original memory to its shadow to stop repeated detections.
A running example.
Figure 6 shows the state of the shadow memory when executing Figure 5.
1 After the first allocation, ARCHEAP updates its heap container and corresponding shadow memory to maintain their consistency, which might be affected by the action.
2 It performs two more allocations so updates the heap container and shadow memory accordingly.
3 After deallocation, p [1] is changed into ⋆ due to unlink() in ptmalloc2 (Figure 2a).
At this point, ARCHEAP detects divergence of the shadow memory from the original heap container.
Since this divergence occurs during deallocation, the impact of exploitation is limited to restricted writes in the heap container.
4 In this case, since the heap write causes the divergence, the actions can trigger arbitrary writes in the heap container.
5 Since this heap write introduces divergence in the global buffer, the actions can lead to arbitrary write in the global buffer.
To find the root cause of exploitation, ARCHEAP refines test cases using delta-debugging [76], as shown in Algorithm 1.
The algorithm is simple in concept: for each action, ARCHEAP re-evaluates the impact of exploitation of the test cases without it.
If the impacts of the original and new test cases are equal, it considers the excluded action redundant (i.e., no meaningful effect to the exploitation).
The intuition behind this decision is that many actions are independent (e.g., buffer writes and heap writes) so that the delta-debugging can clearly separate non-essential actions from the test case.
Our current algorithm is limited to evaluating one individual action at a time.
It can be easily extended to check with a sequence or a combination of heap actions together, but our evaluation shows that the current scheme using a single action is effective enough for practical uses-it eliminates 84.3% of non-essential actions on average (see §8.3).
Input :actions -actions that result in an impact 1 Once minimized, ARCHEAP converts the encoded test case to a human-understandable PoC like that in Figure 5 using one-to-one mapping between each action and C code (e.g., an allocation action → malloc()).
We extended American Fuzzy Lop (AFL) to run our heap action generator that randomly executes heap actions.
The generator sends a user-defined signal, SIGUSR2, if it finds actions that result in an impact of exploitation.
We also modified AFL to save crashes only when it gets SIGUSR2 and ignores other signals (e.g., segmentation fault), which are not interesting in finding techniques.
We carefully implemented the generator not to call heap APIs implicitly except for the pre-defined actions for reproducing the actions.
For example, the generator uses the standard error for its logging instead of standard out, which calls malloc internally for buffering.
To prevent the accidental corruption of internal data structures, the generator allocates its data structures in random addresses.
Thus, the bug actions such as overflow cannot modify the data structures since they will not be adjacent to heap chunks.
This section discusses the new exploitation techniques in ptmalloc2 during our evaluation.
Compared to the old techniques, we determine their uniquenesses in two aspects: root causes and capabilities, as shown in Table 6.
More information (e.g., elapsed time or models) can be found in section §8.
To share new attack vectors in ptmalloc2, the techniques are reported and under review in how2heap [61], the de-facto standard for exploitation techniques.
Most PoC codes are available in Appendix A. Unsorted bin into stack (UBS).
This technique overwrites the unsorted bin to link a fake chunk so that it can return the address of the fake chunk (i.e., an arbitrary chunk).
This is similar to house of lore [7], which corrupts a small bin to achieve the same attack goal.
However, the unsorted bin into stack technique requires only one kind of allocation, unlike house of lore, which requires two different allocations, to move a chunk into a small bin list.
This technique has been added to how2heap [61].
House of unsorted einherjar (HUE).
This is a variant of house of einherjar, which uses an off-by-one NULL byte overflow and returns an arbitrary chunk.
In house of einherjar, attackers should have prior knowledge of a heap address to break ASLR.
However, in house of unsorted einherjar, attackers can achieve the same effect without this pre-condition.
We named this technique house of unsorted einherjar, as it interestingly combines two techniques, house of einherjar and unsorted bin into stack, to relax the requirement of the well-known exploitation technique.
Unaligned double free (UDF).
This is an unconventional technique that abuses double free in a small bin, which is typically considered a weak attack surface thanks to comprehensive security checks.
To avoid security checks, a victim chunk for double free should have proper metadata and is tricked to be under use (i.e., P bit of the next chunk is one).
Since double free doesn't allow arbitrary modification of metadata, existing techniques only abuse a fast bin or tcache, which have weaker security checks than a small bin (e.g., fast-bin-dup in Table 4).
Interestingly, unaligned double free bypasses these security checks by abusing the implicit behaviors of malloc().
First, it reuses the old metadata in a chunk since malloc() does not initialize memory by default.
Second, it fills freed space before the next chunk to make the P bit of the chunk one.
As a result, the technique can bypass all security checks and can successfully craft a new chunk that overlaps with the old one.
Overlapping chunks using a small bin (OCS).
This is a variant of overlapping-chunks (OC) that abuses the unsorted bin to generate an overlapping chunk, but this technique crafts the size of a chunk in a small bin.
Unlike OC, it requires more actions -three more malloc() and one more free()-but doesn't require attackers to control the allocation size.
When attackers cannot invoke malloc() with an arbitrary size, this technique can be effective in crafting an overlapping chunk for exploitation.
Fast bin into other bin (FDO).
This is another interesting technique that allows attackers to return an arbitrary address: it abuses consolidation to convert the type of a victim chunk from the fast bin to another type.
First, it corrupts a fast bin free list to insert a fake chunk.
Then, it calls malloc_consolidate() to move the fake chunk into the unsorted bin during the deallocation process.
Unlike other techniques related to the fast bin, this fake chunk does not have to be in the fast bin.
We exclude this PoC due to space limits, but it is available in our repository.
We also applied ARCHEAP to the 10 different allocators with various versions.
First, we tested dlmalloc 2.7.2, dlmalloc 2.8.6 [41], and musl [59] 1.1.9, which were used in the related work, HeapHopper [17].
Moreover, we tested other real-world allocators: the latest version of musl (1.1.24), jemalloc [19], tcmalloc [26], Microsoft mimalloc [43] with its default and secure mode (noted as mimalloc-secure), and LLVM Scudo [45].
Furthermore, we evaluated allocators from academia: DieHarder [49], Mesh [56], FreeGuard [64], and Guarder [65].
Applying ARCHEAP to other allocators was trivial; we leveraged LD_PRELOAD to use a new allocator.
Under the assumption that internal details of the allocators are unknown, we ran ARCHEAP with four models specifying each impact (i.e., OC, AC, RW, and AW) one by one to exhaustively explore possible techniques.
After 24 hours of evaluation, it found several exploit techniques among seven out of 10 allocators except for Scudo, FreeGuard, and Guarder due to their secure design.
We also tested ARCHEAP with custom allocators from DARPA Cyber Grand Challenge, whose results can be found in §A.1.
As shown in Table 7, ARCHEAP discovers various exploitation techniques for ptmalloc2-related allocators: dlmallocthe ancestor of ptmalloc2 and musl-a libc implementation in embedded systems inspired by dlmalloc.
In dlmalloc 2.7.2, dlmalloc 2.8.6, and musl 1.1.9, ARCHEAP not only re-discovered all techniques found by HeapHopper, but also newly found the following facts: 1) these allocators are all vulnerable to double free, and 2) an arbitrary chunk is still achievable through overflow in dlmalloc-2.8.6.
This was hidden in HeapHopper due to its limitation to handle symbolicsize allocation.
Note that we merged special cases of overflow (O1, O1N) into OV to be consistent with HeapHopper [17], and our claims for new techniques are very conservative; we claim discovery of new techniques only when HeapHopper cannot find equivalent or more powerful ones (e.g., AC is more powerful than OC).
We further compare ARCHEAP with HeapHopper in §8.1.
ARCHEAP also found that musl has no security improvement in the latest version; all techniques in musl 1.1.9 are still working in 1.1.24.
ARCHEAP also successfully found several heap exploit techniques in allocators that are dissimilar to ptmalloc2 (see Table 7) for the following reasons.
First, ARCHEAP's model, which is based on the common designs in allocators ( §2.1), is generic enough to cover non-ptmalloc allocators.
For example, tcmalloc [26] is aiming at high performance computing, resulting in very different design from ptmalloc2's (e.g., heavy use of thread-local cache).
However, tcmalloc still follows our model: its metadata are placed in the head of a chunk (in-place metadata) and consist of linked list pointers (cardinal data).
Thus, ARCHEAP can find several techniques in tcmalloc including one that can lead to an arbitrary chunk using overflow (see Figure A.2).
It is worth emphasizing that our model only depends on metadata's appearance, not on their generation or management, which introduce more variety in design, making generalization difficult.
Second, thanks to standardized APIs, ARCHEAP can find exploit techniques even in allocators that are deviant from our model (e.g., jemalloc).
In particular, ARCHEAP discovered techniques that are reachable only using APIs (e.g., double free) although the allocators have removed in-place metadata for security.ARCHEAP helps to find implementation bugs in allocators by showing unexpected exploit primitives in secure allocators or that can be invokable without a bug.
Accordingly, ARCHEAP found three bugs in mimalloc-secure, DieHarder, and Mesh.
We reported our findings to the developers; two of them got acknowledged and are patched.
It is worth mentioning that our auto-generated PoC has been added to mimalloc as its regression test.
In the following, we discuss each issue that ARCHEAP found.DieHarder, mimalloc-secure: memory duplication in large chunks using double free.
ARCHEAP found the technique that allows the duplication large chunks (more than 64K bytes) in the well-known secure allocators, DieHarder and mimalloc-secure.
Interestingly, even though the allocators have no direct relationship according to the developer of mimalloc [43], ARCHEAP found that both allocators are vulnerable to this technique.
Their root causes are also distinct: DieHarder misses verifying its chunk's status when allocating large chunks, unlike for smaller chunks, and mimalloc checked the status of an incorrect block.
ARCHEAP successfully found this corner case without having any hint about the internals of the allocators using its randomized exploration.
PoC is available in Figure A.3.
Mesh: memory duplication using allocations with negatives sizes.
ARCHEAP found that if an attacker allocates an object with negative size, Mesh will return the same chunk twice (i.e., duplication) instead of NULL.
We applied ARCHEAP to four versions of ptmalloc2 distributed in Ubuntu LTS: precise (12.04, libc 2.15), trusty (14.04, libc 2.19), xenial (16.04, libc 2.23), and bionic (18.04, libc 2.27).
In trusty and xenial, a new security check that checks the integrity of size metadata (refer (1) in Figure 2a) is backported by the Ubuntu maintainers.
To compare each version, we perform differential testing: we first apply ARCHEAP to each version and generate PoCs.
Then, we validate the generated PoCs from one version against other versions.
(see Figure 7).
We identified three interesting trends that cannot be easily obtained without ARCHEAP's automation.
First, a new security check successfully mitigates a few exploitation techniques found in an old version of ptmalloc2: likely, the libc maintainer reacts to a new, popular exploitation technique.
Second, an internal design change in bionic rendered the most PoCs generated from previous versions ineffective.
This indicates the subtleties of the generated PoCs, requiring precise parameters and the orders of API calls for successful exploitation.
However, this does not particularly mean that a new version, bionic, is secure; the new component, tcache, indeed makes exploitation much easier, as Figure 7 shows.
Third, this new component, tcache, which is designed to improve the performance [15], weakens the security of the heap allocators, not just making it easy to attack but also introducing new exploitation techniques.
This is similarly observed by other researchers and communities [17,37].
This section tries to answer the following questions:1.
How effective is ARCHEAP in finding new exploitation techniques compared to the state-of-the-art technique, HeapHopper?
2.
How exhaustively can ARCHEAP explore the securitycritical state space?
3.
How effective is delta-debugging in removing redundant heap actions?
Evaluation setup.
We conducted all the experiments on Intel Xeon E7-4820 with 256 GB RAM.
For seeding, we used 256 random bytes that are used to indicate a starting point of the state exploration and are not critical, as ARCHEAP tends to converge during the state exploration.
HeapHopper [17] was recently proposed to analyze existing exploitation techniques in varying implementations of an allo-Name Bug Impact Chunks # Txn Size TxnList (A list of transactions) FD WF AC Fast 8 {8} M-M-F-WF-M-M UU O1 AW,RW Small 6 {128} M-M-O1-F HS AF AC Fast 4 {48} AF-M PN O1N OC Small 12 {128,256,512} M-M-M-F-O1N-M-M-F-F-M HL WF AC Small 9 {100,1000} M-M-F-M-WF-M-M OC O1 OC Small 8 {120,248,376} M-M-M-F-O1-M UB WF AW,RW Small 7 {400} M-M-F-WF-M HE O1 AC Small 7 {56,248,512} M-M-O1-F-M # Txn:The number of transactions, M: malloc, F: free Table 8: Exploit-specific models for known techniques from HeapHopper.
It is worth noting that the results of variants (i.e., techniques have same prerequisites, but different root causes) are identical for ARCHEAP with no specific model (marked with † and ‡ in Table 9 and Table 10) since ARCHEAP neglects the number of transactions (i.e., # Txn).
cator.
Because of its goal, HeapHopper emphasizes completeness and verifiability, differentiating its method (i.e., symbolic execution) from ARCHEAP's (i.e., fuzzing).
To overcome the state explosion in symbolic execution, HeapHopper tightly encodes the prior knowledge of exploit techniques into its models, e.g., the number of transactions (i.e., non-write actions in ARCHEAP), allocation sizes (i.e., guiding the use of specific bins), and even a certain order of transactions.
By relying on this model, it could incrementally perform the symbolic execution for all permutations of transactions.
Unfortunately, its key idea-guiding the state exploration with detailed models-limits its capability to only its original purpose that validates known exploitation techniques, unlike our approach can find unknown techniques.Despite their different purposes, their outputs are equivalent to heap exploitation techniques; therefore, we need to show the orthogonality of ARCHEAP and HeapHopper; neither of them can replace the other.
To objectively compare both approaches, we performed three experiments: 1 finding unknown techniques with no exploit-specific model (i.e., applying HeapHopper to ARCHEAP's task), 2 finding known techniques with partly specified models (i.e., evaluating the roles of specified models in each approach), and 3 finding known techniques with exploit-specific models (i.e., applying ARCHEAP to HeapHopper's task).
In the experiments, we considered variants of exploit techniques 1 as an equal class since both systems cannot distinguish their subtle differences.
We ran each experiment three times with a 24-hour timeout for proper statistical comparison [40].
We used the default option for HeapHopper since it shows the best performance in the following experiments (see §A.2).
1 New techniques.
We first check if HeapHopper's approach can be used to find previously unknown exploitation techniques that ARCHEAP found (see, §7.1).
To apply HeapHopper, we provided models that specify all sizes for corresponding bins but limit the number of transactions following our PoCs, as shown in Table 9.
Note that, in theory, such relaxation is general enough to discover new techniques given an infinite amount of computing resources.
In the ex-1 Exploit techniques often have the same prerequisite but different root causes such as UBS and HL.
Table 9: The number of experiments (at most three) that discover new exploitation techniques, the number of found techniques -the number after hash (#) sign, elapsed time, and corresponding models.
Briefly, ARCHEAP discovered all four techniques, but HeapHopper failed to.
We omitted FDO, which has a superset model of FD; therefore, it becomes indistinguishable to FD (see , Table 8).
periment, FDO is excluded because its model is a superset of FD; having FDO simply makes ARCHEAP and HeapHopper converge to FD.HeapHopper fails to identify all unknown exploitation primitives with no exploit-specific models (see Table 9).
In fact, it encounters a few fundamental problems of symbolic execution: 1) exponentially growing permutations of transactions and 2) huge search spaces in selecting proper size and orders to trigger exploitation.
Although HeapHopper demonstrated a successful state exploration of seven transactions with three size parameters ( §7.1 in [17]), the search space required for discovering new techniques is much larger, rendering HeapHopper's approach computationally infeasible.
On the contrary, ARCHEAP successfully explores the search space using the random strategies, and indeed discovers unknown techniques.
2 Known techniques with partly specified models.
We also evaluate the role of exploit-specific models in both approaches, which are unavailable in finding new techniques.
In particular, we evaluated both systems with partial models, namely, the size parameters (+Size) and a sequence of transactions (+TxnList), used in HeapHopper (see , Table 8).
To prevent each system from converging to easy-to-find techniques, we tested each model on top of the baseline heap model (i.e., Bug+Impact+Chunks).
This experiment (i.e., 2 in Table 10) shows that ARCHEAP outperforms HeapHopper with no or partly specified models: ARCHEAP found five more known techniques than HeapHopper in both +Size and Bug+Impact+Chunks.
Interestingly, ARCHEAP can operate worse with additional information; ARCHEAP found three fewer techniques in +TxnList.
Unlike ARCHEAP, exploit-specific models are beneficial to HeapHopper, finding one more techniques when +TxnList is given.
This result shows that a precise model plays an essential role in symbolic execution but not in fuzzing.
In short, ARCHEAP is particularly preferable when exploring unknown search space, (i.e., finding new techniques), where an accurate model is inaccessible.
Table 10: The number of discovered known exploitation techniques and elapsed time for discovery in ARCHEAP and HeapHopper with various models.
In summary, ARCHEAP outperforms HeapHopper with no or partly specified models, e.g., ARCHEAP found five more techniques with no specific model (Bug+Impact+Chunks).
Even though HeapHopper found one more technique than ARCHEAP if exploit-specific models are available, it suffers from false positives (marked in gray).
T F O µ σ T F O µ σ T F O µ σ T F O µ σ T F O µ σ T F O µ σ T F O µ σ T F O µ σ FD 3exploit-specific models (+Size, TxnList) are provided, HeapHopper's approach works better: It found one more known technique and found four techniques more quickly than ARCHEAP (as illustrated in 3 in Table 10).
This shows the strength of HeapHopper in validating existing techniques, rendering orthogonality of both tools.
We observed one interesting behavior of HeapHopper in this experiment.
With more exploit models specified, HeapHopper tends to suffer from false positives because of its internal complexity, as noted in the paper [17].
Despite its small numbers -dozens in three experiments -this shows incorrectness in HeapHopper, resulting in failures to find UU and UE.
We confirmed these false positives with HeapHopper's authors.
On the contrary, ARCHEAP's approach does not introduce false positives thanks to its straightforward analysis at runtime.
This experiment also highlights an interesting design decision of ARCHEAP: separating the exploration and reducing phases.
With no exploit-specific guidance, ARCHEAP can freely explore the search space for finding heap exploitation techniques, and so increase the probability of satisfying the precondition of certain exploitation techniques.
For example, if the sequence of transactions of UU (M-M-O1-F) is enforced, ARCHEAP should craft a fake chunk within a relatively small period (i.e., between four actions) to trigger the exploit; otherwise, ARCHEAP has a higher probability to formulate a fake chunk by executing more, perhaps redundant, actions.
However, such redundancy is acceptable in ARCHEAP thanks to our minimization phase that effectively reduces inessential actions from the found exploit.We also confirmed that ARCHEAP can find all tcacherelated techniques [37] and house-of-force, which HeapHopper fails to find because of an arbitrary size allocation.
ARCHEAP can find these techniques within a few minutes, as they require fewer than five transactions.
To show how exhaustively ARCHEAP explores the securitysensitive part of the state space, we counted the number of security checks in ptmalloc2 executed by ARCHEAP.
In 24 hours of exploration, ARCHEAP executed 18 out of 21 security checks of ptmalloc2: it failed to cover C2, C4, and C21 in bug, which is outside of the scope of this work.
C2 and C4 require a strict relationship between large chunks (e.g., the sizes of two chunks are not equal but less than the minimum size), which is probably too stringent for any randomizationbased strategies.
The minimization technique based on delta-debugging is effective in simplifying the generated PoCs for further analysis.
It effectively reduces 84.3% of redundant actions from original PoCs (refer to §7.3) and emits small PoCs that contain 26.1 lines on average (see Table 12).
Although our minimization is preliminary (i.e., eliminating one independent action per testing), the final PoC is sufficiently small for manual analysis to understand impacts of the found technique.
Completeness.
ARCHEAP is fundamentally incomplete due to its random nature, so it would not be surprising at all if someone discover other heap exploitation techniques.HeapHopper, on the other hand, is complete in terms of given models, i.e., exploring all combinations of transactions given the length of transactions.
Since their models are incomplete (or often error-prone), proper use of each approach is dependent on the target use cases.
For example, if one is looking for a practical solution to find new exploitation techniques, ARCHEAP would be a more preferable platform to start with.
Overfitting to fuzzing strategies.
ARCHEAP's approach is quite generic in practice even with its specific fuzzing strategies to the common design decisions in §2.1.
First, ARCHEAP can explore security issues related to APIs (e.g., double free) without loss of generality because of their standardization (see, §7.2).
Second, ARCHEAP's approach to make random metadata is practically useful thanks to the bipartite design of a real-world allocator.
In particular, a performance-focused allocator that places metadata in a chunk (e.g., ptmalloc2) has little motivation to avoid the use of in-place metadata or to violate the cardinal design for its performance.
If an allocator is not performance-oriented, it will move its metadata to a dedicated place for better security (e.g., jemalloc).
Such a design will make all methods to generate metadata useless in finding heap exploitation techniques.
However, ARCHEAP still has a chance to cause overfitting: our fuzzing strategies could be insufficient to examine certain allocators.
In this case, one might have to devise own models for proper space reduction to apply ARCHEAP to non-conventional implementation.
requiring in-depth understanding of a target allocator.
For example, if an allocator uses big-endian encoding for its size, a user should encode this in ARCHEAP's fuzzing strategies.
Scope.
Unlike other automatic exploit generation work, ARCHEAP focuses only on finding heap exploit techniques.
To make end-to-end exploits, we need to properly combine application contexts, which is currently out-of-scope for this project.
Despite many open challenges in realizing fully automated exploit generation, we believe that ARCHEAP can contribute by supplying useful primitives [58].
Moreover, ARCHEAP focuses only on a user-mode allocator.
To extend ARCHEAP to kernel, we need to handle kernel-specific challenges, e.g., non-deterministism and zone-based allocation.
Automatic exploit generation (AEG).
Automatic discovery of heap exploit techniques is a small step toward AEG's ambitious vision [4,10], but it is worth emphasizing its importance and difficulty.
Despite several attempts to accomplish fully automated exploit generation [4,10,11,33,46,58,60,70], AEG, particularly for heap vulnerabilities, is too sophisticated and difficult even for state-of-the-art cyber systems [21,30,62,67].
Recently, Repel et al. [58] propose symbolicexecution-based AEG for heap vulnerabilities, but it only works for much older allocators without security checks (ptmalloc2 version 2.3.3) unlike ARCHEAP (2.23 and 2.27).
Heelan et al. [33,34] demonstrate AEG for heap overflows in interpreters, but specific to scriptable programs.
Unlike the prior work, ARCHEAP focuses on finding heap exploitation techniques, which are re-usable across applications, in modern allocators with full security checks.
Fuzzing beyond crashes.
There has been a large body of attempts to extend fuzzing to find bugs beyond memory safety [29,75].
They often use differential testing, which we used for minimization, to find semantic bugs, e.g., compilers [73], cryptographic libraries [9,53], JVM implementations [14] and learning systems [51].
Recently, SlowFuzz [54] uses fuzzing to find algorithmic complexity bugs, and IMF [69] to spot similar code in binary.
Application-aware fuzzing.
Application-aware fuzzing is one of the attempts to reduce the search space of fuzzing.
In this regard, there have been attempts to use static and dynamic analysis [13,44,52,57], bug descriptions [74], and real-world applications [12,32,39] to extract target-specific information for fuzzing.
Moreover, to reduce the search space for applications that require well-formed inputs, researchers have embedded domain-specific knowledge such as grammar [35,68,73] or structure [9,53] in their fuzzing.
Similar to these works, ARCHEAP reduces its search space by considering its targets and memory allocators, particularly exploiting their common designs.
In this paper, we present ARCHEAP, a new approach using fuzzing to automatically discover new heap exploitation techniques.
ARCHEAP's two key ideas are to reduce the search space of fuzzing by abstracting the common design of modern heap allocators, and to devise a method to quickly estimate the possibility of heap exploitation.
Our evaluation with ptmalloc2 and 10 other allocators shows that ARCHEAP's approach can effectively formulate new exploitation primitives regardless of their underlying implementations.
To further evaluate the generality of ARCHEAP, we applied ARCHEAP to all custom heap allocators implemented for the DARPA CGC competitionsince many challenges share the implementation, we selected nine unique ones for our evaluation (see, Table 13).
We implemented a missing API, (i.e., malloc_usable_size()) to get the size of allocated objects and ran the experiment for 24 hours for each heap allocator.
Similar to the previous one, no specific model is provided.ARCHEAP found exploitation primitives for all of the tested allocators, except for NRFIN_00007, which implements page heap.Such allocator looks secure in terms of metadata corruption, but it is impractical due to its memory overheads causing internal fragmentation.
During this evaluation, we found two interesting results.
First, ARCHEAP found exploitation techniques for NRFIN_00032, which has a heap cookie to overflows.
Although this cookiebased protection is not bypassable via heap metadata corruption, ARCHEAP found that the implementation is vulnerable to an integer overflow and could craft two overlapping chunks without corrupting the heap cookie.
Second, ARCHEAP found the incorrect implementation of the allocator in CROMU_00004, which returns a chunk that is free or its size is larger than the request.
ARCHEAP successfully crafted a PoC code resulting in overlapping chunks by allocating a smaller chunk than the previous allocation.
This experiment indicates that our common heap designs are indeed universal even for in modern and custom heap allocators ( §2.1).
We also evaluated all search heuristics [63] supported by HeapHopper, which can be applied without exploit-specific information; for example, we exclude the strategy called ManualMergepoint, which requires an address in a binary to merge states.
As a result, we collected five search heuristics: DFS, which is the default mode of HeapHopper; Concretizer, which aggressively concretizes symbolic values to reduce the number of paths; Unique, which selects states according to their uniqueness for better coverage; Stochastic, which randomly selects the next states to explore; and Veritesting [5], which merges states to suppress path explosion combining static and dynamic symbolic execution.Unfortunately, as shown in Table 14, none of them was helpful in our evaluation; the default mode (DFS) shows the best performance.
First, these heuristics only help to mitigate, but cannot solve the fundamental problems of HeapHopper: path explosion and exponential growing combinations of transactions.
More seriously, they cannot exploit a concrete model from HeapHopper to alleviate the aforementioned issues unlike DFS.
This explains DFS's best performance and Stochastic's worst performance.
Veritesting failed due to its incorrect handling of undefined behaviors (e.g., NULL dereference) in merged states, which are common in our task assuming memory corruptions.
Figure A.7: A new exploitation technique that ARCHEAP found, named house of unsorted einherjar.
This is a variant of a known heap exploitation technique, house of einherjar, but it does not require a heap address unlike the old one.
We thank the anonymous reviewers for their helpful feedback.
This research was supported, in part, by the NSF award CNS-1563848, CNS-1704701, CRI-1629851 and CNS-1749711 ONR under grant N00014-18-1-2662, N00014-15-1-2162, N00014-17-1-2895, DARPA AIMEE, and ETRI IITP/KEIT [2014-3-00035], and gifts from Facebook, Mozilla, Intel, VMware and Google.
xChallenge Impacts of exploitation Challenge Impacts of exploitation
