Deep Neural Networks (DNNs) are fast becoming ubiquitous for their ability to attain good accuracy in various machine learning tasks.
A DNN's architecture (i.e., its hyper-parameters) broadly determines the DNN's accuracy and performance , and is often confidential.
Attacking a DNN in the cloud to obtain its architecture can potentially provide major commercial value.
Further, attaining a DNN's architecture facilitates other existing DNN attacks.
This paper presents Cache Telepathy: an efficient mechanism to help obtain a DNN's architecture using the cache side channel.
The attack is based on the insight that DNN inference relies heavily on tiled GEMM (Generalized Matrix Multiply), and that DNN architecture parameters determine the number of GEMM calls and the dimensions of the matrices used in the GEMM functions.
Such information can be leaked through the cache side channel.
This paper uses Prime+Probe and Flush+Reload to attack the VGG and ResNet DNNs running OpenBLAS and Intel MKL libraries.
Our attack is effective in helping obtain the DNN architectures by very substantially reducing the search space of target DNN architectures.
For example, when attacking the OpenBLAS library, for the different layers in VGG-16, it reduces the search space from more than 5.4 × 10 12 archi-tectures to just 16; for the different modules in ResNet-50, it reduces the search space from more than 6 × 10 46 architec-tures to only 512.
For the past several years, Deep Neural Networks (DNNs) have increased in popularity thanks to their ability to attain high accuracy and performance in a multitude of machine learning tasks -e.g., image and speech recognition [26,63], scene generation [45], and game playing [51].
An emerging framework that provides end-to-end infrastructure for using DNNs is Machine Learning as a Service (MLaaS) [2,19].
In MLaaS, trusted clients submit DNNs or training data to MLaaS service providers (e.g., an Amazon or Google datacenter).
Service providers host the DNNs, and allow remote untrusted users to submit queries to the DNNs for a fee.Despite its promise, MLaaS provides new ways to undermine the privacy of the hosted DNNs.
An adversary may be able to learn details of the hosted DNNs beyond the official query APIs.
For example, an adversary may try to learn the DNN's architecture (i.e., its hyper-parameters).
These are the parameters that give the network its shape, such as the number and types of layers, the number of neurons per layer, and the connections between layers.The architecture of a DNN broadly determines the DNN's accuracy and performance.
For this reason, obtaining it often has high commercial value.
Furthermore, once a DNN's architecture is known, other attacks are possible, such as the model extraction attack [55] (which obtains the weights of the DNN's edges), and the membership inference attack [39,49] (which determines whether an input was used to train the DNN).
Yet, stealing a DNN's architecture is challenging.
DNNs have a multitude of hyper-parameters, which makes bruteforce guesswork unfeasible.
Moreover, the DNN design space has been growing with time, which is further aggravating the adversary's task.This paper demonstrates that despite the large search space, attackers can quickly reduce the search space of DNN architectures in the MLaaS setting using the cache side channel.
Our insight is that DNN inference relies heavily on tiled GEMM (Generalized Matrix Multiply), and that DNN architecture parameters determine the number of GEMM calls and the dimensions of the matrices used in the GEMM functions.
Such information can be leaked through the cache side channel.We present an attack that we call Cache Telepathy.
It is the first cache side channel attack targeting modern DNNs on general-purpose processors (CPUs).
The reason for targeting CPUs is that CPUs are widely used for DNN inference in existing MLaaS platforms, such as Facebook's [25] and Amazon's [4].
We demonstrate our attack by implementing it on a stateof-the-art platform.
We use Prime+Probe and Flush+Reload to attack the VGG and ResNet DNNs running OpenBLAS and Intel MKL libraries.
Our attack is effective at helping obtain the architectures by very substantially reducing the search space of target DNN architectures.
For example, when attacking the OpenBLAS library, for the different layers in VGG-16, it reduces the search space from more than 5.4 × 10 12 architectures to just 16; for the different modules in ResNet-50, it reduces the search space from more than 6 × 10 46 Deep Neural Networks (DNNs) are a class of Machine Learning (ML) algorithms that use a cascade of multiple layers of nonlinear processing units for feature extraction and transformation [35].
There are several major types of DNNs in use today, two popular types being fully-connected neural networks (or multi-layer perceptrons) and Convolutional Neural Networks (CNNs).
The architecture of a DNN, also called the hyper-parameters, gives the network its shape.
DNN hyper-parameters considered in this paper are: a) Total number of layers.
b) Layer types, such as fully-connected, convolutional, or pooling layer.
c) Connections between layers, including sequential and nonsequential connections such as shortcuts.
Non-sequential connections exist in recent DNNs, such as ResNet [26].
For example, instead of directly using the output from a prior layer as the input to a later layer, a shortcut involves summing up the outputs of two prior layers and using the result as the input for a later layer.
d) Hyper-parameters for each layer.
For a fully-connected layer, this is the number of neurons in that layer.
For a convolutional layer, this is the number of filters, the filter size, and the stride size.
e) The activation function in each layer, e.g., relu and sigmoid.
The computation in each DNN layer involves many multiply-accumulate operations (MACCs) on input neurons.
The DNN weights, also called parameters, specify operands to these multiply-accumulate operations.In a fully-connected layer, each edge out of a neuron is a MACC with a weight; in a convolutional layer, each filter is a multi-dimensional array of weights, which is used as a sliding window that computes dot products over input neurons.DNN Usage DNNs usage has two distinct phases: training and inference.
In training, the DNN designer starts with a network architecture and a training set of labeled inputs, and tries to find the DNN weights to minimize mis-prediction error.Training is generally performed offline on GPUs and takes a relatively long time to finish, typically hours or days [12,25].
In inference, the trained model is deployed and used to make real-time predictions on new inputs.
For good responsiveness, inference is generally performed on CPUs [4,25].
To gain insight into the importance of DNN architectures, we discuss prior DNN privacy attacks [39,49,55,59].
There are three types of such attacks, each with a different goal.
All of them require knowing the victim's DNN architecture.
In the following, we refer to the victim's network as the oracle network, its architecture as the oracle DNN architecture, and its training data set as the oracle training data set.In the model extraction attack [55], the attacker tries to obtain a network that is close enough to the oracle network.
It assumes that the attacker knows the oracle DNN architecture at the start, and tries to estimate the weights of the oracle network.
The attacker creates a synthetic data set, requests the classification results from the oracle network, and uses such results to train a network that uses the oracle architecture.The membership inference attack [39,49] aims to infer the composition of the oracle training data set, which is expressed as the probability of whether a data sample exists in the training set or not.
This attack also requires knowledge of the oracle DNN architecture.
Attackers create multiple synthetic data sets and train multiple networks that use the oracle architecture.
Then, they run the inference algorithm on these networks with some inputs in their training sets and some not in their training sets.
They then compare the results to find the patterns in the output of the data in the training sets.
The pattern information is used to infer the composition of the oracle training set.
Specifically, given a data sample, they run the inference algorithm of the oracle network, obtain the output and check whether the output matches the pattern obtained before.
The more the output matches the pattern, the more likely the data sample exists in the oracle training set.The hyper-parameter stealing attack [59] steals the loss function and regularization term used in ML algorithms, in-cluding DNN training and inference.
This attack also relies on knowing the oracle DNN architecture.
During the attack, attackers leverage the model extraction attack to learn the DNN's weights.
They then find the loss function that minimizes the training misprediction error.
In a cache-based side channel attack, the attacker infers a secret from the victim by observing the side effects of the victim's cache behavior.
Recently, multiple variations of cache-based side channel attacks have been proposed.
Flush+Reload [69] and Prime+Probe [38,43] are two powerful ones.
Flush+Reload requires that the attacker share security-sensitive code or data with the victim.
This sharing can be achieved by leveraging the page de-duplication technique.
In an attack, the attacker first performs a clflush operation to the shared cache line, to push it out of the cache.
It then waits to allow the victim to execute.
Finally, it reaccesses the same cache line and measures the access latency.
Depending on the latency, it learns whether the victim has accessed the shared line.Prime+Probe does not require page sharing.
It is more practical than Flush+Reload as most cloud providers disable page de-duplication for security purposes [58].
The attacker constructs a collection of addresses, called conflict addresses, which map to the same cache set as the victim's line.
In an attack, the attacker first accesses the conflict addresses to cause cache conflicts with the victim's line, and evict it from the cache.
After waiting for an interval, it re-accesses the conflict addresses and measures the access latency.
The latency is used to infer if the victim has accessed the line.
This paper develops a cache-timing attack that quickly reduces the search space of DNN architectures.
The attack relies on the following standard assumptions.Black-box Access We follow a black-box threat model in an MLaaS setting similar to [55].
In a black-box attack, the DNN model is only accessible to attackers via an official query interface.
Attackers do not have prior knowledge about the target DNN, including its hyper-parameters, weights and training data.Co-location We assume that the attacker process can use techniques from prior work [7,8,14,46,57,66,73] to co-locate onto the same processor chip as the victim process running DNN inference.
This is feasible, as current MLaaS jobs are deployed on shared clouds.
Note that recent MLaaS, such as Amazon SageMaker [3] and Google ML Engine [18] allow users to upload their own code for training and inference, instead of using pre-defined APIs.
In this case, attackers can disguise themselves as an MLaaS process and the cloud scheduler will have difficulty in separating attacker processes from victim processes.Code Analysis We also assume that the attacker can analyze the ML framework code and linear algebra libraries used by the victim.
These are realistic assumptions.
First, open-source ML frameworks are widely used for efficient development of ML applications.
The frameworks supported by Google, Amazon and other companies, including Tensorflow [1], Caffe [32], and MXNet [6] are all public.
Our analysis is applicable to almost all of these frameworks.
Second, the frameworks' backends are all supported by high-performance and popular linear algebra libraries, such as OpenBLAS [64], Eigen [23] and MKL [60].
OpenBLAS and Eigen are open sourced, and MKL can be reverse engineered, as we show in Section 6.
The goal of Cache Telepathy is to substantially reduce the search space of target DNN architectures.
In this section, we first discuss how our attack can assist other DNN privacy attacks, and then give an overview of the Cache Telepathy attack procedure.Cache Telepathy's Role in Existing DNN Attacks In settings where DNN architectures are not known, our attack can serve as an essential initial step for many existing DNN privacy attacks, including model extraction attacks [55] and membership inference attacks [49].
Figure 1 demonstrates how Cache Telepathy makes the model extraction attack feasible.
The final goal of the model extraction attack is to obtain a network that is close enough to the oracle network (Section 2.2).
The attack uses the following steps.
First, the attacker generates a synthetic training data set (x).
This step can be achieved using a random feature vector method [55] or more sophisticated techniques, such as hill-climbing [49].
Next, the attacker queries the oracle network via inference APIs provided by MLaaS providers to get labels or confidence values (y).
The synthetic data set and corresponding query results will be used as training data and labels later.
In the case that the oracle architecture is not known, the attacker needs to choose a DNN architecture from a search space (z) and then train a network with the chosen architecture ({).
Steps z-{ repeat until a network is found with sufficient prediction accuracy (|).
This attack process is extremely compute intensive, since it involves many iterations of step {.
Considering the depth and complexity of state-of-the-art DNNs, training and validating each network can take hours to days.
Moreover, without any information about the architecture, the search space of possible architectures is often intractable, and thus, the model extraction attack is infeasible.
However, Cache Telepathy can reduce the architecture search space (z) to a tractable size and make the attack feasible in settings where DNN architectures are unknown.Membership inference attacks suffer from a more serious problem if the DNN architecture is not known.
Recall that the attack aims to figure out the composition of the oracle training data set (Section 2.2).
If there are many different candidate architectures, the attacker needs to consider the results generated by all the candidate architectures and statistically summarize inconsistent results from those architectures.
A large search space of candidate architectures, not only significantly increases the computation requirements, but also potentially hurts attack accuracy.
Consider a candidate architecture which is very different from the oracle architecture.
It is likely to contribute incorrect results, and in turn, decrease the attack accuracy.
However, Cache Telepathy can reduce the search space to a reasonable size.
Moreover, the candidate architectures in the reduced search space have the same or very similar hyper-parameters as the oracle network.
Therefore, they perform very similarly to the oracle network on various data sets.
Hence, our attack also plays an important role in membership inference attacks.Overall Cache Telepathy Attack Procedure Our attack is based on two observations.
First, DNN inference relies heavily on GEMM (Generalized Matrix Multiply).
We conduct a detailed analysis of how GEMM is used in ML frameworks, and figure out the mapping between DNN hyper-parameters and matrix parameters (Section 4).
Second, high-performance GEMM algorithms are vulnerable to cache-based side channel attacks, as they are all tuned for the cache hierarchy through matrix blocking (i.e., tiling).
When the block size is public (or can be easily deduced), the attacker can use the cache side channel to count blocks and learn the matrix sizes.The Cache Telepathy attack procedure includes a cache attack and post processing steps.
First, it uses a cache attack to monitor matrix multiplications and obtain matrix parameters (Sections 5 and 6).
Then, the DNN architecture is reverse-engineered based on the mapping between DNN hyper-parameters and matrix parameters (Section 4).
Finally, Cache Telepathy prunes the possible values of the remaining undiscovered hyper-parameters and generates a pruned search space for the target DNN architecture (Section 8.3).
We consider the attack to be successful if we can generate a reasonable number of candidate architectures whose hyperparameters are the same or very similar to the oracle network.
DNN hyper-parameters, listed in Section 2.1, can be mapped to GEMM execution.
We first discuss how the layer type and configurations within each layer map to matrix parameters, assuming that all layers are sequentially connected (Section 4.1 and 4.2).
We then generalize the mapping by showing how the connections between layers map to GEMM execution (Section 4.3).
Finally, we discuss what information is required to extract the activation functions of Section 2.1 (Section 4.4).
There are two types of neural network layers whose computation can be mapped to matrix multiplications, namely fully-connected and convolutional layers.
In a fully-connected layer, each neuron computes a weighted sum of values from all the neurons in the previous layer, followed by a non-linear transformation.
The ith layer computes out i = f i (in i ⊗ θ i ) where in i is the input vector, θ i is the weight matrix, ⊗ denotes a matrix-vector operation, f is an element-wise non-linear function such as tanh or sigmoid, and out i is the resulting output vector.The feed-forward computation of a fully-connected DNN can be performed over a batch of a few inputs at a time (B).
These multiple input vectors are stacked into an input matrix In i .
A matrix multiplication between the input matrix and the weight matrix (θ i ) produces an output matrix, which is a stack of output vectors.
We represent the computation asO i = f i (In i · θ i )where In i is a matrix with as many rows as B and as many columns as N i (the number of neurons in the layer i); O i is a matrix with as many rows as B and as many columns as N i+1 (the number of neurons in the layer i + 1); and θ i is a matrix with N i rows and N i+1 columns.
Table 1 shows the number of rows and columns of all the matrices.
Input:In i B N i Weight: θ i N i N i+1 Output: O i B N i+1Table 1: Matrix sizes in a fully-connected layer.
In a convolutional layer, a neuron is connected to only a spatial region of neurons in the previous layer.
Consider the upper row of Figure 2, which shows the computation in the ith layer.
The layer generates an output out i (right part of the upper row) by performing convolution operations on an input in i (center of the upper row) with multiple filters (left part of the upper row).
The input volume in i is of sizeW i × H i × D i ,where the depth (D i ) also refers to the number of channels of the input.
Each filter is of sizeR i × R i × D i .
H i W i D i H i+1 W i+1 D i+1 R i R i R i filters filter0 x = R i 2 D i channel0 (W i -R i +P i )(H i -R i +P i ) filter1 …….
.
…….
.
channel1 …….
.
…….
.
① ② ③ ④ in i out i D i F' i in' i out' i R i D i D i+1Figure 2: Mapping a convolutional layer (upper part of the figure) to a matrix multiplication (lower part).
To see how a convolution operation is performed, the figure highlights the process of generating one output neuron in out i .
The neuron is a result of a convolution operation -an elementwise dot product of the filter shaded in dots and the subvolume in in i shaded in dashes.
Both the subvolume and the filter have dimensions R i × R i × D i .
Applying one filter on the entire input volume (in i ) generates one channel of the output (out i ).
Thus, the number of filters in layer i (D i+1 ) is the number of channels (depth) in the output volume.The lower row of Figure 2 shows a common implementation that transforms the multiple convolution operations in a layer into a single matrix multiply.
First, as shown in arrow x, each filter is stretched out into a row to form a matrix F i .
The number of rows in F i is the number of filters in the layer.
Second, as shown in arrow y, each subvolume in the input volume is stretched out into a column.
(W i − R i + P i )(H i − R i + P i ), which is the size of one output channel, namely, W i+1 × H i+1 .
Table 2 shows the number of rows and columns of the matrices involved.
Table 2: Matrix sizes in a convolutional layer.Matrix n_row n_col in i D i × R 2 i (W i − R i + P i )(H i − R i + P i ) F i D i+1 D i × R 2 i out i D i+1 (W i − R i + P i )(H i − R i + P i ) = W i+1 × H i+1The matrix multiplication described above processes a single input.
As with fully-connected DNNs, CNN inference can consume a batch of B inputs in a single forward pass.
In this case, a convolutional layer performs B matrix multiplications per pass.
This is different from fully-connected layers, where the entire batch is computed using only one matrix multiplication.
Based on the previous analysis, we can now map DNN hyperparameters to matrix operation parameters assuming all layers are sequentially connected.
Consider a fully-connected network.
Its hyper-parameters are the number of layers, the number of neurons in each layer (N i ) and the activation function per layer.
As discussed in Section 4.1, the feed-forward computation performs one matrix multiplication per layer.
Hence, we extract the number of layers by counting the number of matrix multiplications performed.
Moreover, according to Table 1, the number of neurons in layer i (N i ) is the number of rows of the layer's weight matrix (θ i ).
The first two rows of Table 3 Stride i+1 width and height Table 3: Mapping between DNN hyper-parameters and matrix parameters.
FC stands for fully connected.
A convolutional network generally consists of four types of layers: convolutional, Relu, pooling, and fully connected.
Recall that each convolutional layer involves a batch B of matrix multiplications.
Moreover, the B matrix multiplications that correspond to the same layer, always have the same dimension sizes and are executed consecutively.
Therefore, we can count the number of consecutive matrix multiplications which have the same computation pattern to determine B.In a convolutional layer i, the hyper-parameters include the number of filters (D i+1 ), the filter width and height (R i ), and the padding (P i ).
We assume that the filter width and height are the same, which is the common case.
Note that for layer i, we consider that the depth of the input volume (D i ) is known, as it can be obtained from the previous layer.We now show how these parameters for a convolutional layer can be reverse engineered.
From Table 2 Table 3, the filter width is attained by dividing the number of rows of in i by the number of rows of out i−1 and performing the square root.
In the case that layer i is the first one, directly connected to the input, the denominator (out 0 ) of this fraction is the number of channels of the input of the network, which is public information.Padding results in a larger input matrix (in i ).
After resolving the filter width (R i ), the value of padding can be deduced by determining the difference between the number of columns of the output matrix of layer i − 1 (out i−1 ), which is W i × H i , and the number of columns of the in i matrix, which is(W i − R i + P)(H i − R i + P).
A pooling layer can be located in-between two convolutional layers.
It down-samples every channel of the input along width and height, resulting in a small channel size.
The hyper-parameter in this layer is the pool width and height (assumed to be the same value), which can be inferred as follows.
Consider the channel size of the output of layer i (number of columns in out i ) and the channel size of the input volume in layer i + 1 (approximately equals to the number of columns in in i+1 ).
If the two are the same, there is no pooling layer; otherwise, we expect to see the channel size reduced by the square of the pool width.
In the latter case, the exact pool dimension can be found using a similar procedure used to determine R i .
Note that a non-unit stride operation results in the same dimension reduction as a pooling layer.
Thus, we cannot distinguish between non-unit striding and pooling.
Table 3 summarizes the mappings.
We now examine how to map inter-layer connections to GEMM execution.
We consider two types of inter-layer connections, i.e., sequential connections and non-sequential connections.
A sequential connection is one that connects two consecutive layers, e.g., layer i and layer i+1.
The output of layer i is used as the input of its next layer i + 1.
According to the mapping relationships in Table 3, a DNN places several constraints on GEMM parameters for sequentially-connected convolutional layers.First, since the filter width and height must be integer values, there is a constraint on the number of rows of the input and output matrices in consecutive layers.
Considering the formula used to derive the filter width and height in Table 3, if layer i and layer i + 1 are connected, the number of rows in the input matrix of layer i + 1 (n_row(in i+1 )) must be the product of the number of rows in the output matrix of layer i (n_row(out i )) and the square of an integer number.
Second, since the pool size and stride size are integer values, there is another constraint on the number of columns of the input and output matrix sizes between consecutive layers.
According to the formula used to derive pool and stride size, if layer i and layer i + 1 are connected, the number of columns in the output matrix of layer i (n_col(out i )) must be very close to the product of the number of columns in the input matrix of layer i + 1 (n_col(in i+1 )) and the square of an integer number.
The two constraints above help us to distinguish nonsequential connections from sequential ones.
Specifically, if one of these constraints is not satisfied, we are sure that the two layers are not sequentially connected.
In this paper, we consider that a non-sequential connection is one where, given two consecutive layers i and i + 1, there is a third layer j, whose output is merged with the output of layer i and the merged result is used as the input to layer i + 1.
We call the extra connection from layer j to layer i + 1 a shortcut, where layer j is the source layer and layer i + 1 is the sink layer.
Shortcut connections can be mapped to GEMM execution.First, there exists a certain latency between consecutive GEMMs, which we call inter-GEMM latency.
The inter-GEMM latency before the sink layer in a non-sequential connection is longer than the latency in a sequential connection.
To see why, consider the operations that are performed between two consecutive GEMMs: post-processing of the prior GEMM's output (e.g., batch normalization) and pre-processing of the next GEMM's input (e.g., padding and striding).
When there is no shortcut, the inter-GEMM latency is linearly related to the sum of the prior layer's output size and the next layer's input size.
However, a shortcut requires an extra merge operation that incurs extra latency between GEMM calls.
Second, the source layer of a shortcut connection must have the same output dimensions as the other source layer of the non-sequential connection.
For example, when a shortcut connects layer j and layer i + 1, the output matrices of layer j and layer i must have the same number of rows and columns.
This is because one can only merge two outputs whose dimension sizes match.These two characteristics help us identify the existence of a shortcut, its source layer, and its sink layer.
So far, this section discussed how DNN parameters map to GEMM calls.
Convolutional and fully-connected layers are post-processed by elementwise non-linear functions, such as relu, sigmoid and tanh, which do not appear in GEMM parameters.
We can distinguish relu activations from sigmoid and tanh by monitoring whether the non-linear functions access the standard mathematical library libm.
relu is a simple activation which does not need support from libm, while the other functions are computationally intensive and generally leverage libm to achieve high performance.
We remark that nearly all convolutional layers use relu or a close variant [26,33,52,53,65].
We now design a side channel attack to learn matrix multiplication parameters.
Given the mapping from the previous section, this attack will allow us to reconstruct the DNN architecture.We analyze state-of-the-art BLAS libraries, which have extensively optimized blocked matrix multiply.
Examples of such libraries are OpenBLAS [64], BLIS [56], Intel MKL [60] and AMD ACML [5].
We show in detail how to extract the desired information from the GEMM implementation in OpenBLAS.
In Section 6, we generalize our attack to other BLAS libraries, using Intel MKL as an example.
Like most modern BLAS libraries, OpenBLAS implements Goto's algorithm [20].
The algorithm has been optimized for modern multi-level cache hierarchies.
Figure 3 depicts the way Goto's algorithm structures blocked matrix multiplication for a three-level cache.
The macro-kernel at the bottom performs the basic operation, multiplying a P × Q block from matrix A with a Q × R block from matrix B.
This kernel is generally written in assembly code, and manually optimized by taking the CPU pipeline structure and register availability into consideration.
The block sizes are picked so that the P × Q block of A fits in the L2 cache, and the Q × R block of B fits in the L3 cache.
As shown in Figure 3, there is a three-level loop nest around the macro-kernel.
The innermost one is Loop 3, the intermediate one is Loop 2, and the outermost one is Loop 1.
We call the iteration counts in these loops iter 3 , iter 2 , and iter 1 , respectively, and are given by: iter 3 = m/P iter 2 = k/Q iter 1 = n/R (1)Algorithm 1 shows the corresponding pseudo-code with the three nested loops.
Note that Loop 3 is further split into two parts, to obtain better cache locality.
The first part performs only the first iteration, and the second part performs the rest.The first iteration of Loop 3 (Lines 3-7) performs three steps as follows.
First, the data in the P × Q block from matrix A is packed into a buffer (bufferA) using function itcopy.
This is shown in Figure 3 as arrow x and corresponds to line 3 in Algorithm 1.
Second, the data in the Q × R block from matrix B is also packed into a buffer (bufferB) using function oncopy.
This is shown in Figure 3 given by:iter 4 = R/3UNROLL or iter 4 = (n mod R)/3UNROLL(2)where the second expression corresponds to the last iteration of Loop 1.
Note that bufferB, which is filled by the first iteration of Loop 3, is also shared by the rest of iterations.
Third, the macro-kernel (function kernel) is executed on the two buffers.
This corresponds to line 6 in Algorithm 1.
The rest iterations (line 8-11) skip the second step above.
These iterations only pack a block from matrix A to fill bufferA and execute the macro-kernel.
The BLAS libraries use different P, Q, and R for different cache sizes to achieve best performance.
For example, when compiling OpenBLAS on our experimental machine (Section 7), the GEMM function for double data type uses P = 512; Q = 256, R = 16384, and 3UNROLL = 24.
Our goal is to find the size of the matrices of Figure 3, namely, m, k, and n. To do so, we need to first obtain the number of iterations of the 4 loops in Algorithm 1, and then use Formulas 1 and 2.
Note that we know the values of the block sizes P, Q, and R (as well as 3UNROLL) -these are constants available in the open-source code of OpenBLAS.In this paper, we propose to use, as probing addresses, addresses in the itcopy, oncopy and kernel functions of Algorithm 1.
To understand why, consider the dynamic invocations to these functions.
Figure 4 shows the Dynamic Call Graph (DCG) of gemm_nn in Algorithm 1.
Each iteration of Loop 2 contains one invocation of function itcopy, followed by iter 4 invocations of the pair oncopy and kernel, and then (iter 3 − 1) invocations of the pair itcopy and kernel.
The whole sequence in Figure 4 is executed iter 1 × iter 2 times in one invocation of gemm_nn.
We will see in Section 5.3 that these invocation counts are enough to allow us to find the size of the matrices.
We now discuss how to select probing addresses inside the three functions-itcopy, oncopy and kernel-to improve attack accuracy.
The main bodies of the three functions are loops.
To distinguish these loops from the GEMM loops, we refer to them in this paper as in-function loops.
We select addresses that are located inside the in-function loops as probing addresses.
This strategy helps improve attack accuracy, because such addresses are accessed multiple times per function invocation and their access patterns can be easily distinguished from noise (Section 8.1).
To understand the procedure we use to extract matrix dimensions, we show an example in Figure 5(a), which visualizes the execution time of a gemm_nn where Loop 1, Loop 2 and Loop 3 have 5 iterations each.
The figure also shows the size of the block that each iteration operates on.
Note that the OpenBLAS library handles the last two iterations of each loop in a special manner.
When the last iteration does not have a full block to compute, rather than assigning a small block to the last iteration, it assigns two equal-sized small blocks to the last two iterations.
In Figure 5(a), in Loop 1, the first three iterations use R-sized blocks, and each of the last two use a block of size (R + n mod R)/2.
In Loop 2, the corresponding block sizes are Q and (Q + k mod Q)/2.
In Loop 3, they are P and (P + m mod P)/2.
During the execution of the first three iterations of Loop 1, iter 4 is R/3UNROLL.
In the last two iterations of Loop 1, iter 4 is ((R + n mod R)/2)/3UNROLL, as can be deduced from Equation 2 after applying OpenBLAS' special handling of the last two iterations.Based on these insights, our procedure to extract m, k, and n has four steps.Step 1: Identify the DCG of a Loop 2 iteration and extract iter 1 × iter 2 .
By probing one instruction in each of itcopy, oncopy, and kernel, we repeatedly obtain the DCG pattern of a Loop 2 iteration (Figure 4).
By counting the number of such patterns, we obtain iter 1 × iter 2 .
Step 2: Extract iter 3 and determine the value of m.
In the DCG pattern of a Loop 2 iteration, we count the number of invocations of the itcopy-kernel pair (Figure 4).
This count plus 1 gives iter 3 .
Of all of these iter 3 iterations, all but the last two execute a block of size P; the last two execute a block of size (P + m mod P)/2 each ( Figure 5(a)).
To estimate the size of this smaller block, we assume that the execution time of an iteration is proportional to the block size it processesexcept for the first iteration which, as we indicated, is different.
Hence, we time the execution of a "normal" iteration of Loop 3 and the execution of the last iteration of Loop 3.
Let's call the times t normal and t small .
The value of m is computed by adding P for each of the (iter 3 -2) iterations and adding the estimated number for each of the last two iterations:m = (iter 3 − 2) × P + 2 × t small t normal × PStep 3: Extract iter 4 and iter 2 , and determine the value of k.
In the DCG pattern of a Loop 2 iteration (Figure 4), we count the number of oncopy-kernel pairs, and obtain iter 4 .
As shown in Figure 5(b), the value of iter 4 is R/3UNROLL in all iterations of Loop 2 except those that are part of the last two iterations of Loop 1.
For the latter, iter 4 is ((R + n mod R)/2)/3UNROLL, which is a lower value.
Consequently, by counting the number of DCG patterns that have a low value of iter 4 , and dividing it by 2, we attain iter 2 .
We then follow the procedure of Step 2 to calculate k. Specifically, all Loop 2 iterations but the last two execute a block of size Q; the last two execute a block of size (Q + k mod Q)/2 each ( Figure 5(a)).
Hence, we time the execution of two iterations of Loop 2 in the first Loop 1 iteration: a "normal" one (t normal ) and the last one (t small ).
We then compute k like in Step 2:k = (iter 2 − 2) × Q + 2 × t small t normal × QStep 4: Extract iter 1 and determine the value of n.
If we take the total number of DCG patterns in the execution from Step 1 and divide that by iter 2 , we obtain iter 1 .
We know that all Loop 1 iterations but the last two execute a block of size R; the last two execute a block of size (R + n mod R)/2 each.
To compute the size of the latter block, we note that, in the last two iterations of Loop 1, iter 4 is ((R + n mod R)/2)/3UNROLL.
Since both iter 4 and 3UNROLL are known, we can estimate (R + n mod R)/2.
We neglect the effect of the ceiling operator because 3UNROLL is a very small number.
Hence, we compute n as:n = (iter 1 − 2) × R + 2 × iter 4 × 3UNROLLOur attack cannot handle the cases when m or k are less than or equal to twice their corresponding block sizes.
For example, when m is less than or equal to 2 × P, there is no iteration of Loop 3 that operates on a smaller block size.
Our procedure cannot compute the exact value of m, and can only say that m ≤ 2P.
Our attack can be generalized to other BLAS libraries, since all of them use blocked matrix-multiplication, and most of them implement Goto's algorithm [20].
We show that our attack is still effective, using the Intel MKL library as an example.
MKL is a widely used library but is closed source.
We reverse engineer the scheduling of the three-level nested loop in MKL and its block sizes.
The information is enough for us to apply the same attack procedure in Section 5.3 to obtain matrix dimensions.Constructing the DCG We apply binary analysis [41,73] techniques to construct the DCG of the GEMM function in MKL, shown in Figure 6.
The pattern is the same as the DCG of OpenBLAS in Figure 4.
Thus, the attack strategy in Section 5 also works towards MKL.
and iteration count for each of the loops, as shown in Formulas 1 and 2.
When the matrix size increases by a block size, the corresponding iteration count increases by 1.
We leverage this relationship to reverse engineer the block sizes for MKL.
Specifically, we gradually increase the input dimension size until the number of iterations increments.
The stride on the input dimension that triggers the change of iteration count is the block size.Special Cases According to our analysis, MKL follows a different DCG when dealing with small matrices.
First, instead of executing three-level nested loops, it uses a singlelevel loop, tiling on the dimension that has the largest value among m, n, k. Second, the kernel computation is performed directly on the input matrices, without packing and buffering operations.
For these special cases, we slightly adjust the attack strategy in Figure 5.
We use side channels to monitor the number of iterations on that single-level loop and the time spent for each iteration.
We then use the number of iterations to deduce the size of the largest dimension.
Finally, we use the timing information for each iteration to deduce the product of the other two dimensions.
Attack Platform We evaluate our attacks on a Dell workstation Precision T1700, which has a 4-core Intel Xeon E3 processor and an 8GB DDR3-1600 memory.
The processor has two levels of private caches and a shared last level cache.
The first level caches are a 32KB instruction cache and a 32KB data cache.
The second level cache is 256KB.
The shared last level cache is 8MB.
We test our attacks on a same-OS scenario using Ubuntu 4.2.0-27, where the attacker and the victim are different processes within the same bare-metal server.
Our attacks should be applicable to other platforms, as the effectiveness of Flush+Reload and Prime+Probe has been proved in multiple hardware platforms [38,69].
Victim DNNs We use a VGG [52] instance and a ResNet [26] instance as victim DNNs.
VGG is representative of early DNNs (e.g., AlexNet [33] and LeNet [34]).
ResNet is representative of state-of-the-art DNNs.
Both are standard and widely-used CNNs with a large number of layers and hyper-parameters.
ResNet additionally features shortcut connections.There are several versions of VGG, with 11 to 19 layers.
All VGGs have 5 types of layers, which are replicated a different number of times.
We show our results on VGG-16.
There are several versions of ResNet, with 18 to 152 layers.
All of them consist of the same 4 types of modules, which are replicated a different number of times.
Each module contains 3 or 4 layers, which are all different.
We show our results on ResNet-50.
The victim programs are implemented using the Keras [10] framework, with Theano [54] as the backend.
We execute each DNN instance with a single thread.Attack Implementation We use Flush+Reload and Prime+Probe attacks.
In both attacks, the attacker and the victim are different processes and are pinned to different cores, only sharing the last level cache.In Flush+Reload, the attacker and the victim share the BLAS library via page de-duplication.
The attacker probes one address in itcopy and one in oncopy every 2,000 cycles.
There is no need to probe any address in kernel, as the access pattern is clear enough.
Our Prime+Probe attack targets the last level cache.
We construct two sets of conflict addresses for the two probing addresses using the algorithm proposed by Liu et al. [38].
The Prime+Probe uses the same monitoring interval length of 2,000 cycles.
We first evaluate our attacks on the GEMM function.
We then show the effectiveness of our attack on neural network inference, followed by an analysis of the search space of DNN architectures.
Figure 7 shows raw traces generated by Flush+Reload and Prime+Probe when monitoring the execution of the GEMM function in OpenBLAS.
Due to space limitations, we only show the traces for one iteration of Loop 2 (Algorithm 1).
Figure 7(a) is generated under Flush+Reload.
It shows the latency of the attacker's reload accesses to the probing addresses in the itcopy and oncopy functions for each monitoring interval.
In the figure, we only show the instances where the access took less than 75 cycles.
These instances correspond to cache hits and, therefore, cases when the victim executed the corresponding function.
Figure 7(b) is generated under Prime+Probe.
It shows the latency of the attacker's probe accesses to the conflict addresses.
We only show the instances where the accesses took more than 500 cycles.
These instances correspond to cache misses of at least one conflict address.
They are the cases when the victim executed the corresponding function.
Since we select the probing addresses to be within in-function loops (Section 5.2), a cluster of hits in the Flush+Reload trace (or misses in the Prime+Probe trace) indicates the time period when the victim is executing the probed function.
In both traces, the victim calls itcopy before interval 2,000, then calls oncopy 11 times between intervals 2,000 and 7,000.
It then calls itcopy another two times in intervals 7,000 and 13,000.
The trace matches the DCG shown in Figure 4.
We can easily derive that iter 4 = 11 and iter 3 = 3.
Comparing the two traces in Figure 7, we can observe that Prime+Probe suffers much more noise than Flush+Reload.
The noise in Flush+Reload is generally sparsely and randomly distributed, and thus can be easily filtered out.
However, Prime+Probe has noise in consecutive monitoring intervals, as shown in Figure 7(b).
It happens mainly due to the nondeterminism of the cache replacement policy [13].
When one of the cache ways is used by the victim's line, it takes multiple "prime" operations to guarantee that the victim's line is selected to be evicted.
It is more difficult to distinguish the victim's accesses from such noise.We leverage our knowledge of the execution patterns in GEMM to handle the noise in Prime+Probe.
First, recall that we pick the probing addresses within tight loops inside each of the probing functions (Section 5.2).
Therefore, for each invocation of the functions, the corresponding probing address is accessed multiple times, which is observed as a cluster of cache misses in Prime+Probe.
We count the number of consecutive cache misses in each cluster to obtain its size.
The size of a cluster of cache misses that are due to noise is smaller than size of a cluster of misses that are caused by the victim's accesses.
Thus, we discard the clusters with small sizes.
Second, due to the three-level loop structure, each probing function, such as oncopy, is called repetitively with consistent interval lengths between each invocation (Fig- ure 4).
Thus, we compute the distances between neighboring clusters and discard the clusters with abnormal distances to their neighbors.These two steps are effective enough to handle the noise in Prime+Probe.
However, when tracing MKL's special cases that use a single-level loop (Section 6), we find that using Prime+Probe is ineffective to obtain useful information.
Such environment affects the accuracy of the Cache Telepathy attack, as we will see in Section 8.3.
We show the effectiveness of our attack by extracting the hyper-parameters of VGG-16 [52] and ResNet-50 [26].
The figures show three data points for each parameter (e.g., m) and each layer (e.g., L1 in ResNet-M2): a hollowed circle, a solid square or rectangle, and a solid circle.
The hollowed circle indicates the actual value of the parameter.
The solid square or rectangle indicates the value of the parameter de-tected with our side channel attack.
When the side channel attack can only narrow down the possible value to a range, the figure shows a rectangle.
Finally, the solid circle indicates the value of the parameter that we deduce, based on the detected value and some DNN constraints.
For example, for parameter m in layer L1 of ResNet-M2, the actual value is 784, the detected value range is [524,1536], and the deduced value is 784.
We will discuss how we obtain the solid circles later.
Here, we compare the actual and the detected values (hollowed circles and solid squares/rectangles).
Figure 8(a) shows that our attack is always able to determine the n value with negligible error.
The reason is that, to compute n, we need to estimate iter 1 and iter 4 (Section 5.3), and it can be shown that most of the noise comes from estimating iter 4 .
However, since iter 4 is multiplied by the small 3UNROLL parameter in the equation for n, the impact of such noise is small.Figures 8(b) and (c) show that the attack is able to accurately determine the m and k values for all the layers in ResNet-M1 and VGG, and for most layers in ResNet-M4.
However, it can only derive ranges of values for most of the ResNet-M2 and ResNet-M3 layers.
This is because the m and k values in these layers are often smaller than twice of the corresponding block sizes (Section 5.3).
In Figure 9, we show the same set of results by analyzing the traces generated using Prime+Probe.
Compared to the results from Flush+Reload, there are some differences of detected values or ranges, especially in ResNet-M3 and ResNet-M4.
In summary, our side channel attacks, using Flush+Reload or Prime+Probe, can either detect the matrix parameters with negligible error, or can provide a range where the actual value falls in.
We will next show that, in many cases, the imprecision from the negligible error and the ranges can be eliminated after applying DNN constraints (Section 8.3.2).
In this section, we compare the number of architectures in the search space without Cache Telepathy (which we call original space), and with Cache Telepathy (which we call reduced space).
In both cases, we only consider reasonable hyperparameters for the layers as follows.
For fully-connected layers, the number of neurons can be 2 i , where 8 ≤ i ≤ 13.
For convolutional layers, the number of filters can be a multiple of 64 (64 × i, where 1 ≤ i ≤ 32), and the filter size can be an integer value between 1 and 11.
To be conservative, when computing the size of the original search space, we assume that the attacker knows the number of layers and type of each layer in the oracle DNN.
There exist 352 different configurations for each convolutional layer without considering pooling or striding, and 6 configurations for each fully-connected layer.
Moreover, considering the existence of non-sequential connections, given L layers, there are L × 2 L−1 possible ways to connect them.A network like VGG-16 has five different layers (B1, B2, B3, B4, and B5), and no shortcuts.
If we consider only these five different layers, the size of the search space is about 5.4 × 10 12 candidate architectures.
A network like ResNet-50 has 4 different modules (M1, M2, M3, and M4) and some shortcuts inside these modules.
If we consider only these four different modules, the size of the search space is about 6 × 10 46 candidate architectures.
Overall, the original search space is intractable.
Using the detected values of the matrix parameters in Section 8.2, we first determine the possible connections between layers by locating shortcuts.
Next, for each possible connection configuration, we calculate the possible hyper-parameters for each layer.
The final search space is computed assearch space = C ∑ i=1 ( L ∏ j=1 x j ) (3)where C is the total number of possible connection configurations, L is the total number of layers, and x j is the number of possible combinations of hyper-parameters for layer j.Determining Connections Between Layers We show how to reverse engineer the connections between layers using ResNet-M1 as an example.First, we leverage inter-GEMM latency to determine the existence of shortcuts and their sinks using the method discussed in Section 4.3.
Figure 10 shows the extracted matrix dimensions and the inter-GEMM latency for the 4 layers in ResNet-M1.
The inter-GEMM latency after M1-L4 is significantly longer than expected, given its output matrix size and the input matrix size of the next layer.
Thus, the layer after M1-L4 is a sink layer.
Next, we check the output matrix dimensions of previous layers to locate the source of the shortcut.
Note that a shortcut only connects layers with the same output matrix dimensions.
Based on the extracted dimension information (values of n and m) in Figure 8, we determine that M1-L1 is the source.
In addition, we know that M1-L1 and M1-L2 are not sequentially connected by comparing the output matrix of M1-L1 and the input matrix of M1-L2 (Section 4.3).
Figure 10 summarizes the reverse engineered connections among the 4 layers, which match the actual connections in ResNet-M1.
We can use the same method to derive the possible connection configurations for the other modules.
Note that this approach does not work for ResNet-M3 and ResNet-M4.
In these layers, the input and output matrices are small and operations between consecutive layers take a short time.
As a result, the inter-GEMM latency is not effective in identifying shortcuts.M1-L1 M1-L2 M1-L3 M1-L4Determining Hyper-parameters for Each Layer We plug the detected matrix parameters into the formulas in Table 3 to deduce the hyper-parameters for each layer.
For the matrix dimensions that cannot be extracted precisely, we leverage DNN constraints to prune the search space.As an example, consider reverse engineering the hyperparameters for Layer 3 in ResNet-M2.
First, we extract the number of filters.
We round the extracted n M2-L3 from Fig- ure 8(a) (the number of rows in F ) to the nearest multiple of 64.
This is because, as discussed at the beginning of Section 8.3, we assume that the number of filters is a multiple of 64.
We get that the number of filters is 512.
Second, we use the formula in Table 3 to determine the filter width and height.
We consider the case where L2 is sequentially connected to L3.
The extracted range of k M2-L3 from Figure 8(b) (the number of rows in in of current layer) is [68,384], and the value for n M2-L2 from Figure 8(a) (the number of rows in out of the previous layer) is 118.
We need to make sure that the square root of k M2-L3 /n M2-L2 is an integer, which leads to the conclusion that the only possible value for k M2-L3 is 118 (one of the solid circles for k M2-L3 ), and the filter width and height is 1.
The same value is deduced if we consider, instead, that L1 is connected to L3.
The other solid circle for k M2-L3 is derived similarly if we consider that the last layer in M1 is connected to layer 3 in M2.We apply the same methodology for the other layers.
With this method, we obtain the solid circles in Figures 8 and 9.
Determining Pooling and Striding We use the difference in the m dimension (i.e., the channel size of the output) between consecutive layers to determine the pool or stride size.
For example, in Figure 8(c) and 9(c), the m dimensions of the last layer in ResNet-M1 and the first layer in ResNet-M2 are different.
This difference indicates the existence of a pool layer or a stride operation.
In Figure 8(c), the extracted value of m M1-L4 (the number of columns in out for the current layer) is 3072, and the extracted range of m M2-L1 (the number of columns in in for the next layer) is [524,1536].
We use the formula in Table 3 to determine the pool or stride width and height.
To make the square root of m M1-L4 /m M2-L1 an integer, m M2-L1 has to be 768, and the pool or stride width and height have to be 2.
Using Equation 3, we compute the number of architectures in the search space without Cache Telepathy and with Cache Telepathy.
Table 4 shows the resulting values.
Note that we only consider the possible configurations of the different layers in VGG-16 (B1, B2, B3, B4, and B5) and of the different modules in ResNet-50 (M1, M2, M3, and M4).
Using Cache Telepathy to attack OpenBLAS, we are able to significantly reduce the search space from an intractable size to a reasonable size.
Both Flush+Reload and Prime+Probe obtain a very small search space.
Specifically, for VGG-16, Cache Telepathy reduces the search space from more than 5.4 × 10 12 architectures to just 16; for ResNet-50, Cache Telepathy reduces the search space from more than 6 × 10 46 to 512.
Cache Telepathy is less effective on MKL.
For VGG-16, Cache Telepathy reduces the search space from more than 5.4 × 10 12 to 64 (with Flush+Reload) or 1936 (with Prime+Probe).
For ResNet-50, Cache Telepathy reduces the search space from more than 6 × 10 46 to 6144 (with Flush+Reload) or 5.7 × 10 15 (with Prime+Probe).
The last number is large because the matrix dimensions in Module M1 and Module 4 of ResNet-50 are small, and MKL handles these matrices with the special method described in Section 6.
Such method is not easily attackable by Prime+Probe.
However, if we only count the number of possible configurations in Modules M1, M2, and M3, the search space is 41472.
Implications of Large Search Spaces A large search space means that the attacker needs to train many networks.
Training DNNs is easy to parallelize, and attackers can request many GPUs to train in parallel.
However, it comes with a high cost.
For example, assume that training one network takes 2 GPU days.
On Amazon EC2, the current price for a single-node GPU instance is ∼$3/hour.
Without Cache Telepathy, since the search space is so huge, the cost is unbearable.
Using Cache Telepathy with Flush+Reload, the reduced search space for the different layers in VGG-16 and for the different modules in ResNet-50 running OpenBLAS means that the training takes 32 and 1024 GPU days, respectively.
The resulting cost is only ∼$2K and ∼$74K.
When attacking ResNet-50 running MKL, the attacker needs to train 6144 architectures, requiring over $884K.
We overview possible countermeasures against our attack, and discuss their effectiveness and performance implications.We first investigate whether it is possible to stop the attack by modifying the BLAS libraries.
All BLAS libraries use extensively optimized blocked matrix multiplication for performance.
One approach is to disable the optimization or use less aggressive optimization.
However, it is unreasonable to disable blocked matrix multiplication, as the result would be very poor cache performance.
Using a less aggressive blocking strategy, such as removing the optimization for the first iteration of Loop 3 (lines 4-7 in Algorithm 1), only slightly increases the difficulty for attackers to recover some matrix dimensions.
It cannot effectively eliminate the vulnerability.Another approach is to reduce the dimensions of the matrices.
Recall that in both OpenBLAS and MKL, we are unable to precisely deduce the matrix dimensions if they are smaller than or equal to the block size.
Existing techniques, such as quantization, can help reduce the matrix size to some degree.
This mitigation is typically effective for the last few layers in a convolutional network, which generally use small filter sizes.
However, it cannot protect layers with large matrices, such as those using a large number of filters and input activations.Alternatively, one can use existing cache-based side channel defense solutions.
One approach is to use cache partitioning, such as Intel CAT (Cache Allocation Technology) [30].
CAT assigns different ways of the last level cache to different applications, which blocks cache interference between attackers and victims [37].
Further, there are proposals for security-oriented cache mechanisms such as PLCache [62], SHARP [67] and CEASER [44].
If these mechanisms are adopted in production hardware, they can mitigate our attack with moderate performance degradation.
Recent research has called attention to the confidentiality of neural network hyper-parameters.
Hua et al. [29] designed the first attack to steal CNN architectures running on a hardware accelerator.
Their attack is based on a different threat model, which requires the attacker to be able to monitor all of the memory addresses accessed by the victim.
Our attack does not require such elevated privilege.
Hong et al. [27] proposed to use cache-based side channel attacks to reverse engineer coarse-grained information of DNN architectures.
Their attack is less powerful than Cache Telepathy.
They can only obtain the number and types of layers, but are unable to obtain more detailed hyper-parameters, such as the number of neurons in fully-connected layers and filter size in convolutional layers.
Batina et al. [9] proposed to use electromagnetic side channel attacks to reverse engineer DNNs in embedded systems.Cache-based side channel attacks have been used to trace program execution to steal sensitive information.
A lot of attacks target cryptography algorithms [15-17, 24, 31, 38, 40, 47,68-70,72], such as AES, RSA and ECDSA.
Recent works also target application fingerprinting [28,42,48,50,73] to steal web content or server data, monitor user behavior [22,36,46,71], and break system protection mechanisms such as SGX and KASLR [11,21,61].
In this paper, we proposed Cache Telepathy, an efficient mechanism to help obtain a DNN's architecture using the cache side channel.
We identified that DNN inference relies heavily on blocked GEMM, and provided a detailed security analysis of this operation.
We then designed an attack to extract the matrix parameters of GEMM calls, and scaled this attack to complete DNNs.
We used Prime+Probe and Flush+Reload to attack VGG and ResNet DNNs running OpenBLAS and Intel MKL libraries.
Our attack is effective at helping obtain the architectures by very substantially reducing the search space of target DNN architectures.
For example, when attacking the OpenBLAS library, for the different layers in VGG-16, it reduces the search space from more than 5.4 × 10 12 architectures to just 16; for the different modules in ResNet-50, it reduces the search space from more than 6 × 10 46 architectures to only 512.
This work was funded in part by NSF under grant CCF-1725734.
