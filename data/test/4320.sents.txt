The hidden vulnerability of distributed learning systems against Byzantine attacks has been investigated by recent researches and, fortunately, some known defenses showed the ability to mitigate Byzantine attacks when a minority of workers are under adversarial control.
Yet, our community still has very little knowledge on how to handle the situations when the proportion of malicious workers is 50% or more.
Based on our preliminary study of this open challenge, we find there is more that can be done to restore Byzantine robust-ness in these more threatening situations, if we better utilize the auxiliary information inside the learning process.
In this paper, we propose Justinian's GAAvernor (GAA), a Gradient Aggregation Agent which learns to be robust against Byzantine attacks via reinforcement learning techniques.
Basically , GAA relies on utilizing the historical interactions with the workers as experience and a quasi-validation set, a small dataset that consists of less than 10 data samples from similar data domains, to generate reward signals for policy learning.
As a complement to existing defenses, our proposed approach does not bound the expected number of malicious workers and is proved to be robust in more challenging scenarios.
Through extensive evaluations on four benchmark systems and against various adversarial settings, our proposed defense shows desirable robustness as if the systems were under no attacks, even in some case when 90% Byzantine workers are controlled by the adversary.
Meanwhile, our approach shows a similar level of time efficiency compared with the state-of-the-art defenses.
Moreover, GAA provides highly interpretable traces of worker behavior as by-products for further mitigation usages like Byzantine worker detection and behavior pattern analysis.
Justinian I, an emperor of Byzantium, reorganized the imperial government to revive the empire's greatness in a dark time.
Gradient Aggregation Agent, a new GAAvernor (pronounced as governor) of distributed learning system, bases its learning policy on historical and auxiliary information to fight against Byzantine attacks.
Over the past few decades, deep learning has achieved abundant breakthroughs driven by big data [38,52].
To deal with the fast scaling-up of data volume, many efficient distributed learning algorithms have been proposed in the past decade [3,22,29], yet their hidden vulnerability to Byzantine attacks [37] have also been observed by a series of recent works [11,16,31,62].
In a typical distributed learning system [3,34,41,43,50,64], a group of workers participate in building a global learning model under the coordination of one parameter server.
In each round, the server first distributes current parameters of the global learning model to each worker, requiring them to compute the corresponding gradient based on their local data.
Once receiving all the submissions from the workers, the server then applies certain Gradient Aggregation Rule (GAR) to yield the next weight update.
As an optimal choice in theory [12,47], most existing distributed learning algorithms implemented their GAR simply by averaging over the whole set of submitted gradients [42,56,63].
However, the behaviors of real-world workers are far from ideal.
As is suggested in [62], a worker may probably submit abnormal gradients due to various causes such as biased batch sampling, computation error, network instability or even malicious attacks.
In [11], a worker with the aforementioned abnormal behavior is usually referred to as a Byzantine worker.
As first observed by Blanchard et al., the classical GAR (i.e., GAR by averaging) is so fragile that even a single Byzantine worker can have a catastrophic effect on the whole learning process, from degraded prediction accuracy [31] to total stagnation [11].
These facts highly emphasize the urgency and significance of effective defense against this type of adversarial behavior, namely Byzantine attack.To fight against Byzantine attacks, most previous studies implement alternative GARs to the classical one [4,11,16,31,62].
These methods view gradients abstractly as highdimensional vectors to apply robust statistical methods such as clustering [11], median [31] or geometric median [4,16,62].
Although it allows previous methods to be highly decoupled with the underlying learning systems, the simplicity is accompanied with several weaknesses: First, as previous GARs computes the weight update direction as the only product, they are unable to provide interpretable information of the workers' behaviors for further mitigation; Second, due to the theoretical bottleneck of robust statistics [48], most known defenses expect that only a minority of workers are compromised.
As a result, they are inadequate and cannot be directly extended to cover more challenging scenarios where the adversary has gained control over a majority of workers and iteratively manipulates an uncertain ratio of workers to play the Byzantine roles.
Our Work.
In this paper, we propose the design of Justinian's GAAvernor (GAA), a Gradient Aggregation Agent which serves as a novel server-side defense that leverages Reinforcement Learning (RL) techniques to learn to be Byzantinerobust from interactions with the workers and from the auxiliary information on the server.
Our defense aims at restoring the robustness of distributed learning in more challenging scenarios characterized by the existence of the malicious majority.By viewing the historical interactions with the workers as its experience and the relative decrease of loss on a quasivalidation set as its reward, GAA searches over a simplex as its policy space for the optimal policy.
Intuitively, each coordinate of a policy of GAA can be interpreted as its current credit on the corresponding worker.
By proposing the weight update at each iteration as a linear combination of the received gradients weighted with its credits, GAA receives the reward signal after the global learning model is updated with the current weight update and it then optimizes its current policy by RL techniques [54].
It is worth to notice, we introduce the notion of a quasi-validation set to denote a collection of data samples that follows a similar but not necessarily identical distribution as the true sample distribution.
In practice, when a golden-labeled validation set (i.e., a set of samples from the true sample distribution) is available during the learning process, GAA can utilize it as its quasi-validation set.
Otherwise, GAA randomly collects a small number of data samples (empirically, less than 10 samples) from similar data domains to form its quasi-validation set.With extensive experiments, we evaluate GAA's robustness on four diverse case studies (i.e., MNIST [39], CIFAR-10 [35], Yelp reviews [1] and CMS public healthcare records [2]), against various attacking settings.
We find our proposed approach shows near-optimal Byzantine robustness in most cases, whenever the ratio of Byzantine workers (i.e., Byzantine ratio) is below or over 50% or fluctuates unboundedly.
Meanwhile, GAA shows comparable time efficiency to known defenses.
We also evaluate GAA's robustness against several adaptive attacks on this novel defense mechanism.
Moreover, we present the application of GAA to Byzantine worker detection, which shows high accuracy, and to behavior pattern analysis of Byzantine attacks, which demonstrates high interpretability of its traces.Contributions.
In summary, we mainly make the following contributions.
• We propose the design of GAA, a novel RL-based defense against Byzantine attacks which requires no upper bound on the Byzantine ratio ( §4).
• We implement and evaluate our proposed defense on four diverse case studies, against various adversarial settings.Empirical results suggest in most cases, GAA with an easily accessible quasi-validation set helps the distributed learning systems achieve almost indistinguishable performance as if the systems were under no attacks ( §5 & §6).
• We also provide a number of analytic results on GAA's robustness in different settings as theoretical evidences ( §4.4).
• Additionally, we demonstrate the interpretability of GAA's traces with visualizations and with applications to Byzantine worker detection and behavior analysis ( §4.5), which we hope will facilitate future mitigation studies.
Gradient-based Distributed Learning and GAR.
In this paper, we focus on the data-parallel distributed learning system with one parameter server (abbrev.
the server) and n workers.
This system model is widely used as one of the commonest implementations of distributed learning algorithms [3,34,41,43,50,64].
We denote the loss function to be minimized as f (θ, D), where θ ∈ R d collects all the free parameters of the underlying model (e.g., a deep neural network) and D denotes the sample distribution.
Usually, the true loss function f (θ, D) is the expectation over the sample distri-bution, i.e. f (θ, D) := E z∼D [ f (θ, z)]where D is unknown to the server.
In practice, the optimization happens on the empirical version of the loss f (θ,D) := 1 |D| ∑ z∈D f (θ, z),where D is a collection of training samples.
For simplicity, we denote the true loss function as f and the empirical loss function calcuated on dataset D asˆfasˆ asˆf D .
The distributed learning process starts with an initial guess θ 0 on parameters.
At iteration t, the server first sends the current parameter θ t to each worker.
Ideally, a worker i then computes the estimated gradient V t i of loss f at parameter θ t based on its local data and submits V t i back to the server.Once the server receives the candidate set of gradients Q t :={V t 1 , . . . ,V t n }, it executes certain GAR F : (R d ) n → R d toaggregate the received gradients into a single weight update direction.
Such a procedure is executed in iterations until a provided termination condition is reached.
Formally, the update rule at iteration t follows θ t+1 = θ t − λF (V t 1 , . . . ,V t n ), where λ is the learning rate.In the literature of distributed learning, the following GARs are the common choices for implementation of F [3,22,29,34,61], while their vulnerability to Byzantine attacks have been studied in a series of recent works [11,16,31,62].
Definition 1 (Classical GAR).
F (V 1 , . . . ,V n ) = 1 n ∑ n i=1 V iDefinition 2 (Linear GAR).
As a generalization of classical GAR, a linear GAR F with parameter α ∈ S n is defined asF (V 1 , . . . ,V n ) = ∑ n i=1 α i V i , where S n := {α ∈ R n : α i ≥ 0, ∑ n i=1 α i = 1} is called an n-dimension simplex.Benign Workers vs. Byzantine Workers.
In order to have a precise understanding of what a Byzantine worker is, we start from a formal definition of benign worker.As is discussed, at iteration t, each worker is expected to estimate the true gradientg t = E z [∇ θ f (θ t , z)] based on its local data set D. Optimally, it computes V t := 1 |D| ∑ z∈D ∇ θ f (θ t , z)as its submission, due to the well-known fact that V t is an unbiased estimator of g t if D is i.i.d. sampled from D [12].
Generally, it inspires us to make the following definition.Definition 3 (Benign Worker).
A worker which submits a gradient V t at iteration t is said to be benign if V t is an unbiased estimator of the true gradient g t , i.e., EV t = g t .
With such a definition of benign worker, it is rather simple to define a Byzantine worker as its opposition.Definition 4 (Byzantine Worker).
Otherwise, a worker is said to be Byzantine at iteration t if V t is biased, i.e., EV t − g t = 0.
A well-established theorem from statistics states that classical SGD is guaranteed to converge if the gradient estimation at each descent step is unbiased [12,14].
If the system is ideally correct, classical GAR is almost the optimal choice.
However, it is usually not the case in real-world settings [62].
In fact, as first noticed by [11], classical GAR and its variants are so fragile that even a single Byzantine worker can totally break the whole learning process, as is stated by the following lemma.
Proposition 1.
[11, Lemma 1] For any linear GAR F with fixed parameter α, the adversary with only one single Byzantine worker can fool F into yielding any arbitrary weight update continually regardless of other submissions.
Throughout this paper, we consider the same threat model as in previous studies [4,11,16,31,62].
Generally speaking, this threat model assumes that, the adversary compromises a proportion β (s.t. β ∈ (0, 1)) of all workers throughout the learning process and he/she commands the compromised workers to present arbitrary behaviors at each iteration.
In other words, the adversary is able to choose the submitted gradients of each manipulated worker.
Noteworthily, at iteration t, the Byzantine ratio can be also smaller than β if some [31,48] n ≥ 2m + 1 O( n m (n − m)d) O( n m + nd) GeoMed [16, 62] n ≥ 2m + 1 O(n 2 d) O(n 2 d) Krum [11] n ≥ 2m + 3 O(n 2 d + n 2 log n) O(n 2 d) Bulyan [31] n ≥ 4m + 3 O(n 2 d) O(n 2 + nd) GAA (ours) n ≥ m + 1 O(n 3 d) O(n 2 + nd)Byzantine workers pretend benign.
To provide a finer-grained description on the threat model, we introduce the following notions.Role Function.
As is discussed, each worker behaves either benignly or maliciously at iteration t. Therefore, we introduce the notion of the role function of worker i to characterize its temporal behaviors.
Formally, the role function is defined as a binary-valued function on Z + , i.e., the timeline.
Intuitively, r i (t) = 1 means worker i behaves normally at iteration t and otherwise, worker i is a Byzantine worker.
Tampering Algorithm.
Byzantine workers can choose different tampering algorithms to produce malicious gradients.
In previous studies, several realizations of tampering algorithms have been used for evaluation of defenses, such as random fault [11] (More details can be found in Section 5.1).
In general, we denote the tampering algorithm as T , which, with the estimated gradient as the input, outputs the tampered gradient for submission.
As in previous studies, we assume the identity of the tampering algorithm for each malicious worker.
With the notions above, the behavior of the manipulated worker i at iteration t can be described as 1.
First, the adversary selects the current role of the worker i as r i (t).
2.
If the role is benign, i.e., r i (t) = 1, then the worker honestly computes the gradient on its local data, that is, V t i .
3.
Otherwise, i.e., r i (t) = 0, it tampers the gradient V t i with certain tampering algorithm T (e.g., random fault) and produces T (V t i ).
4.
Finally, the produced gradient is sent back to the server.
In order to fight against the aforementioned threat model, previous works proposed several alternative GARs to classical GAR and its linear variants.
We briefly review the state-ofthe-art defenses as follows, where m out of n workers are assumed to be Byzantine at certain iteration, s.t. m/n ≤ β.
For an overview, please refer to Table 1.
Brute-Force [31,48] is based on a brute-force search for an optimal subset C * in Q of size n − m with the minimal maximum pairwise distance.
Formally, the optimal set can bewritten as C * = arg min C ∈R max (V i ,V j )∈C ×C V i −V j , where R := {C ⊂ Q : |C | = n − m}.
Then the proposed weight up-date direction is calculated as F (V 1 , . . . ,V n ) = 1 n−m ∑ V ∈C * V .
It was proved to be perfectly robust when n ≥ 2m + 1 [48], while it is almost intractable in highly distributed learning systems.GeoMed [16,62] computes the geometric median of Q as the proposed estimator, which assumes the Byzantine ratio satisfies n ≥ 2m + 1 [16,62].
In consideration of the computational complexity of geometric median when n is large [18], recent works on Byzantine robustness proposed to approximate it with the vector in Q which has the smallest sum of distance with other gradients, i.e.,F (V 1 , . . . ,V n ) := arg min V i ∑ j =i V i −V j .
Krum [11] was recently proposed in [11] as an approximate algorithm to Brute GAR, which assumes the Byzantine ratio satisfies n ≥ 2m + 3.
It first finds the n − m − 2 closest vectors in Q for each V i , which is denoted as i → j in their original work.
Next, it computes a score for each vector V i with the formula s(V i ) = ∑ i→ j V i −V j 2 .
Finally, it proposes the vector V i with the smallest score as the next update step,i.e., F (V 1 , . . . ,V n ) = arg min V i ∈Q s(V i ).
Bulyan [31] was originally designed for Byzantine attacks that concentrate on a single coordinate.
First, it runs Krum over Q without replacement for n − 2m time and collect the n − 2m gradients to form a selection set.
It then computes F coordinate-wise: the i-th coordinate of F is equal to the average of the n − 4m closest i-th coordinates to the median i-th coordinate of the selection set.
Bulyan has the strictest assumption as n ≥ 4m + 3 (and otherwise it is not executable), which significantly limits its practical usage.As we can see, the aforementioned approaches only considered the limited situation when β is expected to be smaller than 1/2.
In more general cases, e.g., when there is no explicit upper bound on the Byzantine ratio in the system, merely no defenses above could remain robust any longer.
The following proposition provides a typical failure case.Proposition 2.
Consider the submitted gradients at iteration t as (V 1 , . . . ,V n−m , B 1 , . . . , B m ) where {B i } m i=1 are Byzantine gradients.
For the slightest violations in each case, i.e., n = 2m for Brute GAR, GeoMed and n = 2m + 2 for Krum, the adversary can simply take B 1 = B 2 = . . . = B m = E to tempt these GARs to always yield E, any arbitrary direction specified by the adversary.In practice, this more challenging situation could happen for distributed learning systems in open network environments [61].
When the adversary has already compromised a majority of workers at the beginning or continuously gains malicious control over each worker during the learning process, the Byzantine ratio in system could go over 1/2 or even fluctuate with uncertainty.
In either cases, the system robustness is no longer under guard with the above defenses.
In order to restore robustness in a more general scenario, we suggest the defender to be combined more tightly with the underlying learning process, by utilizing some auxiliary information inside the distributed learning system for mitigation purposes.
Before providing an overview of our methodology, we first clarify our security assumptions and present our goals of defense.
4.1.1 Security Assumptions.
We make the following assumptions on the distributed learning system where GAA is to be deployed.
Assumption 1.
The server is secure.
Assumption 2.
There is one worker that is never controlled by the adversary.
Here, Assumptions 1 & 3 are commonly adopted in previous studies [4,11,16,31,62].
As GAA is deployed on the server, Assumption 1 guarantees its correct execution.
Noticeably, Assumption 2 relaxes the known slightest requirements on the tolerable Byzantine ratio to 1 − 1/n.
As a trade-off, we require Assumption 4 to introduce an additional condition on the availability of a quasi-validation set that follows a similar but not necessarily identical distribution as the true sample distribution.
In theory we prove the lower the divergence, the better the model performance will be (Thm.
1 & 2).
Through empirical evidences, we show this assumption can be easily satisfied with the quasi-validation set that consists of few samples from similar data domains, if there is no provided golden validation set [34,61].
4.1.2 Defender's Goals.
Towards Byzantine robustness, the defender's primary goal is to guarantee the distributed learning process can minimize the loss function f to an acceptable threshold, usually compared to the global minimum of the loss function [31].
In practice, it is also reasonable to measure the robustness of certain defense by the gaps among the model's utility (e.g., the accuracy of an image classifier) when the defense is equipped, unequipped with or without attacks.
We will provide more details in Section 5.
4.1.3 Methodology Overview.
Before detailing the implementations, we provide an overview of our proposed approach ( Fig. 1).
Robust distributed learning with GAA follows the following procedures: First, on receiving the submitted gradients from each worker, GAA, an additional module deployed on the server, executes certain policy to pose credit on each worker.
Intuitively, GAA has limited credit in total and it will pose higher credit on the worker it trusts more (Step 1).
Next, GAA aggregates the gradients based on the credit and then proposes the weight update decision to the underlying learning process (Step 2).
Finally, the learning process produces a reward signal based on the quasi-validation set, which is used to indicate the quality of the update direction (Step 3) and can further help GAA adjust its policy dynamically (Step 4).
Following the conventions of Reinforcement Learning (RL) [53], we first define the notion of environment, with which an agent interacts.
Standardly, the environment of a Markov Decision Process (MDP) is represented as a tu-ple (S , A,R, p 0 , p, γ), where S,A are respectively the set of states and of actions, R : S → R is the reward function, p 0 : S → R + is the initial probability density over states and p : S × A × S → R + is the transition probability density, with γ ∈ (0, 1] the discount factor.
In the context of distributed learning, our specifications for these components are stated as follows.
Fig. 2 shows an overview of our MDP settings.Set of States S.
In the terminology of MDP, a state usually has the intuitive meaning as a context, based on which the agent makes a decision.
Naturally, our GAA at iteration t refers to the tuple s t := (Q t , θ t , ˆ f B (θ t )) as the current state to decide the next weight update direction.
Recall θ t , Q t are respectively the parameter and the received gradients at iteration t, whilêwhilê f B (θ t ) is defined as the loss at θ t estimated by the server on the quasi-validation set B.Set of Actions A. Taking advantage of the simplicity of linear GAR, we propose to define the action space as an n-dimension simplex, where n is the number of workers.
Generally speaking, our motivation here is to regularize the action space with prior knowledge and therefore the cost on searching the optimal policy can be largely scaled down.
By restricting the feasible action to the space of linear GARs, GAA at each iteration chooses a candidate internal action α t ∈ S n based on the current state s t and the previous action α t−1 .
Intuitively, this process can be considered as GAA's posing credit on each worker.
Based on α t , GAA then proposes the current update step asθ t+1 = θ t − λ(∑ n i=1 α (i) t V t n ).
It is worth to notice, although the aggregation rule of GAA is linear in its form, it largely differs from linear GARs in that the coefficient α t is chosen by a sophisticated agent adaptively at each iteration rather than predefined, which therefore makes our model immune to the vulnerability innate to linear GARs [11].
Reward Function R. Reward function is usually defined as a function from each state s to a scalar value, which provides heuristics for policy learning.
In our context, we set the reward at iteration t asR t := ˆ f B (θ t ) − ˆ f B (θ t+1 ), namely the relative loss decrease on the quasi-validation set B. Intuitively, if KL(P m ||D) is 0, the reward R t highly reflects the changes in the true loss f [47] and thus provides a good guidance for GAA's policy learning.
For other situations when P m is similar but not necessarily identical with the true distribution, empirical studies show the reinforcement learning techniques still work well, probably due to its innate tolerance of noises in rewards [53].
Initial and Transition Probability Density p 0 , p. Usually, these terms are partially unknown to an agent, which could only be estimated implicitly from observed trajectories [57].
Similarly, our GAA only has the partial knowledge regarding θ andˆfandˆ andˆf B (θ) of p 0 , with random initialization of parameters, and of p, with the updating rule above, but totally ignorant of the initial distribution of Q 0 and its transition.
In fact, the learning of GAA is exactly paralleled with an incrementally accurate estimation of p 0 and p, which equivalently means a better knowledge of the undertaking Byzantine attacks.
Discount Factor γ.
Discount factor as a constant in (0,1] describes how the rewards in history influence the current decision, the value of which is determined by different application scenarios.
Our configurations can be found in the evaluation parts.
In the MDP setting above, our GAA is required to search for certain optimal policy π(α|s) to maximize the expectation of accumulated reward [54], where π(α|s) denotes a parametrized distribution over the action space A, conditioned on the currently observed state s. Formally, the optimization objective for training GAA is defined asmax π E s 0 ,a 0 ,...,s T ,a T [∑ T t=0 γ t R(s t )], where (s 0 , a 0 , . . . , s T , a T )is called a trajectory (or, experience) of length T + 1, which has the joint probability density p(s 0 , α 0 , . . . , s T , α T ) = p 0 (s 0 ) ∏ T t=1 p(s t |s t−1 , α t−1 )π(α t−1 |s t−1 ).
In the context of RL, the objective above has been intensively studied and various mature algorithms such as policy gradient descent [54] or Q-learning [57] have been proposed to solve it.
We expect our GAA can be seamlessly fused into the learning process of the underlying model with a similar behavior as statistical GARs.
Therefore, we propose to approximately model the chained term ∏ T t=1 p(s t |s t−1 , α t−1 )π(α t−1 |s t−1 ) in the joint probability density with a general Recurrent Neural Network (RNN [27,59]).
The full computational graph of our proposed implementation is illustrated in Fig. 3.
Starting from the initial state s 0 ∼ p 0 and initial action α 0 := ( 1 n , . . . , 1 n ), we formulate the auxiliary RNN as follows ∀t ∈ {0, . . . , T − 1}, α t+1 = h ψ (s t+1 , α t ), where h ψ denotes certain recurrent unit with parameter ψ, with its range as a subset of S n .
Practically, such a condition can be easily realized with a softmax layer [10].
For details, please see Section 5.1.
Therefore, the optimization objective of GAA is refor-mulated as min ψ E s 0 ∼p 0 [∑ T −1 t=0 γ t ( ˆ f B (θ t+1 ) − ˆ f B (θ t ))], where θ t is uniquely determined with the update rule conditioned on α t−1 and θ t−1 .
By expansion ofˆfofˆ ofˆf B , we can formulate the final optimization objective of GAA in episode i as min ψ1 S ∑ T −1 t=0 γ t ∑ z∈B f (θ t+1 , z) − f (θ t , z), where θ 0 is initialized randomly while α 0 in episode i always inherits value from α T in episode i − 1.
Our learning algorithm is listed in Algorithm 1.
In this part, we present theoretical evidence on Byzantine robustness of distributed learning with GAA when the Byzantine ratio is fixed or fluctuates with uncertainty.
Please note in the following analysis we focus on the empirical version of f on the training set, as the omitted leap from our proved results to f is guaranteed by standard results in generalization theory [58].
For the same reason, we maintain the notation f for its empirical version.
We assume the loss function f is convex and η-smooth with pointwise bounded gradient ∇ f 2 ≤ M. For non-convex objective, our results can be exAlgorithm 1: Robust Distributed Learning against Byzantine attacks with GAA 1 Initialize parameters of recurrent unit h ψ randomly ;2 Initialize α old = α 0 = ( 1 n , . . . , 1 n ) ∈ S n ; 3 for i ∈ {1, . . . , N} do 4Initialize parameters of f as θ 0 randomly ;5 for k ∈ {1, . . . , K} do 6 α 0 ← α old , GAA ← 0; 7 for t ∈ {0, . . . , T − 1} do 8Send the current parameters θ t to each worker ;9 Receive submitted gradients Q t := (V t 1 , . . . ,V t n ) ; 10 θ t+1 ← θ t − λ(∑ n i=1 α i t V t i ) ; 11 GAA ← GAA + 1 S γ t ∑ z∈B f (θ t+1 , z) − f (θ t , z) ; 12 α t+1 ← h ψ (s t+1 , α t ) 13 end 14Update ψ with a step of gradient descent on GAA ; tended with quadratic approximations [13].
Due to the page limit, we provide the detailed proofs for the results in this part at the website pertaining to this paper 1 .
Theorem 1.
After t steps of gradient descent with GAA when the Byzantine ratio is fixed as β, Algorithm 1 yields a parameter θ t s.t.f (θ t ) − f (θ * ) < 2RM (1 − β)nt + SηR 2 t + √ 2 f ∞ KL(P m ||D) + O(e −t ) (1)where R is the diameter of parameter space.Corollary 1.
As long as β is smaller than 1 and P m = D a.e., Algorithm 1 in the above setting will asymptotically converge to the global optimum with rate O(1/ √ t).
Intuitively, Theorem 1 suggests, when the Byzantine ratio is fixed over time, GAA is proved to help the underlying system attain a sub-optimal parameter with error ε + O(KL(P m ||D)) in O( 1 (1−β)ε 2 ) steps.It suggests a lower KLdivergence bound (at the scale of 10 −2 in our case studies with a quasi-validation set constructed from similar data domains) and a smaller Byzantine ratio will lead to a more accurate sub-optimum.
When the quasi-validation set is from the true distribution, Corollary 1 further guarantees the convergence of the learning process with rate O(1/ √ t), which is relatively larger than the optimal rate O(1/t) in Byzantium-free learning case [14].
We provide a more detailed explanation on the meaning of each term and an empirical validation of Theorem 1 in Appendix A.4.
Ratio.Theorem 2.
After t steps of gradient descent with GAA when the Byzantine ratio fluctuates randomly other than 1, Algorithm 1 yields a parameter θ t s.t.f (θ t ) − f (θ * ) < 2RM + M √ t + SηR 2 t + √ 2 f ∞ KL(P m ||D)(2) where R is the diameter of parameter space.Corollary 2.
Specifically, if P m = D a.e., the learning process will asymptotically converge to the global optimum with convergence rate O(1/ √ t).
Intuitively, Theorem 2 suggests, although there is still a guarantee for GAA to attain the sub-optimum in this case, the error term on the right of (2) is independent from β and is slightly larger than the one in (1).
It is mainly because GAA in this case would pose all its credit on one single worker that is never compromised and therefore the distributed learning system degrades to a single-noded version when Byzantine ratio fluctuates.
Similarly, Corollary 2 proves the convergence of GAA in this more challenging case when a golden-labeled validation set is available.
In principle, when a policy is learned on how to determine an optimal action α t according to the current state s t and the historical information, our GAA is expected to master a good knowledge of the undertaking Byzantine attacks.
Generally speaking, since the action proposed by our GAA is always constrained in S n , it is therefore reasonable to view each component of α t as the credit on the corresponding worker.
Specifically, we present its application in detection and behavioral pattern analysis of Byzantine workers below.
4.5.1 Byzantine Worker Detection.
When the Byzantine ratio is fixed, accurate detection of Byzantine workers can help accelerate the learning process by eliminating potential Byzantine workers at an early stage.
Therefore, we suggest detection algorithms should aim at selecting K most suspicious workers at iteration t. Although most statistical GARs are not directly applicable for detection tasks, we find one exception is GeoMed, for which we provide a straightforward extension as follows.Procedure 1 (GeoMed+).
Given Q t = {V t 1 , . . . ,V t n },Step 1.
Initialize O t = {} Step 2.
O t ← i * := arg max i∈{1,...,n} ∑ V t j ∈Q t V t i −V t jStep 3.
Q t ← Q t \{V t i * }Step 4.
If |O t | = K, output O t .
Otherwise, go to Step 2.
As a comparison, Byzantine worker detection with GAA can be conducted in a more natural way.Procedure 2 (GAA+).
Step 1.
Find K smallest coordinate of α t .
Step 2.
Output the corresponding index set as O t 4.5.2 Byzantine Behavior Analysis.
When the Byzantine ratio fluctuates with unknown patterns, detecting temporal characteristics is a much more challenging task compared with the aforementioned case.
Barely any previous statistical GARs can be adapted for addressing this task due to their lack of interpretability, while our proposed GAA can be applied directly for Byzantine behavior analysis with visualizations.
In this case, we can visualize the policy sequence {α t } to understand the temporal patterns of Byzantine attacks.
A concrete demonstration on a situation when the Byzantine ratio fluctuates periodically is presented in Section 6.5.
5.1.1 Benchmark Systems.
We build GAA into the distributed learning process of four benchmark systems for text and image classification listed in Table 2.
On MNIST and CIFAR-10, each worker shares a copy of the training set, while on Yelp and Healthcare, each worker has its local dataset.
In all the cases, the loss function f is set as the cross entropy loss between the prediction of classifier g and the ground-truth.
More details are provided in Appendix A.3.
• Static Attack: All the βn compromised workers play the role of Byzantine workers during the whole learning process.
• Pretense Attack: In this case, the βn manipulated workers pretend to be benign in the first L rounds and start the attack from the (L + 1)-th round.
• Randomized Attack: At beginning, each compromised worker (βn in total) is assigned with its role r i (0) by the adversary.
During the learning process, it changes its role with a probability q at a period of p rounds.It is worth to notice, the first pattern is a realization for the case in Section 4.4.1, when the Byzantine ratio is fixed over time, while the pretense and randomized attacks correspond to the setting in Section 4.4.2 when the Byzantine ratio fluctuates with or without uncertainty.
Moreover, the latter two patterns are designed as adaptive attacks on the RL mechanism adopted by GAA.
Both randomized attack and pretense attack attempt to mislead GAA into making wrong credit assignments, by letting the manipulated workers pretend to be benign and submit normal gradients in a certain time span of the learning process.
In experiments, we evaluate the impact of two realizations of the tampering algorithm T .
• Random Fault (RF) [11].
For RF, Byzantine workers submit noisy gradients sampled from a multi-dimensional Gaussian N (µ, σ 2 I).
In our experiments, we take µ = (0.5, . . . , 0.5) ∈ R d and σ = 2 × 10 −6 .
• Adaptive Fault (AF).
For AF, we consider an adversary has some knowledge of the quasi-validation set, which allows the manipulated workers to submit well-crafted gradients that can tempt GAA to assign them with high credits and meanwhile maximize the overall training loss.
We provide the details on the implementation of this fault in Section 6.3.
We implement the recurrent unit h ψ of GAA in the following experiments as a fully connected, feed-forward neural network with no hidden layer, with an input layer of size (3n + 2) × d (i.e., the dimension of concatenation of s t and α t ) and an output layer of size d with softmax activation.
For other common hyperparameter settings in Algorithm 1, we set the learning rate λ as 0.05, discount factor γ as 0.9, the episode length T as 5, the number of episode N as 5.
Each benign worker computes the gradient on randomly sampled mini-batch of size 64 for MNIST & CIFAR-10 and 256 for Yelp & Healthcare.5.1.5 Choice of the Quasi-Validation Set B. For MNIST and CIFAR-10, we set the quasi-validation set as a random mini-batch of training samples.
For Yelp and Healthcare, we implement the quasi-validation set as a small subset of samples from similar data domains.
On Yelp, each worker holds 20k restaurants' reviews (randomly selected from the raw restaurant reviews) from one of the 10 US states with the most recorded Yelp reviews (including Arizona, Illinois and so on).
We randomly sampled 1k reviews from South California, which is not in the top-10 states, as the full quasi-validation set.
On Healthcare, each worker holds 20k treatment descriptions from local hospitals in one of the 50 different states, while we use a subset of descriptions from Alaska as the full quasi-validation set, which contains 1k records in total.
For all our experiments on Yelp and Healthcare, we use less than 10 random samples from the full quasi-validation set as the working quasi-validation set.
We highlight some experimental findings below.
• Robustness -GAA effectively defends the 4 benchmark systems against 3 attacking patterns and 2 tampering algorithms, with a wide range of configurations.
It helps the underlying systems achieve comparable performance in limited rounds as if the systems were not under attacks.
• Efficiency -The time efficiency of GAA is on a similar scale with previous statistical defenses.
• Interpretablity -A well-trained GAA provides informative and interpretable traces that can be used for Byzantine worker detection and behavior pattern visualization.
Figure 4: Test accuracy of the benchmark systems under static attacks when different defenses are applied up to a fixed round.
We compare the Byzantine robustness of our proposed GAA with 6 baselines under static attacks with RF: (A) Classical GAR (B) Brute-Force (C) GeoMed (D) Krum (E) Bulyan and (F) Classical GAR without attack.
We include the last baseline for measuring the degradation of each method under attacks.
We set the Byzantine ratio β in the static attack as 0.2, 0.5, 0.7, where 0.2 is a tolerable Byzantine ratio for all the baselines and 0.5 corresponds to the breaking point of the baselines.
Fig. 4 shows the final test accuracy of the four benchmark systems with different defenses equipped, up to 5k, 10k, 20k, 40k rounds respectively.
As Bulyan is not executable when n ≥ 4m + 3, the corresponding result is not collected when β ≥ 0.5.
Moreover, Brute-Force on MNIST, CIFAR-10 & Healthcare and Bulyan on CIFAR-10 fail to finish the learning in 10 days due to the high time complexity (we provide evaluations in Section 6.1.2 and Table 1), the corresponding results are not reported.Results & Analysis.
As we can see from Fig. 4, when the Byzantine ratio is as small as 0.2, each baseline method is observed to be Byzantine robust, which conforms to the reported results in previous works [31].
In this case, our GAA also helps the underlying model achieve a similar test accuracy.
Noticeably, the robustness of our GAA is strongly demonstrated by its comparable performance to classical GAR without attack, when the Byzantine workers are in majority.
For example, as the β = 0.5 cases represent the breaking point of Brute-Force, Krum and GeoMed, on Yelp the benchmark systems with the baseline defenses perform no better than a random guesser, while GAA helps the system achieve over 80% accuracy, which is very close to the 84.5% accuracy when the system is under no attack.
A similar phenomenon was observed even when we further enlarge the Byzantine ratio to 0.7.
These results imply GAA does complement the existing defenses when the Byzantine ratio is larger than 0.5.
6.1.2 Time Efficiency.
We measure the time cost of our defense and provide a tentative comparison with previous defenses.
We run the four benchmark systems with different defenses under the same static attack in the previous part and record the time cost of 100 iterations with 10 repetitions in the same environment described in Appendix A.1.
Table 3 lists the running time of different defenses in each case.
As the results imply, GAA brings computation overheads on a similar scale compared with previous defenses, which roughly corresponds to the theoretical complexity listed in Table 1.
In this part, we evaluate the robustness of GAA when the adversary attempts to mislead the credit assignment by letting the manipulated workers pretend to be benign.
shows GAA helps the benchmark system on MNIST achieve about 90% accuracy on average, which is close to the 96.4% accuracy of the system under no attack.
As a comparison, the systems equipped with the baseline defenses either has final performance much lower than the expected or totally stagnate.
Moreover, from Fig. 5(e)-(h), we find no fluctuation happens when the manipulated workers begin to attack after 1K rounds, which implies the RL mechanism of GAA is robust against pretense.
Below, we present a more careful evaluation of GAA under a wide range of attack configurations.
6.2.2 GAA under Adaptive Attacks with Varied Configurations.
Besides, we further evaluate GAA's robustness against the randomized attacks and the pretense attacks with diverse configurations on Yelp and Healthcare.
Fig. 6 presents the learning curves of the underlying benchmark systems under attacks of varied configurations listed in the legends, where the shaded part of the curves denotes the variance of the accuracy within 10 repetitions.Results & Analysis.
As we can see from Fig. 6, under randomized Byzantine attacks of most configurations, GAA helps the benchmark systems on Yelp and Healthcare achieve desirable performance, compared with the accuracy of systems without Byzantine attacks.
For example, in most configurations for Yelp, the final accuracy is around 83%, which is close to the optimal accuracy 84.5%.
Although from Fig. 6(b) we notice the q = 0.0 case on Yelp has a larger variance, the average final accuracy is only about 10% lower compared with the optimal accuracy, which is still acceptable considering the high Byzantine ratio up to 0.7.
Similarly, from Fig.
Although Assumption 1 and the randomness in the composition of the Quasi-Validation set (abbrev.
QV set) imply the exact samples in the QV set is hard to be known by the adversary, we further examine the following two worst-case leakages of the QV set, which may allow the adversary to submit carefully crafted gradients (or called Adaptive Fault (AF)) based on the knowledge of the QV set to attempt to mislead GAA.
• Case A.
The adversary knows the distribution where the QV set is sampled.
• Case B.
Some classes are missing in the QV set and the malicious worker can target on the missing classes.
Intuitively, Case A is possible when the adversary expects GAA would use samples from similar data domains as the QV set, while Case B is possible when the QV set is too small to cover all different classes.
It is worth to notice, for the adversary in Case A, the probability of determining the exact samples in the QV set is very low in theory, as the QV set contains less than 10 samples that are chosen independently by the server while the sample space of the distribution known to the adversary, practically the local dataset held by the manipulated worker, can contain as large as 10 3 samples when deep learning models are deployed.In both cases, we consider the AF follows the same principle: it minimizes the loss on a dataset D 0 , which is chosen based on the knowledge about the QV set, to tempt GAA to assign the manipulated worker with high credit.
In the meanwhile, the AF maximizes the overall loss on D 1 , (a subset of) its own training set, to compromise the whole distributed learning process.
Accordingly, we formulate the gradient V t i submitted by a malicious worker (i.e., Worker i) at iteration t with AF byV t i ∝ ∇ θ ((θ t , D 0 ) − α(θ t , D 1 )),where α is a hyperparameter that controls the stealthiness of the adaptive fault.
, we find in most cases the final accuracy of the benchmark systems remains close to the optimal accuracy.
For example, under the combo adaptive attack on both the RL mechanism and the QV set (i.e., Config.
b in Fig. 7(a)&(b)), GAA achieves respectively about 82% and 65% accuracy on Yelp and Healthcare, which is close to the performance of the system under no attack.
The results imply that, GAA is robust against the adaptive adversary knowing the distribution where the QV set is sampled.
From our perspective, lacking the knowledge of the exact QV set would let the adversary only count on his/her own inexact guess on the QV set.
Hence, combining with the malice on maximizing the loss on the local training set, the gradient directions crafted by the malicious workers would be less effective in minimizing the loss on the QV set than the benign workers and therefore would be less trusted by GAA.
However, when the adversary somehow knows the exact QV set the server uses, he/she would craft gradients that always minimize the loss on the QV set and mislead GAA to fully trust the manipulated worker, while this case would be rare, if not impossible, depending on the randomness of sampling and the security of the server.
6.3.2 Adaptive Attacks in Case B.
In this setting, the manipulated worker can target on the missing classes by maximizing the loss on samples belonging to these missing classes, which forms the D 0 , while minimizing the loss of samples from other existing classes, which forms D 1 .
Experimental Settings.
We first sample 10 records from the full QV set on Healthcare (Yelp) to cover all the classes.
For Healthcare, we reduce the number of classes from 9 to 1 with stride 2 by eliminating the samples belonging to the missing classes that we specify.
For Yelp, we consider the case when the QV set contains only positive or only negative samples.
With the QV sets with missing classes, we conduct the GAA defense against three typical attack patterns listed in the legends and titles of Fig. 7(c)-(f), which present the learning curves of the benchmark systems under the considered adaptive attack on the QV set.Results & Analysis.
As we can see from Fig. 7(c)-(f), even when the adversary targets on the missing classes in the QV set, GAA is still able to guarantee the benchmark systems to reach satisfying performance.
For example, under static Byzantine attacks on Healthcare (in Fig. 7(d)), the final performance with 5 missing classes in the QV set is around 75%, even better than the 73.1% accuracy of the system under no attack.
Also, Config.
c in Fig. 7(c) and Fig. 7(f) demonstrates GAA remains robustness under combo attacks on the RL mechanism and the missing classes.
Furthermore, we notice the number of missing classes has minor influence on GAA's defense quality, which strongly demonstrates the robustness of GAA against the adaptive adversary knowing the missing classes in the QV set.
6.3.3 GAA vs. Different Attacks.
Despite the robustness of GAA against various attacks, the empirical performance does show subtle differences when GAA is against different attacks.
For example, comparing Fig. 5 and Fig. 4, we find that the final accuracy of the benchmark systems under randomized and pretense attacks, two attacks exploiting the knowledge that GAA uses the RL mechanism to learn credit, is overall no better than that under static attacks.
Similarly, as we can see from the corresponding results in Fig. 7 and Fig. 4, adaptive attacks that exploits the knowledge on the QV set are relatively more threatening than static attacks, where the threat is not further enlarged when the adversary exploits both the knowledge on the RL mechanism and the QV set, if comparing Config.
b & c in Fig. 7(a) & (b) with the corresponding results in Fig. 5.
These phenomena interestingly show, the more knowledge the adversary has of the deployed defense, the more threatening the attack could be against GAA.
In this part, we report the accuracy of Byzantine worker detection when the system is under static Byzantine attacks via our proposed GAA+ in Proc.
2, compared with the baseline method the GeoMed+ algorithm in Proc.
1.
β = 0.7 K=199.9%/2.85% 0.0%/0.0% K =10 99.9%/28.5% 0.0%/0.0% K =35 99.9%/99.9% 57.1%/57.1%Experimental Settings.
By choosing Byzantine ratio β = 0.3, 0.7, we apply two detection algorithms on MNIST with the total number of workers as 50.
Since we have defined the task of Byzantine worker detection as a top-K classification task, we report precision/recall in Table 4.
Both precision and recall are calculated as an average over 1 × 10 3 randomly subsequent iterations after 1 × 10 4 iterations of distributed learning with GAA.Results & Analysis.
As we can see from above, with small Byzantine ratio, both GeoMed+ and our method achieve near perfect detection of each Byzantine worker.
These empirical results not only justify that GeoMed+ is indeed a strong baseline, but also validates GAA+'s comparable performance with statistical counterparts in slight Byzantium.
However, when the Byzantine ratio β is set up to 0.7, GeoMed+ fails to detect Byzantine workers any longer, while our method still detects each Byzantine worker perfectly, regardless of its majority in total.
In the final part of experiments, we present several interesting visualizations on the policy curve of GAA after learning under randomized attacks of q = 1.0, that is, each manipulated worker inverses its role periodically.
Experimental Settings.
We consider two specific randomized attacks on MNIST with the following configurations: (a) n = 10, q = 1.0, p = 1k with initial β = 0.9 and (b) n = 10, q = 1.0, p = 400 with initial β = 0.5.
In other words, we consider the cases when all workers are manipulated and invert their role periodically.
We collect GAA's action sequence in each configuration up to 40k rounds and plot the policy curves of each worker over a representative slice of iterations in Fig. 8 after normalization, where the policy curves for the initially Byzantine workers are warm-toned and the initially benign workers cool-toned.
Results & Analysis.
First, in both cases the periodic characteristic of the undertaking Byzantine attack is captured well by our GAA, as its policy curve presents a period close to the ground-truth.
To analyze with more care, we notice, in Fig.
8(b), as GAA's decision on Byzantine workers appears to be correct initially, its policy curve mainly evolves vertically.
In other words, GAA tends to behave stable after an optimal policy is attained.
Differently in Fig.8(a), although a low credit is assigned to the only initially benign worker in the first half period, GAA wisely skips the other half and swiftly adjust its policy in the subsequent period by heuristics of reward.
The phenomenon is highlighted by the slashed region in Fig. 8(a).
On Assumptions 1 & 2.
Assumption 1 is used to guarantee the correct execution of Algorithm 1 and GAA itself would not be compromised by the adversary, while Assumption 2 is used to guarantee GAA has at least one worker to trust.
We claim both assumptions are reasonable.
On one hand, the former assumption is commonly assumed in previous studies of Byzantine robustness [4,11,15,16,20,31,62], which serves as a standing point of most published defenses, since otherwise the adversary could easily tamper the global model itself.
On the other hand, the security level of the central server in real world distributed systems is always on a much higher level than working nodes, due to, e.g., rigorous access control mechanisms [55].
Therefore, the cost of attacks on central server is much higher than that on workers.Moreover, we find it is quite straightforward to satisfy Assumption 2 if Assumption 1 is valid.
For instance, the parameter server can spare certain computation resources to simulate one worker node on its own devices.
Therefore, falling back on the properness of Assumption 1, we could claim the simulated worker is an always benign worker and thus satisfies the second assumption.
On Assumptions 3 & 4.
These two assumptions regularize the range of learning tasks which GAA can help.
Assumption 3 is again a commonly adopted assumption in most known defenses [4,11,15,16,20,31,62].
On one hand, if the workers share a copy of the same training set as in many conventional distributed learning systems (including the MNIST & CIFAR-10 cases) [3,34,41,43,50,64], both Assumptions 3 & 4 can be naturally satisfied due to the availability of a validation set from the same data source.
For some newly proposed distributed learning systems (e.g., federated learning [34]) when the workers have their local datasets (including the Yelp & Healthcare cases), we demonstrate with the experimental results in Fig. 9, where we control the size of the QV set on Yelp and Healthcare to be 1 and 10, 100, · · · , 1000 by sampling from the full QV set, that the requirement on the QV set is relatively easy to be satisfied with only a small number of samples from similar data domains.
For example, from Fig.
9(b), we find the final accuracy on Yelp under randomized attacks is both close to the bottleneck accuracy whenever the QV set size is 1 or 1k, despite a slightly larger variance of performance and a lower convergence rate when the QV set is smaller.
Moreover, experiments in Section 6.3 has proved that a small QV set is not likely to be exploited as a weak spot of the system whenever it may have missing classes or share a similar distribution with the local datasets of the manipulated workers.
Despite this, we admit the QV set may be a weak spot for GAA if it is fully known by the adversary, while this case would be rare, if not impossible, in practice due to the randomness in preparing the QV set by the server and the security of the server.For a validation of the requirement on the QV set in Assumption 4, we numerically estimate the average KL divergence among the local datasets and the full QV set on Healthcare.
We find the empirical value is about 0.1.
By inserting the empirical values of the KL divergence and the other terms in Section 4.4, we find the convergence rate predicted by Theorem 1 is quite close to the empirical learning curves.
We provide more details in Appendices A. 2 & A.4.
However, GAA could have certain limitations to guarantee Assumption 4 when the server has no knowledge about the data domain of the undergoing distributed learning process or the learning protocol may have privacy requirements [61], which we leave as an interesting future work.
On Threat Model.
Does the real world distributed learning environment really show such malice that the Byzantine ratio has no explicit upper bound or even fluctuate?
It may not the case for current distributed learning systems in stable local network environments [52].
Existing real world cases are, for example, distributed systems in unstable network environment with low-specification working machines, where a majority of nodes would send faulty gradients due to network or computation errors in an unpredictable manner.
In this situation, GAA turns out to be a promising tool to help the underlying learning process converge to a near-optimal solution.
Other possible use cases of GAA can be found in federated learning systems [34,61], where end users are allowed to build a global learning model in cooperation.
From our perspective, we suggest the threat model in this case should be formulated as malicious as possible, since the reliability of end users can be hardly guaranteed, similar to the case of DDoS attack [45].
Limitations and Future Directions.
In one repetitive test of GAA, we observed a fluctuated test result on MNIST, which, based on our detailed analysis in Appendix A.5, could probably occur when the reward distribution of malicious workers is almost indistinguishable from that of benign workers.
This may weaken the defense capability of GAA against attacks that aim at misclassification of targeted data samples instead of the overall accuracy we focus on in the current work.
This kind of targeted attacks can be highly stealthy in terms of worker behavior [8] and remains an open challenge in building robust distributed learning systems [24].
Due to the limited access to distributed learning systems in industry, we have tried our best to cover typical use cases in image classification, sentiment analysis and intelligent healthcare, where the latter two are based on datasets from real-world applications and are minimally preprocessed to reflect the characteristics of data in practice.
Nevertheless, more research efforts are required to provide a more thorough evaluation of GAA's security and performance in more application domains within industrial environments, which is very meaningful to be pursued as a future work.
Although the distributed learning paradigm we study remains a mainstream techniques, there do exist other distributed learning paradigms such as second-order optimization based paradigms [50] or model-parallel paradigms [33].
To generalize GAA to more distributed learning paradigms will also be an interesting direction to follow.
Byzantine Robustness of Gradient-Based Distributed Learning Systems.
Recent years, distributed learning systems under Byzantine attacks have aroused emerging research interests.
Mainstream works in this field mainly focus on Byzantine robustness of the distributed learning protocol we introduce in Section 2.
As we have reviewed in Section 3.2, most previous works are more interested in the defense side and usually utilize statistical approaches towards Byzantine robustness [4,11,16,31,62].
At the attack side, two very recent works [6,25] have devised carefully-crafted attacks against Krum and GeoMed, while the attack techniques are highly dependent on the target defense and are hard to be generalized to GAA.
Correspondingly, we in turn investigate the robustness of GAA under adaptive attacks on its own mechanism in Sections 6.
2 & 6.3.
During our paper preparation, we notice one recent work that also attempts to break the β = 0.5 bound [60].
The work is not learning-based and uses the loss decrease at the current iteration on the training set to rank the workers' credibility, which can be viewed a special case of our algorithm when the workers share the same training set and T = 1 in Algorithm 1.
Moreover, the work only considers a 4-layer convolutional network on CIFAR-10 as the only benchmark system, while we provide more comprehensive evaluations in four typical scenarios, including the case they studied.
Byzantine Problem in Other Contexts.
Aside from the aforementioned works on gradient-based distributed learning, there also exist some researches on other distributed learning protocols.
For example, Chen et al. proposed a robust distributed learning protocol by requiring workers submitting redundant information [15]; Damaskinos et al. studied the Byzantine robustness of asynchronous distributed learning [20]; another thread of works exploited the vulnerability of distributed learning protocols where a worker is directly allowed to submit the local model to the master [5,7,28].
In this paper, we focus on the gradient-based distributed learning system model as studied by the mainstream defenses and therefore none of the aforementioned works are directly related to this paper.Besides the Byzantine robustness in the context of machine learning, it has also been studied in many other contexts, like the multi-agent systems [46] and file systems [21], and was first studied in the seminal work by Lamport [37].
From a higher viewpoint on adversarial machine learning, challenges like adversarial example [30], data poisoning [9] and privacy issues [26,44,51] remain open problems and require future research efforts on building more robust and reliable machine learning systems.
In this paper, we have proposed the design of a novel RLbased defense GAA against Byzantine attacks, which learns to be Byzantine robust from interactions with the distributed learning systems.
Due to the interpretability of its policy space, we have also successfully applied our method to Byzantine worker detection and behavioral pattern analysis.
With theoretical and experimental efforts, we have proved GAA, as a promising defense and a strong complement to existing defenses, is effective, efficient and interpretable for guaranteeing the robustness of distributed learning systems in more general and challenging use cases.
All the defenses and experiments are implemented with Torch [19], which is an open-source software framework for numeric computation and deep learning.
All our experiments are conducted on a Linux server running Ubuntu 16.04, one AMD Ryzen Threadripper 2990WX 32-core processor and 2 NVIDIA GTX RTX2080 GPUs.
We simulate the distributed learning setting by sequential computation of gradients on randomly sampled mini-batches.
We design the following procedures to estimate the pairwise KL-divergence between datasets D i and D j on Healthcare, which consist of samples of form (x, y) s.t. x ∈ R n , y ∈ [K], where n = 1024 and K = 10.
Fig. 10 shows the heatmap of the KL-divergence among the local datasets on each worker and the full QV set.
KL(D i ||D j ) = 1 K × N N ∑ k=1 K ∑ c=1 p i (x k |y = c) log p i (x k |y = c) p j (x k |y = c)(3)Figure 10: Estimated KL-divergence among local datasets and the prepared validation set on Healthcare.However, it is true that it is challenging to estimate the KL-divergence when the QV set is very small.
To leverage the above algorithm for estimation, ideally we require the knowledge of the distribution where the QV set is sampled, so that we can estimate the conditional distribution p(y|x) via learning-based approaches.
Intuitively, if QV set contains more samples, the estimated conditional distribution is less biased and thus the error of estimating the KL-divergence is smaller.
To be concrete, the minimum requirement for conducting the estimation is, the QV set should contain at least one sample from each class and thus we can estimate the conditional distribution with support vector classifier or KNearest Neighbor (KNN).
As a future work, it would be a meaningful direction to study how to guarantee a low KLdivergence in a distributed learning protocol that may have privacy requirements [61].
which produces the curve of the predicted training loss in Fig. 11(d).
Compared with the empirical training loss curve, we find the prediction from Theorem 1 roughly conforms to GAA's empirical behavior in this case.
In one repetitive test of GAA, we noticed a fluctuated test result on MNIST under randomized attacks of p = 0.5, q = 5, initially β = 26/50, which we report below in Fig. 12.
In fact, through a larger number of repetitive experiments, we have observed this phenomenon only on MNIST but not on other three benchmarks.
We would like to clarify that this Figure 11: Empirical values of the theoretical terms in Theorem 1, alongside the predicted training loss curves.phenomenon is not a common case in repetitive tests and we reported this result here mainly because we think this singular phenomenon may help the readers understand the behavior of GAA more thoroughly.
Below, we further investigate the possible causes of this phenomenon.
As we can see from Fig. 12, the policy curve of GAA is more unstable than that in other cases, which in other words means GAA's credit on each worker fluctuates a lot.
This phenomenon indicates that GAA somehow could not recognize the always benign worker in this situation.
As a hypothesis, we speculate the reason as the low complexity of the MNIST task [17,40,49], which makes the reward from the workers' gradient on MNIST is not as distinguishable as in other cases.
To validate this point, we plot the distribution of the rewards (i.e., the relative loss decrease) yielded by the benign workers and the randomized Byzantine workers on each benchmark as follows.In detail, we set the worker number as 2 and set their roles respectively as benign and Byzantine with the RF tampering algorithm.
We execute the classical distributed learning protocol for 10 epochs over the corresponding training set and collect the yielded reward (calculated on the quasi-validation set of the same settings in Section 5.1) respectively from the benign and Byzantine workers for every 1k iterations.
We then plot the histogram of rewards on MNIST and CIFAR-10 in Fig. 13.
As we can see from Fig. 13, on CIFAR-10 the Byzantine worker always yields zero reward, which is highly divergent from that of the benign worker.
Differently, on MNIST the Byzantine worker and the benign worker yield rewards that follow similar distributions, which thus may bring difficulties for GAA to distinguish one from the other.
A noticeable point is the Byzantine worker tends to yield rewards that distribute in a slightly wider range than the benign one, which could be another cause of the instability in GAA's learning curve on MNIST.
This speculation is also supported by the MNIST case under static Byzantine attacks of ratio over 0.5 & 0.7 (in Fig. 4), where the baseline methods were observed to perform slightly stronger than the random-guess, while on other datasets they did not.
This phenomenon suggests that the model on MNIST still learns something from even incorrect gradients.
We sincerely appreciate the shepherding from Yuan Tian.
We would also like to thank the anonymous reviewers for their constructive comments and input to improve our paper.
This work was supported in part by the National Natural Science Foundation of China (61972099, U1636204, U1836213, U1836210, U1736208, 61772466, U1936215, and U1836202), the National Key Research and Development Program of China (2018YFB0804102), the Natural Science Foundation of Shanghai (19ZR1404800) s1.
MNIST: The first case is training a fully connected feedforward neural network for the hand-written digital classification task on the MNIST dataset [39], with 50 workers.
This public dataset contains 60000 28 × 28 images of 10 digits for training and 10000 for testing.
Each worker shares a copy of the training set.
The model consists of 784 inputs, 10 outputs with soft-max activation and one hidden layer with 30 rectified linear units (ReLu [36]).
A.4 An Empirical Validation of the Analytic ResultsWithout loss of generality, we take Theorem 1 as an example.
First, we explain the terms R, M, α and S one by one with more care and give the empirical values on Healthcare for demonstration.
In general, our terminology follows the conventions in [14], a standard text on optimization theory.
• Diameter R: The diameter R of a parameter space Θ (i.e., the feasible set of parameters of the underlying learning model) is defined as the maximal 2-norm of an element θ ∈ Θ.
Formally, R = sup{{θ 2 : θ ∈ Θ}.
On Healthcare, we estimate the 2-norm of the flattened parameter of the neural network during the learning process to estimate as the scale of R, which is plotted in Fig. 11(a).
The average value of R is around 11.05.
• Upper bound of gradient norm M: The term M is used to denote the upper bound of the gradient norm.
Formally, M = sup θ∈Θ ∇ θ ˆ f (θ, D train ) 2 .
On Healthcare task, we compute the 2-norm of the gradient submitted by the always-benign worker during the learning process to estimate the scale of M, which is plotted in Fig. 11(b).
The average value of M is around 0.36.
• Smoothness factor η: The term η occurs in our assumption that the loss function f is η-smooth.
Formally, the loss function f is said to be η-smooth ifWe estimate the empirical scale of α by calculating the expressions at both sides of the definition during the learning process, which is plotted in Fig. 11(c).
The average value of η is around 0.50.
• Size of mini-batch S: The term S denotes the training size of the mini-batch on which the always-benign worker calculates the gradient.
In addition, S is required to be no less than 1 (i.e., the training set contains at least one sample) or otherwise the theorem is invalid.
On Healthcare, S is set as 256.
• Finally, the max-norm of the loss function (which is implemented as a cross-entropy) is upper bound by the maximal entropy of the K-class classification task (i.e., f ∞ ≤ 1.
MNIST: The first case is training a fully connected feedforward neural network for the hand-written digital classification task on the MNIST dataset [39], with 50 workers.
This public dataset contains 60000 28 × 28 images of 10 digits for training and 10000 for testing.
Each worker shares a copy of the training set.
The model consists of 784 inputs, 10 outputs with soft-max activation and one hidden layer with 30 rectified linear units (ReLu [36]).
Without loss of generality, we take Theorem 1 as an example.
First, we explain the terms R, M, α and S one by one with more care and give the empirical values on Healthcare for demonstration.
In general, our terminology follows the conventions in [14], a standard text on optimization theory.
• Diameter R: The diameter R of a parameter space Θ (i.e., the feasible set of parameters of the underlying learning model) is defined as the maximal 2-norm of an element θ ∈ Θ.
Formally, R = sup{{θ 2 : θ ∈ Θ}.
On Healthcare, we estimate the 2-norm of the flattened parameter of the neural network during the learning process to estimate as the scale of R, which is plotted in Fig. 11(a).
The average value of R is around 11.05.
• Upper bound of gradient norm M: The term M is used to denote the upper bound of the gradient norm.
Formally, M = sup θ∈Θ ∇ θ ˆ f (θ, D train ) 2 .
On Healthcare task, we compute the 2-norm of the gradient submitted by the always-benign worker during the learning process to estimate the scale of M, which is plotted in Fig. 11(b).
The average value of M is around 0.36.
• Smoothness factor η: The term η occurs in our assumption that the loss function f is η-smooth.
Formally, the loss function f is said to be η-smooth ifWe estimate the empirical scale of α by calculating the expressions at both sides of the definition during the learning process, which is plotted in Fig. 11(c).
The average value of η is around 0.50.
• Size of mini-batch S: The term S denotes the training size of the mini-batch on which the always-benign worker calculates the gradient.
In addition, S is required to be no less than 1 (i.e., the training set contains at least one sample) or otherwise the theorem is invalid.
On Healthcare, S is set as 256.
• Finally, the max-norm of the loss function (which is implemented as a cross-entropy) is upper bound by the maximal entropy of the K-class classification task (i.e., f ∞ ≤
