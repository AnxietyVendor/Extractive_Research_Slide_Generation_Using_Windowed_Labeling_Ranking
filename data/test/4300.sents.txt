The strongest threat model for voting systems considers coer-cion resistance: protection against coercers that force voters to modify their votes, or to abstain.
Existing remote voting systems either do not provide this property; require expensive operations for tallying; or burden users with the need to store cryptographic key material and with the responsibility to deceive their coercers.
We propose VOTEAGAIN, a scalable voting scheme that relies on the revoting paradigm to provide coercion resistance.
VOTEAGAIN uses a novel deterministic ballot padding mechanism to ensure that coercers cannot see whether a vote has been replaced.
This mechanism ensures tallying takes quasilinear time, making VOTEAGAIN the first revoting scheme that can handle elections with millions of voters.
We prove that VOTEAGAIN provides ballot privacy, coercion resistance, and verifiability; and we demonstrate its scalability using a prototype implementation of its core cryptographic primitives.
Remote electronic voting in which voters cast their ballot outside a poll-booth environment, from their own devices, is susceptible to large-scale vote buying and coercion [29].
Yet, many deployed electronic voting systems [2,24,36] do not support coercion resistance.
This might be suitable in Western democracies where freedom and privacy are well rooted in society.
However, under authoritarian regimes [27] or in younger democracies [32], coercion is a serious problem.There are two kind of coercion-resistant electronic voting systems in the literature.
The first kind provides users with fake voting credentials that voters use/produce when coerced, enabling deletion of coerced votes [11,29].
This approach has several downsides: (i) voters need to store their true voting credential on their devices, (ii) the system cannot give feedback on whether the correct credential was used, and thus voters cannot be sure if their vote has been recorded correctly at the * This author's work was partly performed while working at Minsait, Indra.
time of voting, and (iii) voters need to convincingly lie while being coerced which may be a challenge.
The second kind relies on the revoting paradigm [1,23,30,33].
These schemes avoid the drawbacks associated with the fake-credential approach by allowing voters to submit fully to coercers, and later on supersede coerced votes by casting a new ballot.
This approach requires that the coercer cannot detect whether a voter has cast new ballots.
To achieve this, state-of-the-art schemes [1,30] require a quadratic number of operations, concretely a pair-wise comparison of all ballots, to privately filter superseded ballots.
As an example, for the Iowa Democratic caucus with only 176,574 voters, Achenbach et al.'s solution [1] would require 1.1 core years to filter the ballots.We propose VOTEAGAIN, a scalable coercion-resistant (re)voting scheme.
VOTEAGAIN's efficiency relies on two key insights: First, one can hide the number of ballots per user by inserting a deterministic number of dummy ballots which depends solely on the number of voters and the number of cast ballots.
Thus, it reveals nothing about the number of ballots cast by individual voters, hiding any (re)voting patterns induced by voters or coercers.
Second, because of the deterministic nature of the approach one can execute filtering in the clear, reducing the filtering time from quadratic to quasilinear: O(n log n) where n is the number of ballots.
As a result, for the Iowa caucus our construction requires under 14 core minutes.
We estimate that VOTEAGAIN using 224 cores (less than $50 on Amazon, or $75K on dedicated hardware) can filter hundreds of millions of ballots in hours.We make the following contributions:We introduce VOTEAGAIN, a novel remote electronic revoting scheme based on well defined and widely used cryptographic constructions.We introduce a novel efficient deterministic padding scheme that hides revoting at a low cost.
The complexity of the resulting filtering phase is O(n log n) where n is the number of ballots.
Our experiments show that in many practical scenarios the cost can be even lower.We show that previous definitions of coercion resistance in 29th USENIX Security Symposium 1553 JCJ [8,11,29] No 1 Yes n 2 Yes k-out-of-t k-out-of-t k-out-of-t k-out-of-t + AC Black-box [21] TTP No n Yes Unclear k-out-of-t TTP TTP Revote [1,30] k-out-of-t Yes n 2 Yes TTP k-out-of-t TTP k-out-of-t + AC Helios [2] revoting is not possible No TTP k-out-of-t TTP N/A VOTEAGAIN TTP Yes n log n No TTP k-out-of-t TTP TTP the revoting setting are vacuous.
We provide a new coercionresistance definition and we adapt modern definitions of ballot privacy [6] and verifiability [12,13] to the revoting setting.
We prove that VOTEAGAIN satisfies these definitions.We evaluate the scalability of VOTEAGAIN on a prototype implementation of the core cryptographic primitives.
Our results show that VOTEAGAIN can support elections with millions of users.
Coercion-resistant voting schemes fall under two categories: either they enable voters to generate fake authentication credentials or they allow the voter to revote.
Coercion-resistant schemes using fake credentials, introduced by Juels et al. [29] (JCJ), are used in several voting schemes [3,8,10,11].
In these schemes, the voter has both real and fake authentication credentials (or pre-registered passwords and panic passwords [10,16]).
When coerced, the voter lies to the coercer, using a fake authentication credential (or handling it to the coercer), resulting in a non-counted ballot.
Ballots cast with the real credential are counted.
These schemes provide the real authentication credential to the voter during registration phase (in which the coercer must be absent).
The voter must securely store these authentication credentials for later use, i.e., voters need to maintain cryptographic state.Coercion resistant schemes based on revoting allow voters to cast multiple ballots and then filter these ballots, typically counting the last ballot per voter.
For such a scheme to be coercion resistant, the filtering stage must be deniable [1], i.e., it must not expose which ballots are filtered, as this would expose revoting actions.
Black box filtering where a trusted third party (TTP) performs the filtering privately is deniable [21], but not verifiable.
To the best of our knowledge, there exist two publicly-verifiable deniable re-voting schemes [1,30].
To obtain public verifiability, these schemes use a distributed authority to compare each pair of ballots before shuffling to privately mark superseded ballots, requiring O(n 2 ) operations.
After shuffling, these marks are decrypted and the tallying server verifiably filters superseded ballots.
As literally specified in these papers, these schemes are 'not efficient for large scale elections'.
We confirm in Section 7 that Achenbach et al.'s scheme [1] cannot efficiently handle small elections of a hundred thousand users.Both the JCJ based and the private revoting based schemes offer a solution with a k-out-of-t assumption for coercion resistance.
However, on top of that, these schemes require the existence of anonymous communications channels (AC) to avoid coercion attacks such as forced abstention.For authentication, most schemes require users to store cryptographic state [1,8,11,21,29,30,35], or remember special passwords [10,16].
Helios [2] and Apollo [20] rely on regular username/password.
To improve verifiability by distributing the trust of the entity deciding which users are eligible voters, some schemes require that voters authenticate to k out of t parties [3,8,11,29].
However, this results in a complex registration phase for the user Revoting based schemes (including VOTEAGAIN) can be extended to this setting to reduce the trust assumptions required for authentication correctness (and hence verifiability).
Table 1 summarizes the comparison between VOTEAGAIN and previous work.
by the voters.
During the tally phase, the tally server and trustees post their proofs and results to the bulletin board.
Ad-hoc implementations [25] or blockchain-based implementations [18,19] would be suitable for VOTEAGAIN's PBB.Tally Server (TS).
The TS filters the ballots.
It adds dummy ballots, shuffles the ballots, groups them by voter, and selects the last ballot for each voter.Trustees.
The trustees mix and decrypt the selected ballots to reveal the outcome of the election.
Each trustee has a partial decryption key for a k-out-of-t encryption system.Threat model.
We assume an adversary A whose goal it is to coerce voters into casting votes for a particular candidate or to abstain.
This adversary, although computationally bounded, may coerce any voter -but not all voters.
Under coercion, the coerced voter does exactly as instructed (without needing to lie).
The coercer learns all information stored and received by the voter at the time of coercion.
We assume that after coercion, and before the end of the election, the coercer does not control a voter, such that the voter can cast at least one more vote.
We also assume that the user's means of authentication is inalienable [1], that is, a coercer can neither eliminate nor duplicate a voter's means of authentication.
While these assumptions are strong, we point out that so are the assumptions behind coercion resistant solutions that rely on fake credentials [8,11,29] (see Table 2).
Fake-credential based solutions assume that users cannot be coerced during registration and hence need inalienable means of authentication during this phase; that users can store and hide cryptographic key material and hence are required to have access to where this material is stored during the voting phase; and that users can lie convincingly.
These assumptions are not needed in VOTEAGAIN.
Our construction allows users to vote from any device, preventing coercion attacks that rely on destroying or stealing the voting device.In VOTEAGAIN, voters authenticate against the PA every time they vote to obtain an ephemeral voting token.
The PA must be honest with respect to verifiability and coercion resistance.
To enable quasilinear filtering we also require that the TS is honest with respect to coercion resistance.
This assumption is stronger than Achenbach et al.'s k-out-of-t assumption on the trustees [1], but their relaxation comes at a quadratic computational cost, see Table 1.
Finally, we require VOTEAGAIN to satisfy the following informal properties.
We formalize these properties in Section 6.
Table 3 summarizes the trust in each party required for achieving them.Definition 1 (Ballot privacy [6]).
Assuming at most k − 1 malicious trustees, no coalition of malicious parties (including the PA and TS) can learn the vote of an honest user.Definition 2 (Coercion resistance).
Assuming that the PA, the TS, and the PBB are honest, no coercer can use the information made public on PBB to determine if coercion was successful or not, provided that the election outcome does not leak this information.If voters use an anonymous communication system to post their ballots to the PBB, then the trust assumption on the PBB for coercion is not necessary.Definition 3 (Verifiability).
Assuming that the PA is honest, VOTEAGAIN guarantees that: (i) the last ballot per voter will be tallied, (ii) adversary A cannot include more malicious votes in the tally than the number of voters it controls, and (iii) honest ballots cannot be replaced.
If voters do not verify that their ballots are correctly appended to the PBB, ballots can be dropped or replaced by earlier ballots if those exist.
We sketch the key ideas of VOTEAGAIN.
For simplicity, in this section we omit the zero-knowledge proofs that parties 135 Figure 1: Basic filtering process by tally server without using dummies.
Ballots consist of an encrypted voter identifier ( ), an encrypted ballot index ( ), and an encrypted vote ( ).
use to show that they performed operations correctly.
We describe the protocols in detail in Section 5.1.
VOTEAGAIN proceeds in three phases: the pre-election phase, the election phase, and the tally phase.
During the preelection phase, the polling authority (PA) assigns to each voter i a random voter identifier vid i , and a random initial ballot index m i .
These values are known only to the PA.
Casting ballots.
During the election phase, voters can cast as many votes as they want.
To cast a vote, voter i first authenticates to the PA using her inalienable authentication means to obtain an ephemeral voting token.
This voting token includes: an encrypted voter identifier γ, containing vid i , and an encrypted ballot index I, containing m i .
After each authentication, the PA increases m i by one.
Next, the voter encrypts her choice of candidate as v. Finally, the voter sends the encrypted vote v, the encrypted voter identifier γ, the encrypted ballot number I, and a signature using the ephemeral token to the bulletin board.
Filtering ballots.
The encrypted voter identifiers and ballot indices enable the tally server (TS) to efficiently select the last ballot for each voter.
The TS uses the simplest mechanism possible: It shuffles the ballots, and then decrypts the voter identifiers and ballot indices.
The ballots can then publicly be grouped per voter, and the last ballot can be identified by inspection.
Finally, the trustees tally the last ballot of each voter.
See Figure 1.
Hiding patterns using dummies.
By itself, shuffling and filtering is not a coercion-resistant mechanism.
A coercer can still perform the 1009 attack [37] in which the coercer forces a voter to cast a specific number of ballots and looks for a group of that size in the filtering step.
If such group does not exist, the coerced voter has revoted.
In VOTEAGAIN, the TS inserts a deterministic number of dummy ballots and dummy voters before shuffling the ballots to hide such patterns while maintaining the simple public filtering procedure.We illustrate VOTEAGAIN's dummy mechanism in Fig- ure 2, in a scenario with two voters (A and B) where, the coercer forces voter A to cast 2 ballots.
At the end of the election phase the coercer observes 4 ballots and must determine whether A revoted (situation 2) or not (situation 1).
Without dummies, distinguishing these situations is trivial: if A revoted there is a group of 3 ballots and one of 1 ballot, and there are two groups of 2 ballots otherwise.
We add dummy ballots and voters to make both situations look identical.
The idea is to find a cover of ballots that could result from both situations.
For instance, adding to either situation two dummy voters that cast four dummy ballots total yields groups of 1, 2, 2, and 3 ballots.
This observation makes both situations indistinguishable for the coercer (Figure 2, right).
To ensure that the cover is independent from the voters' real actions, its appearance must depend only on the information available to the coercer: (1) the number of ballots n B posted by users to the bulletin board; and (2) the number of voters ν that cast a ballot.
The goal of the dummy generation strategy is to allocate dummy ballots such that the adversary observes the same cover regardless of the actual distributions of the n B ballots over ν voters.Consider the case of two voters, i.e., ν = 2, and 9 ballots, i.e., n B = 9.
As the filtering stage only reveals the sizes of the groupings and not their relation to voters the adversary's possible observations are (1,8), (2, 7), (3, 6), and (4, 5).
To cover all these scenarios one needs 8 voters (6 of which are dummy) casting 1, 2, 3, 4, 5, 6, 7, and 8 ballots, for a total of 36 − 9 = 27 dummy ballots.We add dummy ballots to real voters as well to reduce the number of group sizes that are possible.
For example, in the previous scenario one can pad the cases (4,8).
This can be covered with a cover containing voters with 1, 2, 4, 8 ballots each.
Building this cover requires only 2 dummy voters and 15 − 9 = 6 dummy ballots.
We stress that the number of added dummy ballots is independent of how the real ballots are actually distributed among the two voters.
(1, 8), (2, 7), (3, 6), (4, 5) to (1, 8), (2, 8), (4, 8),We refer to Section 5.2 for a generic, efficient algorithm for computing a cover.
Filtering with dummies.
Before shuffling the ballots, the TS adds dummy ballots to achieve the desired grouping.
To ensure that the TS cannot modify the election outcome, the TS assigns different tags to real and dummy ballots.To determine how to add dummies, the TS inspects the decrypted voter identifiers and ballot indices; determines a cover; and then computes how many dummies to add to exist- ing voters, and how many dummies to add to dummy voters.Consider the example in Figure 3.
Given 3 voters and 5 ballots, a cover with groups of size 1,1,2,2, and 3 suffices.
The TS therefore adds 4 dummy ballots in step 2: 2 dummies to existing voter 531, and two dummy voters, 74 and 103, each with one dummy vote.
After adding the dummy ballots, the TS shuffles all ballots.
Next, the TS decrypts the voter identifiers and ballot indices; groups ballots per voter, and selects the last ballot per voter.
The tags enable the TS to prove that it did not omit real ballots cast by real voters, and it did not count dummy votes cast by dummy voters.
In particular, the TS proves in zero-knowledge that the selected votes are either tagged as a real vote and therefore must correspond to the last ballot of a real voter; or the selected vote corresponds to a dummy voter (i.e., all the ballots in the group are tagged as dummies).
Finally, the TS privately discards the selected votes corresponding to dummy voters.
We refer the reader to Section 5.1 for the full details.Design choices.
Obtaining coercion resistance requires strong assumptions on some of the parties.
In this section, we discuss our design choices and motivate our trust assumptions (see Table 2 for a comparison with other protocols).
First, we believe that revoting is an easy to understand solution to achieve coercion resistance.
It requires no extra devices, no memorization, no interaction with several entities during registration, and no lying.
For instance, Estonians have used a revoting model for years for their elections [26] with 44% of the electorate having used internet voting [15].
Second, it does not require voters to securely store cryptographic material, allowing a vote cast from any device.
This further reduces the possibility of coercion attacks by confiscating the credential storage device.All coercion resistance schemes require absence of the coercer at some point during the process.
Fake-credential solutions assume that the coercer is absent during registration and at some point during the voting phase.
Revoting, instead, assumes that a voter will have time after the coercion to cast the last vote.
In the case of a remote registration process, a targeted attack will most likely succeed in both scenarios.
However, attacks scale much better in the fake-credential setting: coercers have the entire registration period (e.g., 24 days in Spain) to coerce a voter.
In contrast, coercers in the revoting setting must monitor all coerced voters after coercion to prevent them from revoting before the election closes.We decide to trade-off trust with respect to coercion resistance on the PA and TS to obtain high gains in usability and efficiency: trust on the PA relieves users from keeping cryptographic state; and trust on the TS enables VOTEAGAIN's quasilinear filtering of ballots.
Preliminaries.
Let be a security parameter.
Let G be a cyclic group of prime order p generated by generator g.
We write Z p for the integers modulo p.
We write a ∈ R A to denote that a is chosen uniformly at random from the set A.VOTEAGAIN uses ElGamal's encryption scheme given by: A key generation algorithm EC.KeyGen(G, g, p) which outputs a public-private key-pair (pk = g sk , sk) for sk ∈ R Z p ; an encryption function EC.Enc(pk, m) which takes as input a public key pk and a message m ∈ G and returns a ciphertext c = (c 1 , c 2 ) = (g r , m·pk r ) for r ∈ R Z p ; and an decryption algorithm EC.Dec(sk, c) which returns the message m = c 2 · c −sk 1 .
VOTEAGAIN uses deterministic encryption (with randomness zero) as a cheap verifiable 'encoding' for the ballot tags.
Because the encryption is deterministic, verifiers can cheaply check that the encrypted tags have been correctly formed.We use a traditional signature scheme given by: A key generation algorithm Sig.Keygen(1 ) that generates a public-private key-pair (pk σ , sk σ ); a signing algorithm σ = Sig.Sign(sk σ , m) that signs messages m ∈ {0, 1} * ; and a veri-fication algorithm Sig.Verify(pk σ , σ, m) that outputs if σ is a valid signature on m and ⊥ otherwise.We use verifiable shuffles [4] to support coercion resistance in a private way.
These enable an entity to verifiably shuffle a list of homomorphic ciphertexts in such a way that it is infeasible for a computationally bounded adversary to match input and output ciphertexts.VOTEAGAIN uses mixnets, a standard approach [7,28,34] to compute the election result given the filtered ballots output by the TS.
The trustees jointly run the Vote.DKeyGen(1 , k,t, n C ) protocol where is the security parameter , n C the number of candidates, t the number of trustees, and k is the number of trustees needed to decrypt ciphertexts.
This protocol outputs a public encryption key pk T and each trustee i obtains a private decryption key sk T,i .
To encrypt her vote for candidate c, a voter calls (v, π) = Vote.Enc(pk T , c) to obtain an encrypted vote v and proof π that v encrypts a choice for a valid candidate.
We denote the encryption of the zero candidate (i.e., no candidate) with explicit randomizer r ∈ R Z p by Vote.ZEnc(pk T ; r).
The algorithm Vote.Verify(pk T , v, π) outputs if the encrypted vote v is correct, and ⊥ otherwise.
Given a list of selected votes {V 1 , . . . ,V κ }, the trustees jointly run the (r, Π) ← Vote.MixDecryptTally(pk T , {V 1 , . . . ,V κ }) protocol to compute the election result r and a proof of correctness Π.
Internally, Vote.MixDecryptTally uses a standard verifiable mix network and verifiable decryption to shuffle and decrypt the ballots, and then computes the final result in the clear.
Any verifier can run Vote.VerifyTally(pk T , {V 1 , . . . ,V κ }, r, Π) to verify whether the result r is computed correctly.The TS uses standard zero-knowledge proofs of knowledge [22] to prove that it operated correctly.
We use the FiatShamir heuristic [17] to convert them into non-interactive proofs of knowledge.
We adopt the Camenisch-Stadler notation [9] to denote such proofs and write, for example,SPK{(sk) : pk = g sk ∧ m = EC.Dec(sk, I)}to denote the non-interactive signature proof of knowledge that the prover knows the private key sk corresponding to pk and that I decrypts to m under sk.
VOTEAGAIN proceeds in three phases: the pre-election phase, the election phase, and the tally phase.
See Table 4 for a summary of frequently used symbols.
In the pre-election phase, the PBB publishes the candidates, and the TS and the trustees prepare their cryptographic material.
The PA assigns a unique, random voter identifier vid i to each eligible voter.
The correspondence between voters and their identifiers is private to the PA.
The PA also generates a Procedure 1 (Setup).
To setup an election system with security parameter , electoral roll E, candidate list C , threshold k, and t trustees, the different entities run the Setup(1 , E,C , k,t)procedure.
First, they pick a group G with generator g and prime order p.
They then proceed with the following steps: that the trustees keep private.
In the election phase (see Figure 4), voters first authenticate to the PA to obtain an ephemeral voting token τ.
They use this token to sign their ballot β, and post the ballot on the bulletin board.
The bulletin board verifies that the ballot is valid.
We formalize this phase in three procedures:Procedure 2 (GetToken(id, Auth)).
On input her identity id and her inalienable means of authentication Auth:1.
The voter authenticates to the PA using Auth.
2.
The PA looks up the corresponding voter identifier vid i and ballot index m i .
Then, the PA encrypts the voter identifier γ = EC.Enc(pk TS , vid i ) and ballot number I = EC.Enc(pk TS , m i ) (it first encodes m i as an element of G), and increments the ballot index m i := m i + 1.
The PA hides the index m i from the user to prevent coercerswho can see what users can see under coercion -from being able to detect whether the user revoted.
3.
The PA creates an ephemeral signing key (pk, sk) = Sig.Keygen() and signs this key together with the encrypted voter identifier and ballot number:σ τ = Sig.Sign(sk PA , pk γ I)and returns the token τ = (pk, sk, γ, I, σ τ ) to the user.
4.
The user verifies the token τ = (pk, sk, γ, I, σ τ ) by checking that Sig.Verify(pk PA , σ τ , pk γ I) = .
Procedure 3 (Vote(τ, c)).
To cast a vote, the voter takes as private input the ephemeral voting token τ = (pk, sk, γ, I, σ τ ) and a candidate c ∈ C , and then proceeds as follows:1.
Encrypts her candidate c as (v, π) = Vote.Enc(pk T , c) to obtain ciphertext v and zero-knowledge proof of correct encryption π.
β = (v, π, pk, γ, I, σ τ , σ)where σ = Sig.Sign(sk, v π pk γ I σ τ ).
The voter posts the ballot β to the public bulletin board.
3.
The public bulletin board runs Valid(β), see below, to check that the ballot is valid, before appending it.
4.
Finally, the voter verifies that the ballot β has been appended to the bulletin board.
The PBB checks the correctness of the encrypted vote; of the user's signature using the ephemeral key pk; and the PA's signature on this ephemeral key pk, the encrypted voter identifier γ, and the encrypted ballot number I:Vote.Verify(pk T , v, π) = Sig.Verify(pk, σ, v π pk γ I σ τ ) = Sig.Verify(pk PA , σ τ , pk γ I) = .2.
The PBB checks that neither the encrypted vote v nor the key pk appear in any ballot β on the bulletin board.If any of these checks fails, the bulletin board returns ⊥, otherwise, the PBB returns .
In the tally phase (see Figure 5), the TS takes the ballots from the PBB, adds dummy ballots, and shuffles them.
Then, it selects the last vote per voter (see Figure 6).
To prevent dummy voters from causing overhead in the trustees' shuffle and decrypt phase, the TS shuffles the selected ballots and removes all ballots cast by dummy voters.
Finally, the trustees shuffle and decrypt the selected ballots from real voters.
Formally, we define two procedures, one to filter votes (Filter), and one to tally the selected ballots (Tally):Procedure 5 (Filter).
After the election closes, the TS selects the selected votes V i and produces the filter proof Φ.
If it aborts, it publishes the current Φ to the public bulletin board.1.
The tally server (TS) retrieves an ordered list of ballots [β 1 , . . . , β n B ] from the PBB, whereβ i = (v i , π i , pk i , γ i , I i , σ τ i , σ i ).
The TS verifies the ballots by running step 1 of Valid and verifies that there are are no duplicate votes v i or ephemeral public keys pk i on the bulletin board.
If any of these checks fails, the TS sets Φ = ⊥, posts it to the bulletin board, and aborts.
2.
The TS removes the proofs and signatures to obtain stripped ballots.
It provably tags the ballots as 'real' ballots using a deterministic ElGamal encryption (with randomness zero) of the value Figure 6: High-level overview of ballot filtering and grouping.
Let n B be the number of ballots, n D be the number of dummies, n T = n B + n D be their sum, κ be the number of voters plus number of dummy voters, and χ i be the number of (dummy) ballots for (dummy) voter i. First, the TS adds dummy ballots and proves they are well-formed.
Then shuffles all ballots without the proofs, hiding which ballots were dummies.
Then it verifiably decrypts both the encrypted voter identifiers γ i and the encrypted indices I i to group the ballots by vid and to select the last votes V i .
Finally, it outputs the selected votes V i without dummies.g 0 = 1 G , θ R = EC.Enc(pk TS , g 0 ) = (g 0 , g 0 pk 0 ) = (1 G , 1 G ):β i = (v i , γ i , I i , θ R ).
Next, the TS creates n D dummy ballots and provably tags them as such using a deterministic ElGamal encryption of the value g, θ D = EC.Enc(pk TS , g) = (1 G , g · pk 0 ):β i = (v ε , γ i , I i , θ D ),where i > n B and v ε = Vote.ZEnc(pk T ; 0 , which it adds, together with a proof π σ that this shuffle was performed correctly, to Φ.
4.
The TS now operates on each shuffled ballotβ i = (v i , γ i , I i , θ i ).
It decrypts γ i to recover the shuffled and decrypted identifier vid i .
It also decrypts I i to obtain the shuffled ballot index m i and proves it did so correctly:π dec i = SPK{(sk TS ) : pk TS = g sk TS ∧ vid i = EC.Dec(sk TS , γ i )∧ m i = EC.Dec(sk TS , I i )} It then adds C = [(vid 1 , m 1 , π dec 1 ), . . . , (vid n B +n D , m n B +n D , π dec n B +n D )] to Φ.The TS aborts and adds ⊥ to Φ if the decrypted ballot indices m i are not unique for a given voter identifier.
More precisely, it aborts if there exists indices i, j; i = j such that (vid i , m i ) = (vid j , m j ).
5.
The TS groups the ballots with the same voter identifier, and selects the ballot with the highest ballot index from each group.
Let G 1 , . . . , G κ be the sets of ballot indices grouped by voter identifier.
Consider group G j of size χ j .
Let j * = argmax k,k∈G j m k be the index for which the ballot index m j * is maximal.
Group G j either corresponds to a real voter, or to a fake voter.
The TS produces a reencryption V j of the encrypted votes as follows:(a) If the group G j corresponds to a real voter, then the TS simply reencrypts the vote corresponding to the last ballot, i.e., it picks r j at random and setsV j = v j * · Vote.ZEnc(pk T ; r j ),to a randomized encryption of v j * .
(b) If the group G j corresponds to a fake voter, then picks r j at random and sets V j to an empty vote: V j = Vote.ZEnc(pk T ; r j ).)
= g 1 .
Let G j = {i 1 , . . . , i χ j } and θ = ∏ χ j k=1 θ i k, then the TS constructs the proofπ sel j = SPK{(r j , sk TS ) : pk TS = g sk TS ∧ ((g 0 = EC.Dec(sk TS , θ j * )∧V j = v j * ·Vote.ZEnc(pk T ; r j ))∨ (g χ j = EC.Dec(sk TS , θ)∧V j = Vote.ZEnc(pk T ; r j )))}.
The TS adds the list of filtered encrypted votes 8.
Finally, the TS publishes the remaining votes S = [V 1 , . . . ,V n ] and the full proof Φ to the public bulletin board.F = [(vid 1 ,V 1 , π sel 1 ), . . . , (vid κ ,V κ , π sel κ )] to Φ.
6.
The list S D = [V 1 , . . . ,V κ ] of selectedThe filter procedure ensures that the TS cannot replace ballots by real voters: a selected vote must either correspond to a ballot by a real voter (condition a) or the selected vote is empty and the voter is a dummy voter (condition b).
Moreover, the TS can only remove votes cast by dummy voters.Procedure 6 (Tally).
To compute the final tally, the trustees proceed as follows:1.
The trustees verify that the TS operated honestly by running the VerifyFilter() algorithm (see below).
If VerifyFilter returns ⊥ they return (r, Π) = (⊥, ⊥).2.
Let S = [V 1 , . . . ,V n ].
The trustees jointly run the (r, Π) ← Vote.MixDecryptTally(pk T , S).
They publish the election result r and the zero knowledge proof of correctness Π to the public bulletin board.
Any external auditor can use the PBB to verify that all steps in the tally and filtering phases were performed correctly.
We define the following verification procedures:Procedure 7 (VerifyFilter).
Any party can verify that the filtering processes was performed correctly by running VerifyFilter().
This algorithm examines the content of the bulletin board and performs the following checks: 1S = [S D [i] | i / ∈ D].
If any of the checks fail, it returns ⊥, and otherwise.Procedure 8 (Verify).
Any party can verify the result r and proof Π against the public bulletin board.
To do so, they proceed as follows:1.
Verify that the TS operated honestly by running the VerifyFilter() algorithm.
If VerifyFilter returns ⊥, then return if (r, Π) = (⊥, ⊥), otherwise return ⊥.2.
Given the selected votes S, return the result of Vote.VerifyTally(pk T , S,r,Π).
In this section we provide a formal description of the dummy generation algorithm introduced in Section 4.
Finding a cover.
Formally, a cover is a set C = {(s i , z i )} i formed by groupings (s i , z i ) ∈ Z + × Z + .
Here, s i is the size of the ballot groups within that grouping, and z i is the upper bound on the number of times that such a ballot group can occur in any distribution of the n B real ballots among real voters.
We aim to find a cover of minimal size |C| = ∑ i s i · z i to minimize the number of dummies added.
A sufficient cover.
We derive an upper bound on the amount of dummies required to build a cover.
We do not use the number of real voters for this bound.
Let n B be the number of real ballots on the PBB.
For simplicity, assume padded group sizes are powers of two, i.e., s i = 2 i for i ≥ 0.
Given n B ballots, any distribution can have at most z 0 = n B groups of size s 0 = 1 (one ballot per voter).
Similarly, any distribution can have at most z 1 = n B /2 groups of size s 1 = 2.
Recall we pad ballot groups to the next bigger size, so a ballot group of 3 would be padded to one of size s 2 = 4 ballots, therefore z 2 = n B /3.
More generally, there can be at most z i = n B /(2 i−1 + 1) groups of s i = 2 i ballots.
The biggest possible group (if all ballots were cast by the same voter), has size 2 log 2 n B .
Therefore, the size of the cover |C| is bounded by:|C| = log 2 n B ∑ i=0 z i · s i = n B + log 2 n B ∑ i=1 2 i n B 2 i−1 + 1 ≤ n B + log 2 n B ∑ i=1 2 i 2 i−1 + 1 n B ≤ n B + log 2 n B ∑ i=1 2n B = (1 + 2log 2 n B )n B .
An efficient cover.
Knowing the number of real voters ν enables to obtain a tighter cover.
Consider the example of Sec- tion 4 with ν = 2 and n B = 9.
If we only consider n B = 9, one of the possible distributions of votes would be having s 1 = 9/2 = 4 groups of size 2.
However, knowing ν = 2 rules out this possibility.
There can be at most one group of size two: if there were 2 groups, each of the 2 voters could only cast 2 ballots, i.e., 4 ballots in total.
However, we know there are 9 ballots so at least one voter has voted more than twice, implying that s 1 = 1.
When the number of ballots grows this reasoning becomes intractable.
Consider ballot groups with group sizes, s = k i for i ∈ [0, . . . , log k n B ] for a real number k > 1.
We assume that n B > ν, otherwise the cover would be trivial: C = {(s 0 = 1, z 0 = ν)}.
We compute the cover as follows.1.
Consider groups of size s 0 = k 0 = 1.
As n B > ν, at least one voter must cast more than one ballot, resulting in (s 0 , z 0 ) = (1, ν − 1).
2.
Consider groups of size s i = k i .
We know that given n B , there can be at most α i = n B /(k i−1 + 1) groups of size k i .
The number of groups is also bound by the number of voters.
If ν · s i ≥ n B then all ballots can be assigned to the ν voters given groups of maximum size s i , and we set ν i = ν, otherwise set ν i = ν − 1 so that one voter is not in this grouping.
Finally, we need at least z i (k i−1 + 1) ballots to make z i groups, but we must have enough ballots left over to make ν groups in total, i.e., n B ≥ z i (k i−1 + 1) + (ν − z i ).
Rewriting gives boundβ i = (n B − ν)/k i−1 .
We set z i = min(α i , ν i , β i ).
Assuming n B > ν, the cover has |C| = ∑ log k n B i=0z i s i > n B ballots, necessitating dummy ballots, and ∑ log k n B i=1z i > ν groups, necessitating dummy voters.Creating dummy voters and allocating dummy ballots.
The TS recovers all voter identifiers vid by decrypting the γ i s, and the corresponding ballot indices by decrypting the I i s.So far, we assumed that ballot index sequences are continuous.
However, there can be gaps if some tokens were not used (e.g., the coercer does not use some tokens to identify index gaps in the filtering phase).
The TS first requests the number of obtained tokens n B from the PA, and adds exactly n B − n B dummy ballots to fill up any gaps, such that n B equals the number of obtained tokens.
The TS can create a dummy ballot for voter vid by setting γ = EC.Enc(pk TS , vid).
Given the current number of ballots n B and the number of real voters ν the TS computes a cover C = {(s i , z i )} i .
To this end the TS performs a search to find the best k, i.e., the one that gives the smaller cover.
In our experiments in Section 7, k tends to be in the 2 to 4 range, and the search takes less than a second.
The TS performs the following steps:1.
For every voter vid j , j ∈ {1, . . . , ν} with t ballots, let (s i , z i ) ∈ C be the cover group with the smallest size s i such that s i ≥ t. To ensure that dummy ballots are never counted, the TS adds t − s i dummy votes to vid j with descending (and unused) ballot counters smaller than the last cast vote by this voter.
2.
For each grouping (s i , z i ) ∈ C let z i be the number of real voters that were assigned to this group.
The TS adds z i − z i dummy voters.
For each dummy voter, it picks a random vid and initial ballot index m and creates s i dummy ballots with increasing ballot indices.The algorithms Filter and VerifyFilter are quasilinear in the number of real ballots n B .
The TS first adds n D dummies, so that the bulletin board contains a total of n T = n B + n D = O(n B log n B ) ballots (see the bound above).
All other steps in Filter and Verify filter are linear in n T .
The claim follows.
We analyze VOTEAGAIN's ballot privacy, verifiability, and coercion resistance.
We follow Bernhard et al. [6] and model the trustees as a single trusted party with keys (pk T , sk T ), but we note that the result holds when trustees are distributed.
We explicitly model the bulletin board PBB as an append only string BB.
To ease modeling, we use the following redefinition of our voting scheme V = (Setup, GetToken, Vote, Valid, Filter, VerifyFilter, Tally, Verify) where the algorithms output changes to the bulletin board rather than posting to it directly.
While Bernhard et al. model voter registration implicitly, we make the registration step explicit using the GetToken function because it forms an integral part of our voting scheme and may happen more than once.
The redefined algorithms in V are as follows:• Setup(1 , E,C ) as in Setup in procedure 1 but explicitly returns the public key pk = (pk PA , pk TS , pk T ) and the corresponding private keys sk PA , sk TS , sk T .
• GetToken(i) returns a token τ as in GetToken() in procedure 2.
• Vote(τ, c) returns β as in Vote(τ, c) in procedure 3 but does not post the ballot to the bulletin board.
Moreover, the voter first verifies the token τ as in step 4 of procedure 2, and returns ⊥ if it does not validate.
• Valid(BB, β) returns the result of Valid(β) in procedure 4 with respect to the bulletin board BB.
• Filter(BB, n B , sk TS ) as in Filter in procedure 5, but takes the number of registrations n B as explicit input, and returns S Φ instead of adding them to the board.
• VerifyFilter(BB, S,Φ) runs VerifyFilter from procedure 7 on BB = BB S Φ and returns the result.
• Tally(BB, sk T ) returns (r, Π) as in Tally in procedure 6.
• Verify(BB, r, Π) is as in Verify in procedure 8 operating on the bulletin board BB r Π.
We base our ballot privacy definition on the game-based definition by Bernhard et al. [6].
They model ballot privacy usingExp bpriv,b A,V (, E,C ): (pk, sk PA , sk TS , sk T ) ← Setup(1 , E,C ) b ← A O (pk, sk PA , sk TS )Output bOvoteLR(τ,c 0 , c 1 ):Let β 0 = Vote(τ, c 0 ) and β 1 = Vote(τ, c 1 ) If Valid(BB b , β b ) = ⊥ return ⊥ Else BB 0 ← BB 0 β 0 and BB 1 ← BB 1 β 1Ocast(β): an indistinguishability game which simultaneously tracks two bulletin boards, BB 0 for the "real" world and BB 1 for the "fake" world.
Only one is accessible to the adversary (see Fig- ure 7).
The adversary, controlling the polling authority (PA) and the tally server (TS), needs to determine whether the tally was evaluated over the "real" or "fake" world.
It can decide how voters vote.
Formally, the adversary can make calls to the oracle OvoteLR(τ,c 0 , c 1 ) to let a user with token τ cast a vote for candidate c 0 on BB 0 and a vote for c 1 on BB 1 ; and to the oracle Ocast(β) to cast ballots β (constructed by the adversary) on BB 0 and BB 1 .
Because the adversary controls the PA, it can create as many voting tokens as it needs.
The outcome of the election is always computed on the real bulletin board BB 0 .
The adversary can once ask to compute the outcome by calling the oracle Otally(S,Φ) where S, Φ is the output of Filter computed by the adversary.
The tally oracle aborts if S,Φ is not valid.
If the adversary saw the "real" result corresponding to BB 0 , the tally protocol proceeds as normal and publishes a correct tally proof Π with respect to BB 0 .
If the adversary saw the "fake" bulletin board BB 1 , the experiment simulates the tally proof Π with respect to BB 1 using the algorithm SimTally and returns the real result r.If Valid(BB b , β) = ⊥ return ⊥ Else BB 0 ← BB 0 β and BB 1 ← BB 1 β Oboard(): return BB b Otally(S,Φ) If VerifyFilter(BB b , S,Φ) = ⊥ return ⊥ BB b ← BB b S Φ BB 1−b ← BB 1−b Filter(BB 1−b , |BB 1−b |, sk TS ) (r, Π 0 ) ← Tally(BB 0 , sk T ) Π 1 = SimTally(BB 1 , r) return (r, Π b )Definition 4.
Consider a voting scheme V = (Setup, GetToken, Vote, Valid, Filter, VerifyFilter, Tally, Verify) for an electoral roll E and candidate list C .
We say the scheme has ballot privacy if there exists an algorithm SimTally such that for all probabilistic polynomial time adversaries A PrExp bpriv,0 A,V (, E,C ) = 1 − Pr Exp bpriv,1 A,V (, E,C ) = 1is a negligible function in .
In the extended version of this paper [31], we prove the following theorem.
Theorem 1.
VOTEAGAIN provides ballot privacy under the DDH assumption in the random oracle model.Bernhard et al. [6] also define strong consistency, to ensure that the result r does not leak information about individual ballots, and strong correctness to ensure that valid ballots are never refused by the bulletin board.
We restate these notions and prove that VOTEAGAIN satisfies them in the extended version of this paper [31].
Coercion resistance means that a coercer should not be able to determine whether a coerced user submitted to coercion -assuming it cannot learn this by seeing the result of the election (e.g., if there are zero votes for the selected candidate, the coercer knows the coerced user did not submit).
In VOTEAGAIN, this means that the coercer should not be able to determine whether a coerced user voted again, or not.
Existing coercion resistant models are insufficient.
Juels, Catalano and Jakobsson (JCJ) model coercion resistance by comparing a real-world game with an ideal game [29].
In JCJ, voters evade coercion by providing the coercer with a fake credential.
The real-world models normal execution.
The adversary plays the role of the coercer and chooses a set of corrupted voters and identifies the coerced voter.
Then, the honest voters cast their ballots (or abstain).
If the coerced voter does not submit she also casts her true ballot.
Thereafter, the adversary is given the credentials of all corrupt users, a credential for the coerced voter (which is fake if that voter resists), and the current bulletin board.
The adversary can now cast more ballots.
Upon seeing the result and the tally proof the adversary decides if the coerced voter submitted.
In the ideal game, the adversary is not shown the content of the bulletin board, and she is given the true credential of the coerced voter and can therefore cast real ballots for the coerced voter.
However, a modified tally function does not count ballots for the coerced voter cast by the adversary if the coerced voter resists.
Once the election phase is over, the adversary is shown only the tally result, not the tally proof.The JCJ model does not work for the revoting setting where the coerced voter casts another ballot after casting the ballot under coercion.
Achenbach et al. [1] propose a variant in which the coerced voter acts after the adversary has cast his votes, revoting if she resists or doing nothing if she submits.
Thereafter, the adversary is shown the new bulletin board and the resulting tally and proof.
In the ideal model, the adversary is only provided the length of the bulletin board.The model proposed by Achenbach et al. [1] does not capture coercion resistance.
Following the real/ideal paradigm, in the ideal game it should hold with overwhelming probability that the adversary cannot distinguish between a submitting and a resisting coerced voter.
Then, the proof would show that the adversary cannot learn more in the real world than it could in the ideal world.
However, in the ideal game proposed by Achenbach et al., the coercion resistance property does not hold.
The adversary can always distinguish between these two cases by simply observing the length of the bulletin board (which increases by one ballot if the coerced voter revotes).
Therefore, proofs in this model say nothing about whether the real scheme offers coercion resistance.
The Achenbach et al. [1] scheme seems to be coercion resistant, but coercion resistance does not follow from the proof in their model.
Finally, the model by Achenbach et al. does not capture the leakage resulting from the state kept by the voter, or as in our protocol, by the polling authority.
Our protocol deliberately hides the ballot counter from the voter, so that if the coercer coerces the voter again, it cannot determine whether the coerced voter re-voted based on this counter.
Achenbach et al.'s model does not capture this property, as the coercer is not allowed to coerce a voter more than once.A new coercion resistance definition.
We propose a new game-based coercion resistance definition inspired by Bernhard et al.'s ballot privacy definition.
The game tracks two bulletin boards, BB 0 and BB 1 , of which only one is accessible to the adversary (depending on the bit b).
We ensure that regardless of the bit b, the same number of ballots are added to the bulletin board.
The goal of the adversary is to determine b (see Figure 8).
Recall that we assume that the PA, TS, and trustees are honest with respect to coercion resistance.
We model honesty of the PBB (respectively, the use of an anonymous communication channel) by not revealing which voter posted to the bulletin board.To model submits versus resists, we provide the adversary with an OvoteLR(i 0 , c 0 , i 1 , c 1 ) oracle to let voter i 0 , a "coerced" cast votes using any user by calling Ogettoken(i) to obtain a voting token τ for voter i on the board that it can see, and a token τ for the other board.
It can then run β = Vote(τ, c) and β = Vote(τ , c) itself to create ballots for candidate c, on both boards and cast them using Ocast(β,β ).
Note that per our assumptions, the adversary does not get access to the voter's means of authentication.
Moreover, we require that the adversary always casts valid ballots to both boards (but the encoded candidate need not be the same).
Finally, the adversary can make one call to Otally() which performs the filtering step and returns the result r (always computed on BB 0 ) and the tally proof.
The result of Filter is accessible using Oboard.
To correct for leakage stemming from the tally result, as in the ballot privacy game, we simulate the filter and tally proofs if the adversary sees BB 1 .
This game models all the coercion attacks applicable to VOTEAGAIN:• The 1009 attack.
The adversary casts a ballot as co- Definition 5.
Consider a voting scheme V = (Setup, GetToken, Vote, Filter, VerifyFilter, Tally, Verify) for an electoral roll E and candidate list C .
We say the scheme has coercion resistance if there exist algorithms SimFilter and SimTally such that for all probabilistic polynomial time adver-saries A Pr Exp cr,0 A,V (, E,C ) = 1 − Pr Exp cr,1 A,V (, E,C ) = 1is a negligible function in .
In Appendix A, we prove the following theorem.Theorem 2.
VOTEAGAIN provides coercion resistance under the DDH assumption in the random oracle model.
In [12], our game does not model voter's intent, and assumes that the voting hardware, i.e., the device and software running Vote, is honest.
We refer to Cortier et al. [13] for a formal process-based computational model that does model verifiability with voter intent.
We note that the correctness definition by Juels et al. [29] was renamed to 'verifiability' by Cortier et al. [12], and therefore any model satisfying the latter also satisfies the former.
In a nutshell, a voting scheme is verifiable [12] if for n C corrupt voters, the result of the election always includes: (1) all votes by honest voters that verified whether their ballots were cast correctly, (2) at most n C corrupted votes, and (3) a subset of the votes by honest voters that did not check if their ballots were cast correctly.
These conditions ensure that while a malicious bulletin board can drop ballots of voters that do not check, it can insert at most n C new votes.Extending the current verifiability definition.
We extend the definition presented by Cortier et al. [12] for the revoting setting to explicitly consider the number of votes cast by a voter, see Figure 9.
The PA is honest, but the adversary controls the bulletin board, the TS, and the trustees.
The system implicitly tracks the number of tokens #tokens(i) that have been obtained by voter i.
The game tracks when each voter is corrupted in a (initially empty) list of corruption events C, and tracks the honest votes in HVote.
The adversary can call two oracles: Ovote(i,c) to request that honest voter i outputs a ballot for candidate c, and Ogettoken(i) to get a voting token for user i.
This models both corruption and coercion of voter i.
After a call to Ogettoken(i), voter i is considered corrupted until it casts an honest ballot using Ovote(i,c).
Eventually, the adversary outputs a bulletin board BB, the selected votes S and proof Φ, the election outcome r ∈ R, and a tally proof Π (line 3).
The adversary loses if Φ or Π do not verify (line 4).
If it verifies, the adversary wins if the result does not satisfy the three intuitive conditions above.The game computes the following groups of voters:• Corrupted (line 6): voters considered corrupted, i.e., voters that were once corrupted (by calling Ogettoken) and thereafter never cast a checked honest vote.
• Checked (line 7): voters that verified a ballot and were not corrupted thereafter.
• Unchecked (line 8): voters that were never corrupted, but did not check their ballots either.The game computes allowed candidates for honest voters:• AllowedVotes[i] (line 9): A list of candidates that voter i honestly voted for in or after the last checked ballot.
If voter i never checked a ballot, this list includes all candidates this voter ever voted for.The adversary wins if the result r verifies but violates any of the following conditions (lines 10-13): (1) For each honest voter that verified a ballot and was not thereafter corrupted (i.e., voters in Checked) the result should include either the candidate in that ballot, or a candidate in a later ballot.
This corresponds to the candidates {c V i } n V i=1 in the game.
(2) Of the honest voters that did not check their ballots but were never corrupted (i.e., voters in Unchecked), at most one candidate that the honest voter voted for (in any ballot) can be included.
This corresponds to the candidates {c U i } n U i=1 in the game.
Note that n U can be smaller than |Unchecked| or in fact zero.
(3) At most n C corrupted (or bad) votes were counted (i.e., the candidates {c B i } n B i=1 ).
In the game, the sum of these choices is modeled by the tallying function ¯ A,V (, E,C ):1 (pk, sk PA , sk TS , sk T ) ← Setup(1 , E,C ) 2 Set HVote ← / 0 and C ← / 0 3 (BB, S,Φ,r,Π) ← A O (pk, sk TS , sk T ) 4If VerifyFilter(BB, S,Φ) = ⊥ or Verify(BB S Φ, r, Π) = ⊥ return 0 5 Let Verified = {(i 1 , ctr 1 ), . . . , (i λ , ctr λ )} correspond to checked ballots.6 Let Corrupted = {i | (i, ctr) ∈ C ∧ ∀(i, ctr ) ∈ Verified : ctr < ctr} 7 Let Checked = {i | (i, _) ∈ Verified} \ Corrupted 8 Let Unchecked = {i | (i, _, _) ∈ HVote ∧ (i, _) ∈ C} \ Checked 9 Let AllowedVotes[i] = {c | (i, ctr, c) ∈ HVote s.t. ∀(i, ctr ) ∈ Verified : ctr ≥ ctr } 10 If ∃ c V 1 , . . . , c V n V s.t. c j ∈ AllowedVotes[i V j ] where Checked = {i V 1 , . . . , i V n V } 11 ∃ (i U 1 , c U 1 ), . . . , (i U n U , c U n U ) s.t. i U j ∈ Unchecked, c U j ∈ AllowedVotes[i U j ], i U j different 12 ∃ c B 1 , . . . , c B n B ∈ C s.t. 0 ≤ n B ≤ |Corrupted| 13 s.t. r = ¯ ρ({c V i } n V i=1 ) R ¯ ρ({c U i } n U i=1 ) R ¯ ρ({c B i } n B i=1 ) 14Then return 0, otherwise return 1Ovote(i,c):Let τ = GetToken(i) Add (i, #tokens(i), c) to HVote Return Vote(τ, c)Ogettoken(i): A,V , the adversary A has access to the oracles O = {Ogettoken, Ovote}.
Let τ = GetToken(i) Add (i,partial tallying, i.e., for any two lists S 1 and S 2 we have that¯ ρ(S 1 ∪S 2 ) = ¯ ρ(S 1 ) R ¯ ρ(S 2 )for a commutative binary operator R : R × R → R. Note that a tally function that outputs the number of votes per candidate naturally admits partial tallying.
GetToken, Vote, Filter, VerifyFilter, Tally, Verify) for an electoral roll E and candidate list C .
We say the scheme is verifiable if for all probabilistic polynomial time adversary A PrExp ver,0 A,V (, E,C ) = 1 − Pr Exp ver,1 A,V (, E,C ) = 1is a negligible function in .
In the extended version of this paper [31], we prove the following theorem.Theorem 3.
VOTEAGAIN is verifiable under the DDH assumption in the random oracle model.
We evaluate the performance of VOTEAGAIN using a Python prototype implementation of its core cryptographic operations.
2 We did not implement the GetToken protocol, but we note that as it relies on standard cryptography it can be implemented easily and cheaply; nor did we implement the bulletin board as it is not core to our design.
We use the petlib [14] binding to OpenSSL for the group operations using the fast NIST P-256 curve.
We ran all experiments in Linux on a single core of an Intel i3-8100 processor running at 3.60GHz.
We expect nation-wide elections to have much more processing power available.
For example, the Swiss CHVote system, which aims to support 8 million voters, has around 32 cores available per party in the system.
We also include performance estimates of running the system on a large machine with 8 Intel Xeon Platinum 8280L processors with 28 cores each, running at 2.7Ghz.
As our scheme is almost completely parallelizable (only the hash functions for the non-interactive zero-knowledge proofs need to be computed sequentially), we estimate a 90% parallelization gain: a speedup of 170 times when using the 8x28 cores with respect to the single core.For all experiments we empirically select the best cover size k by sweeping over values from 1 to 64.
In the majority of cases the optimal k is in the range [2,4].
Creating a ballot.
We use an ElGamal ciphertext to encrypt the voter's choice, and a Bayer and Groth [5] zero-knowledge proof of membership to show that the selected candidate is eligible.
Creating a ballot from 1000 eligible candidates costs 1.2 seconds, while verifying its correctness costs 0.17 seconds.
The size of this proof is 1.5 kB.Impact of revoting.
Figure 10 shows the overhead depending on the number of votes, in terms of number of dummies per real ballot.
This overhead influences the computation time of shuffling and filtering in the tally phase.
In the leftmost figure we model users' revoting behaviour as a percentage of the number of voters: 50% models that half of the voters revoted once, and 200% models that all voters revote twice.
We note that the overhead of 100% voters revoting once is equivalent to, for example, 25% of the voters revoting 4 times.
As expected, the overhead increases with both the number of voters and the number of revoted ballots.
However, even for 100 million voters revoting twice (200% revotes), the overhead is at most a factor of 32 (Figure 10 left).
Casting a vote takes time.
Thus, revoting patterns are constrained by the number of ballots that can be cast during an election.
We consider an election period of 24h (larger than most countries), and bound how often a single voter can vote 10 (1 ballot per second, per ten seconds, and per minute).
As this limits the number of voters with a large amount of ballots, we do not need large covers, reducing the overhead (see Fig- ure 10, center).
Similarly, assuming that all voters will revote is very conservative.
In a normal election one expects the vast majority of voters to vote once.
In Figure 10, right, we show the overhead when the number of voters that cast more than one vote is limited.
As fewer voters revote, the total amount of votes is smaller and so are the covers.Filtering.
We implemented a non-optimized version of BayerGroth's verifiable shuffle protocol [4] to implement steps 3 and 6 of Procedure 5.
We measure the execution time of filtering and verifying, when varying the number of voters.
Figure 11 left shows the times to run Filter and VerifyFilter on a single core machine.
Figure 11 middle shows the estimated processing times on the big 8 processor Xeon machine.
We estimate that the 8 processor machine can filter and tally the second round presidential election in Brazil (147 million registered voters) in 65 minutes if no voter revotes, and within 11 hours assuming 50% extra ballots and at most one ballot per voter per ten seconds.
We note that elections usually tally ballots per state, city, or smaller electoral district.
In general we expect the number of ballots to be much smaller.
All ballot groups in Figure 11 left and center have size one.
Figure 11 right shows the effect of larger ballot groups resulting from revoting and dummy voters.
As the average group size increases, the computation time goes down.
Therefore, Figure 11 gives an upper bound on the processing time, given a known cover size.For comparison we computed a lower bound on the filter cost of Achenbach et al.'s filter method by counting the number of group operations needed per ballot.
We used this number to compute the estimate in Figure 11 left.
A smalltown election with 100.000 ballots takes 5 core months to filter in their scheme.
Even on the large Xeon machine, an election with 1 million ballots takes over four months to complete.
Our method needs respectively 7 core minutes and 30 seconds.
The sizes of the tally proofs in VOTEAGAIN for these examples are 54 and 501 MB respectively.Smaller regions.
Many countries report election results per region, such as a province, a city, or a neighborhood.
In those cases, results can be computed per region at lower computation cost.
However, even in this setting, Achenbach et al.'s quadratic approach scales poorly.
We note that the allowable size of reporting regions depend on local regulations, with the smallest regions likely being cities or neighborhoods, which can easily total 100.000s of voters.
As Figure 11 (left) shows, even in this configuration, Achenbach et al.'s quadratic approach requires 3 to 4 orders of magnitude more computation resources than VOTEAGAIN.Tallying.
We also measured the execution time of a single step of the mix network -a single shuffle and one verifiable decryption -using our verifiable shuffle implementation.
Our results show that one step is a factor of three times faster than our filter protocol, e.g., mix-and-decrypting the 100.000 ballots takes less than 2 core minutes and 1 million ballots take less than 7 seconds on the Xeon machine.
Due to its complexity and cost, coercion resistance has been often overlooked in remote voting schemes.
We introduced VOTEAGAIN, a revoting scheme that enables cleartext filtering thanks to efficient deterministic padding.
VOTEAGAIN does not require users to store cryptographic material, and can efficiently handle millions of votes.
We provided a new coercion resistance definition and updated existing definitions for ballot privacy and verifiability to the revoting setting.
We have proven that VOTEAGAIN satisfies all of them.
Iñigo Querejeta-Azurmendi was supported by Ministerio de Economía, Industria y Competitividad (MINECO), Agencia Estatal de Investigación (AEI), and European Regional Development Fund (ERDF, EU), through project COPCIS, grant no.
TIN2017-84844-C2-1-R.
eProof of theorem 2.
We first specify how to construct SimTally and SimFilter.
As in the ballot privacy proof, SimTally(BB, r) simply simulates the proof of shuffle and the proof of correct decryption in Tally, so that regardless of the values in S, r is the correct outcome.The algorithm SimFilter(BB, n B , r) proceeds similarly.
It takes as input the bulletin board BB, which it uses to determine the number of ballots n B , the number of registrations n B , and the result r. Moreover, it derives the number of real voters n using r.
It uses these data to compute the cover, and it adds the correct number of dummy ballots (for these, it sets γ and I to random ciphertexts) to obtain B .
Then it computes a list of zero ciphertexts (encryptions of zero) of equal length, and simulates the shuffle proof π σ .
It then generates fake voter identifiers vid and m corresponding to the cover it computed earlier, associates these to shuffled ballot β i , and simulates the proofs π dec i .
Next, for each resulting group, it generates a random encryption of zero V j = Vote.ZEnc(pk T , r j ) and simulates the corresponding proof π sel j .
Then, it returns the randomness r j and the indices of the dummy voters corresponding to the cover it computed early.
Finally, for each remaining vote, it generates a random V j and simulates the shuffle proof π σ .
In this proof, we will step by step replace all the ciphertexts that depend on the bit b by random ciphertexts.
In particular, we first show that the adversary learns nothing about b during the election phase.
We then show that it also learns nothing about b during the tally phase.
The result follows.
(Note that contrary to the proof of ballot privacy we do not fix the value for b.) Game G 2 .
Game G 2 is as game G 1 , but we compute the result directly based on the ballots on BB 0 .
Let [β 1 , . . . , β n B ] be the list of ballots where, and m i = EC.Dec(sk TS , I i ).
Then compute the result:As per strong consistency, games G 2 and G 1 are indistinguishable.
Game G 3 .
Game G 3 is as game G 2 , but with all the zeroknowledge proofs replaced by simulations.
This includes the shuffle proof π σ , the decryption proofs of π dec i of the shuffled γ i and I i s, the reencryption proofs π sel i , and the shuffle proof π σ produced in Filter; as well as the tally proof Π 0 which we replace by the output of SimTally(BB 0 , r).
We use the random oracle to simulate this step, which is indistinguishable by the simulatability of the zero-knowledge proof system.
Game G 4 .
Game G 4 is as game G 3 but we do not decrypt the γ i and I i anymore when running Filter.
Instead, we proceed as follows.
All ballotson the bulletin boards are valid.
Hence, σ i is a valid signature by PA 0 resp.
PA 1 on γ i and I i .
Since the signature scheme is unforgeable, we know these ciphertexts were created by PA 0 resp.
PA 1 .
Hence, we can associate to them the corresponding plaintexts vid i and m i .
Moreover, we know the permutation used by the TS during Filter, so we can also provide the correct plaintexts in step 4 of Filter on BB 0 (recall the proofs of decryption π dec i are already simulated).
Game G 5 .
Game G 5 is as game G 4 , but we replace the ciphertexts γ i and I i in the token τ i by random ciphertexts for all tokens.
Similarly, we replace the γ i and I i ciphertexts for the dummy ballots by random ciphertexts.
Note that per the change in game G 4 we still associate the correct plaintexts vid i and m i in the Filter protocol.
A hybrid argument with reductions to the CPA security of the ElGamal encryption scheme shows that games G 5 and G 4 are indistinguishable.
This reduction is possible since we no longer need to decrypt these ciphertexts.
Game G 6 .
Game G 6 is as game G 5 , but we replace the encrypted votes v i in the OvoteLR() call by encryptions of the zero vector, i.e., v i = Vote.ZEnc(pk T , r) for a uniformly random randomizer r.
As in the ballot privacy proof, a hybrid argument with a reduction to the NM-CPA security of the ElGamal encryption scheme with zero-knowledge proof shows that games G 6 and G 5 are indistinguishable.Note that in this reduction we use the Odec of the NM-CPA challenger to decrypt votes in the adversary-determined ballots before computing the result r.Note that as of game G 6 , the adversary's view of the bulletin board before calling Otally() is independent of the value of b. (The ballots resulting from the OvoteLR call also contain a random ephemeral public key pk and the signatures σ τ and σ, but these are also independent of the actual voter selected.)
We now proceed to show that the adversary also cannot learn anything from the output of Filter.
Notice that, regardless of the value of b, the filter step is computed with the same number of voters ν, the same number of ballots n B and the same number of obtained tokens n B .
Therefore, the output of Filter applied to BB 0 and that of SimFilter applied to BB 1 should be indistinguishable.
In the following game steps we replace the ciphertexts after shuffling by zero-ciphertexts and show that these steps are indistinguishable for the adversary.Game G 7 .
Game G 7 is the same as game G 6 , but we replace the ciphertexts γ i , I i and θ i after shuffling by random encryptions of zero.
We proceed as if they still decrypt to the correct values.
Note that we already simulate the shuffle proof and decryption proofs.
Again, a hybrid argument with reductions to the CPA security of the ElGamal encryption scheme shows that the games G 7 and G 6 are indistinguishable.
This reduction is possible since we no longer need to decrypt these ciphertexts.
Game G 8 .
Game G 8 is the same as game G 7 , but we replace the shuffled encrypted votes v i by random encryptions of zero.
Similarly, we replace the randomizations, R, of the votes corresponding to dummy voters by the corresponding new randomization.
This causes the pre-selected votes V j per group to be incorrect, but this does not matter as we simulate the second shuffle proof, π σ , anyway.
As before, the indistinguishability of this step follows from the NM-CPA security of the vote encryption scheme.
Game G 9 .
Game G 9 is the same as game G 9 , but we replace the second shuffled votes V j by random encryption of zero.
This causes the selected votes V j after the shuffle to be incorrect with respect to the result, but this does not matter as we simulate the proof of the tally.
As before, the indistinguishability of this step follows from the NM-CPA security of the vote encryption scheme.
Game G 10 .
Game G 10 is as game G 8 , but we replace the filter and tally proofs on BB 0 by simulations: we set (S 0 , Φ 0 ) ← SimFilter(BB 0 , n B , r) and Π 0 ← SimTally(BB 0 , r).
Note that this difference is purely syntactic, as per the changes we made before, we already computed exactly the output of SimFilter on BB 0 and the result r.Clearly the resulting view is independent of b. And coercion resistance follows.
Proof of theorem 2.
We first specify how to construct SimTally and SimFilter.
As in the ballot privacy proof, SimTally(BB, r) simply simulates the proof of shuffle and the proof of correct decryption in Tally, so that regardless of the values in S, r is the correct outcome.The algorithm SimFilter(BB, n B , r) proceeds similarly.
It takes as input the bulletin board BB, which it uses to determine the number of ballots n B , the number of registrations n B , and the result r. Moreover, it derives the number of real voters n using r.
It uses these data to compute the cover, and it adds the correct number of dummy ballots (for these, it sets γ and I to random ciphertexts) to obtain B .
Then it computes a list of zero ciphertexts (encryptions of zero) of equal length, and simulates the shuffle proof π σ .
It then generates fake voter identifiers vid and m corresponding to the cover it computed earlier, associates these to shuffled ballot β i , and simulates the proofs π dec i .
Next, for each resulting group, it generates a random encryption of zero V j = Vote.ZEnc(pk T , r j ) and simulates the corresponding proof π sel j .
Then, it returns the randomness r j and the indices of the dummy voters corresponding to the cover it computed early.
Finally, for each remaining vote, it generates a random V j and simulates the shuffle proof π σ .
In this proof, we will step by step replace all the ciphertexts that depend on the bit b by random ciphertexts.
In particular, we first show that the adversary learns nothing about b during the election phase.
We then show that it also learns nothing about b during the tally phase.
The result follows.
(Note that contrary to the proof of ballot privacy we do not fix the value for b.) Game G 2 .
Game G 2 is as game G 1 , but we compute the result directly based on the ballots on BB 0 .
Let [β 1 , . . . , β n B ] be the list of ballots where, and m i = EC.Dec(sk TS , I i ).
Then compute the result:As per strong consistency, games G 2 and G 1 are indistinguishable.
Game G 3 .
Game G 3 is as game G 2 , but with all the zeroknowledge proofs replaced by simulations.
This includes the shuffle proof π σ , the decryption proofs of π dec i of the shuffled γ i and I i s, the reencryption proofs π sel i , and the shuffle proof π σ produced in Filter; as well as the tally proof Π 0 which we replace by the output of SimTally(BB 0 , r).
We use the random oracle to simulate this step, which is indistinguishable by the simulatability of the zero-knowledge proof system.
Game G 4 .
Game G 4 is as game G 3 but we do not decrypt the γ i and I i anymore when running Filter.
Instead, we proceed as follows.
All ballotson the bulletin boards are valid.
Hence, σ i is a valid signature by PA 0 resp.
PA 1 on γ i and I i .
Since the signature scheme is unforgeable, we know these ciphertexts were created by PA 0 resp.
PA 1 .
Hence, we can associate to them the corresponding plaintexts vid i and m i .
Moreover, we know the permutation used by the TS during Filter, so we can also provide the correct plaintexts in step 4 of Filter on BB 0 (recall the proofs of decryption π dec i are already simulated).
Game G 5 .
Game G 5 is as game G 4 , but we replace the ciphertexts γ i and I i in the token τ i by random ciphertexts for all tokens.
Similarly, we replace the γ i and I i ciphertexts for the dummy ballots by random ciphertexts.
Note that per the change in game G 4 we still associate the correct plaintexts vid i and m i in the Filter protocol.
A hybrid argument with reductions to the CPA security of the ElGamal encryption scheme shows that games G 5 and G 4 are indistinguishable.
This reduction is possible since we no longer need to decrypt these ciphertexts.
Game G 6 .
Game G 6 is as game G 5 , but we replace the encrypted votes v i in the OvoteLR() call by encryptions of the zero vector, i.e., v i = Vote.ZEnc(pk T , r) for a uniformly random randomizer r.
As in the ballot privacy proof, a hybrid argument with a reduction to the NM-CPA security of the ElGamal encryption scheme with zero-knowledge proof shows that games G 6 and G 5 are indistinguishable.Note that in this reduction we use the Odec of the NM-CPA challenger to decrypt votes in the adversary-determined ballots before computing the result r.Note that as of game G 6 , the adversary's view of the bulletin board before calling Otally() is independent of the value of b. (The ballots resulting from the OvoteLR call also contain a random ephemeral public key pk and the signatures σ τ and σ, but these are also independent of the actual voter selected.)
We now proceed to show that the adversary also cannot learn anything from the output of Filter.
Notice that, regardless of the value of b, the filter step is computed with the same number of voters ν, the same number of ballots n B and the same number of obtained tokens n B .
Therefore, the output of Filter applied to BB 0 and that of SimFilter applied to BB 1 should be indistinguishable.
In the following game steps we replace the ciphertexts after shuffling by zero-ciphertexts and show that these steps are indistinguishable for the adversary.Game G 7 .
Game G 7 is the same as game G 6 , but we replace the ciphertexts γ i , I i and θ i after shuffling by random encryptions of zero.
We proceed as if they still decrypt to the correct values.
Note that we already simulate the shuffle proof and decryption proofs.
Again, a hybrid argument with reductions to the CPA security of the ElGamal encryption scheme shows that the games G 7 and G 6 are indistinguishable.
This reduction is possible since we no longer need to decrypt these ciphertexts.
Game G 8 .
Game G 8 is the same as game G 7 , but we replace the shuffled encrypted votes v i by random encryptions of zero.
Similarly, we replace the randomizations, R, of the votes corresponding to dummy voters by the corresponding new randomization.
This causes the pre-selected votes V j per group to be incorrect, but this does not matter as we simulate the second shuffle proof, π σ , anyway.
As before, the indistinguishability of this step follows from the NM-CPA security of the vote encryption scheme.
Game G 9 .
Game G 9 is the same as game G 9 , but we replace the second shuffled votes V j by random encryption of zero.
This causes the selected votes V j after the shuffle to be incorrect with respect to the result, but this does not matter as we simulate the proof of the tally.
As before, the indistinguishability of this step follows from the NM-CPA security of the vote encryption scheme.
Game G 10 .
Game G 10 is as game G 8 , but we replace the filter and tally proofs on BB 0 by simulations: we set (S 0 , Φ 0 ) ← SimFilter(BB 0 , n B , r) and Π 0 ← SimTally(BB 0 , r).
Note that this difference is purely syntactic, as per the changes we made before, we already computed exactly the output of SimFilter on BB 0 and the result r.Clearly the resulting view is independent of b. And coercion resistance follows.
