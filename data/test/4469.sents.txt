Smart homes are under attack.
Threats can harm both the security of these homes and the privacy of their inhabitants.
As a result, in addition to delivering pleasing and aesthetic devices, smart home product designers need to factor security and privacy into the design of their devices.
Further, the need for user-centered security and privacy design is particularly important for such an environment, given that inhabitants are demographically-diverse (e.g., age, gender, educational level) and have different skills and (dis)abilities.
Prior work has explored different usable security and privacy solutions for smart homes; however, the applicability of user experience (UX) principles to security and privacy design is under-explored.
In this paper, we present a qualitative study to explore the development of smart home cameras manufactured by three companies.
We conduct semi-structured interviews with 20 designers and their collaborators, and analyze these interviews using Grounded Theory.
We find that UX was seen as helpful by our participants in fostering innovation in the design of privacy solutions.
However, UX was not used or considered in the design of security solutions due to an explicit need for established, tried-and-tested solutions (i.e., previous traditional security solutions that were seen as effective and reliable to fix certain design problems).
Drawing from the findings of our study, we propose a model of UX factors influencing security and privacy design of smart home cameras.
We also extract a set of recommendations to improve the security and privacy design of smart cameras.
We finally outline several areas for future investigation.
Homes are increasingly becoming instrumented, connected, and smart.
Devices (including light bulbs, doorbells, door locks, thermostats, and coffee makers) are designed to be Internet-connected and offer greater convenience, functionality, and energy efficiency.
However, the rise in the adoption and use of smart home devices is accompanied by new security and privacy threats [1].
Most smart home devices have always-on sensors that collect different types of data and then transmit the data over the Internet to various destinations [2,3].
The data can be used to spy on or create fine-grained inferences of device users and other home inhabitants.
Smart devices also increase the technical complexity of the security infrastructure in smart homes; non-expert home users are expected to protect themselves and their families from various attacks.
Increasing numbers of attacks, which have seen attackers taking control of smart homes (e.g., [4,5]), emphasize the need for protecting smart homes.
When such attacks happen, smart home device manufacturers often blame users for behaving insecurely (e.g., choosing weak passwords to secure smart home mobile applications [5]), while users ascribe the shortfalls to manufacturers.Smart home devices do not only affect the privacy and lifestyle of users of these devices, but also those of every inhabitant of the home.
In one example, a husband decided to unplug a smart home camera that his wife placed in their house (to check in on her family while she was away) because he felt that the camera was staring at him while he was making coffee [6].
This example illustrates, as Hassenzahl and Tractinsky [7] state, that product attributes must be linked to the needs and values of users (and bystanders in this instance).
This entails considering the affective consequences of technology on people, as well as the situatedness and temporality of the product (e.g., the actual or anticipated experience of a user-and a bystander-with the product).
Hassenzahl and Tractinsky argue that experience is a combination of elements including the product and internal states of the user (e.g., mood, expectations, active goals), which extends over time.
The elements interact and modify one another and, hence, understanding the user experience (UX) of security and privacy design is important for the successful adoption and use of smart home devices.
The importance of UX in the design of smart home devices has long been recognized and advocated for [8][9][10].
UX encompasses more than usability: it covers emotions, psychological responses, beliefs, perceptions, behaviors, and accomplishments [11].
Prior work has investigated how to achieve security and privacy in smart homes, focusing mainly on technical aspects (e.g., [12,13]).
Other work has also explored the security and privacy of smart homes in a user-centered way, specifically investigating users' knowledge of threats, attitudes, and expectations (e.g., [14]).
While studies suggest that factoring UX into security and privacy design can be challenging [15,16], research is needed to explore the practices of designers (or manufacturers) of smart home devices.To make a step in this direction, we conducted qualitative user studies with three companies that developed smart home security cameras.
We interviewed 20 participants from these companies (n=6, n=8, n=6) who were involved in the design process of smart cameras.
Our aim was twofold: (1) to understand how companies factored UX into the design of security and privacy solutions and (2) to investigate how UX influenced the design of security and privacy solutions.
We summarize our key findings and contributions below:1.
Product design teams used UX as a means of designing innovative privacy solutions.
2.
UX was not used to design innovative security solutions due to an explicit need for established security solutions.
3.
Data protection regulations triggered security and privacy considerations, but some regulations were regarded as impractical from a UX perspective.
4.
Conflicting interests among departments represented in the design team impeded the UX design of security and privacy solutions.The rest of the paper is structured as follows: we give an overview of relevant literature in Section 2.
We describe our methods in Section 3.
We present and discuss our results in Section 4 and Section 5, respectively.
Finally, we present our design recommendations in Section 6.
Several studies have explored users' experiences, values, needs, and concerns in relation to smart home surveillance (e.g., data collection, use, and sharing) [17][18][19][20].
Zeng et al. [14] interviewed 15 smart home users and found that their understanding of threats depended on the sophistication of their mental models.
Malkin et al. [21] surveyed 116 users of smart speakers and found that they were protective of the audio command history of children and guests, and that they strongly opposed third-party data tracking.
Malkin et al. [22] also surveyed 591 smart TV users and found that they disagreed with their data being shared with other parties despite a lack of understanding of regulations that protected their rights.
Geeng and Roesner [23] interviewed 18 smart home users to investigate multi-user interactions and found tensions during installation, normal use, and long-term use.
Abdi et al. [24] conducted interviews with 17 smart assistant users and found that they had limited understanding of data storage and sharing.
Naeini et al. [25] conducted a vignette study with 1,007 users to investigate privacy preferences, and found that users were more comfortable with data collected publicly, and that they would more likely consent to providing data if it were perceived as beneficial.
Apthorpe et al. [26] surveyed 1,731 smart home users to measure the acceptability of third-party data sharing.
They provided insights into existing privacy norms and extracted best design practices.Other studies have investigated the concerns and perceptions of bystanders-such as visitors or co-habitants-who do not make the choice to install smart home devices.
Yao et al. [27] ran focus groups and design activities with 18 participants and found three factors impacting the privacy perceptions of bystanders.
Bernd et al. [28] proposed to use the framework of Contextual Integrity to research the privacy of domestic workers that are affected by smart home devices, and the design process of product teams who build such devices.
As technology use evolves and becomes embedded in everyday life, the focus on usability (i.e., how easy, efficient, and effective technology is to use) becomes necessary, but insufficient.
Broader issues need to be considered, such as social communication, contextual trust, and even aesthetic aspects of security and privacy design.Dunphy et al. argue that it is crucial to understand how UX is factored into the security and privacy design of technologies [29].
However, there are several gaps in this space: Shava and Van Greunen [30] state there is a "missing link" between UX and usable security and privacy.
Other researchers have also reported on the lack of scientific research into UX and usable security and privacy [31,32].
There has been an increased focus on designing user-centered smart home devices [14,23,24,[33][34][35][36].
However, there has been little research into the role of UX in the security and privacy design of smart home devices [37].
Further, there has been little work exploring how designers and their collabora-tors think about the UX of these devices [38,39].
In particular, there is a research gap in how designers consider UX during the security and privacy design of smart home devices [40,41].
Bergman and Johansson [42] conducted a structured literature review of 150 smart home research papers and found that there was no research into how product teams factored UX into the security and privacy design of smart homes.
Without an in-depth understanding of designers' processes, challenges, and responsibilities, we argue that existing security and privacy issues in smart homes will persist.Existing research has focused mostly on the practices of developers.
Assal and Chiasson [43] interviewed 20 developers to explore real-life software security practices during the development lifecycle and found that security was not considered in the design stage.
Similarly, Waldman [44] interviewed 36 product developers and found that product teams did not consider privacy in their decision-making.
Further, previous research has found that many gaps existed among product teams on the one hand and security teams on the other hand, which included miscommunication and lack of security knowledge [45,46].
As a result, they found that some companies contracted developers who were security experts to act as an intermediary between product and security teams [47].
The literature suggests that security and privacy may pose UX challenges for smart home developers.
Oh and Lee [16] analyzed reviews of quantified self applications and found that privacy was a key problem affecting UX, security, and privacy design processes.
This was later confirmed by Bergman et al. [15], where they explored how 11 smart home companies captured UX requirements and found that security and privacy posed a UX challenge for designers.
Rowland et al. [48] found that smart home designers often faced tensions between UX and security in smart homes (e.g., trade-offs between strong authentication and users' ease of interaction with smart devices).
Unlike desktop computing, smart home applications span inter-connected physical and digital devices [49], increasing the complexity of factoring UX into the design of smart devices [50].
While UX and usability guidelines are established for desktop systems and web applications, guidelines that specifically target smart home devices are underresearched [49,51].
Moreover, smart home devices lack standardization and quality dimensions [52].
Some general UX rules are recommended for the design and implementation of smart home devices [53], but their effectiveness and suitability have not been explored in detail [54].
In summary, prior research has uncovered a variety of design-related security and privacy issues from the user perspective for which UX is critical (e.g., the need to consider aesthetic aspects of security and privacy).
Researchers have argued for understanding and designing the UX of security and privacy [29].
However, the literature reveals that there has been limited work on frameworks, models, and scientific research bridging UX, security, and privacy.
Our work takes a step to solve this problem by investigating the role of UX in the security and privacy design of smart home cameras.
We designed and conducted a qualitative user study of designers of smart home cameras based on approaches described in [55][56][57].
We interviewed 20 participants in the United Kingdom, focusing on understanding the design processes and practices of smart home cameras manufactured by three different companies A (n=6), B (n=8), and C (n=6).
We aimed to investigate the design, development, and implementation of three security camera products that had been in production for years.
We concentrated on the design of these products because smart home security cameras (i) have a growing adoption rate [58], (ii) are subject to increased security attacks [59], and (iii) are seen as particularly invasive by end-users [60,61].
Our institution's ethics committee approved this study.
Our work aims to address the following research question:RQ.
How do product design teams factor UX into the security and privacy design of smart home cameras?To address our main research question, we explore the following sub-questions:1.
How do designers and their collaborators make decisions during the security and privacy design process of smart home cameras?
2.
What are the different aspects of the design process of smart home cameras that explicitly deal with UX factors?
3.
What are the challenges that different stakeholders face when factoring UX into the security and privacy design of smart home cameras?
To recruit our participants, we posted flyers and distributed leaflets in the United Kingdom, and advertised the study on online platforms (e.g., LinkedIn).
We also recruited participants through snowball sampling, which allowed us to reach employees that were not easily accessible through other strategies.
At the time of recruitment, interested participants were employees who were active at their company and responsible for the design, development, or maintenance of a smart home camera product.
The participants we recruited from each company were all on the same development team and worked on the same product.We asked interested participants to complete an online screening questionnaire (see Appendix A).
We received 31 complete responses.
In addition to asking demographic ques- Sixteenth Symposium on Usable Privacy and Security 187 tions, we provided participants with a list of job titles and then asked them to choose the title that best described their position at the company (e.g., UX Designer, Security Engineer, Product Manager).
We describe the demographics of our participants in Table 1 in Section 4.
Additionally, we asked participants to provide information about their company.
We also asked them to specify the type of products that their company manufactured (e.g., security, lighting, or phone systems), as well as the specific products their company manufactured (e.g., cameras, hubs, voice assistants, lights).
Finally, we asked participants to estimate the number of employees who worked at their company.
The number allowed us to establish the company size (e.g., startup, mid-size, enterprise) since we were interested in targeting large-scale product development companies that were more likely to put effort into improving the UX of products [62].
We conducted semi-structured interviews with 20 employees working at companies that manufactured smart home devices: Company A (n=6), Company B (n=8), and Company C (n=6).
We used the funnel technique [63] to structure our initial interview questionnaire (study script), starting with general questions and then drilling down to specific ones.The interview started with general questions characterizing participants' role at the company (e.g., responsibilities, duration of employment), the type of products they designed or developed, and their perspectives on UX, security, and privacy.
Members of design teams referred to different groups of people (e.g., device purchaser, device administrator, and device user in the house) as 'users' without distinction.
We then asked questions related to requirements gathering and specification in the design phase, as well as questions about how UX was factored into the design process (e.g., UX in the security and privacy design process, UX design methods, techniques, and artifacts).
Finally, we asked specific questions related to the profession of participants: regulatory stakeholders were asked about data protection regulations, product liabilities, and regulatory affairs; management stakeholders were asked about roles and responsibilities related to security and privacy; security stakeholders were asked about security requirements, the design of security, security maintenance, and security breaches.We conducted our interviews remotely using Skype and Zoom.
We also audio-recorded and transcribed all interviews.
Interviews lasted for an average of 52 minutes.
Our interview questions can be found in Appendix C.
After creating our initial interview questions (see Appendix B), we conducted a pilot study with four smart home product designers at a local conference.
Two researchers recorded and analyzed the pilot interviews.
We used the findings to identify potential problems (e.g., adverse events, time, cost) in advance prior to conducting the full-scale study.
Drawing from our findings, we made the following changes:• We refined our interview questions to reduce bias and improve their quality.
• We changed our data analysis method from Thematic Analysis to Grounded Theory because we aimed to (i) develop a substantive theory, (ii) deeply explore design processes, and (iii) derive grounded recommendations.
• We were better informed of the average duration of our interviews, which turned out to be around 50 minutes.
We transcribed and analyzed all 20 semi-structured interviews using Grounded Theory, following Strauss and Corbin's procedure [64].
Grounded Theory enables the examination of topics and situations from many different angles, leading to comprehensive and deep explanations.
It can uncover beliefs and meanings behind behaviors and events, through examining both rational and irrational aspects of behaviors [65].
Four researchers in total analyzed the transcripts.
The primary researcher (who conducted the interviews) and a second researcher independently completed the initial coding of all interview transcripts.
To verify the credibility of the initial codes, a third researcher cross-checked the codes against the interview transcripts.
At the same time, the fourth researcher reviewed the initial codes and supporting quotes.
The four researchers discussed any differences and generated a codebook of 155 codes.
The researchers then grouped the codes into themes (axial coding) and categories (selective coding).
We observed data saturation [66][67][68] between the 18 th and the 20 th interview; i.e., no new codes emerged in interviews 18-20, and, hence, we stopped interviewing.
After creating the final codebook (see Table 4 in Appendix D), we tested for inter-rater reliability.
The average Cohen's kappa coefficient (κ) for all codes in our data was 0.81.
Cohen's kappa values over 0.80 indicate almost perfect agreement [69].
The University of Oxford Central University Research Ethics Committee reviewed and approved the study (CUREC/CS_C1A_19_049).
Before each interview, we asked participants to read an information sheet and sign a consent form that presented all the information required.
Participants had the option to withdraw at any point during the study.
Security, privacy, and regulatory matters are sensitive issues in big organizations like the ones we interviewed (see Table 2).
Our participants' corporate responsibilities as well as their company's reputation might have biased their responses.
To mitigate this, we explained to our participants that data would be collected and processed in accordance with the General Data Protection Regulation (GDPR).
Further, self-reporting bias is common in user studies [70].
Some participants might not have responded accurately to our questions because they did not remember specific details or wanted to be viewed as socially acceptable.
To maximize validity and minimize self-reporting bias, we avoided leading questions and relied on open-ended questions, inviting participants to provide in-depth answers in their own words.Finally, our qualitative work is limited by the size and diversity of our sample.
Following recommendations from prior work to interview between 12 and 20 participants [71], we interviewed 20 participants until new codes stopped emerging.
In this section, we detail the findings of our study.
We present our participant demographics (Section 4.1), and then discuss our key findings organized according to the main themes of our analysis, as illustrated in Figure Table 1 summarizes the demographics of our sample (n=20).
We interviewed 12 male and eight female participants.
Ages ranged from 25 to 52.
Ten participants had a college (or an undergraduate) degree, and ten had a graduate (or postgraduate) degree.
We divided our participants (n=20) into six groups of stakeholders based on employment: security stakeholders (n=4), regulatory stakeholders (n=3), UX stakeholders (n=5), management stakeholders (n=4), software stakeholders (n=2), and hardware stakeholders (n=2).
Table 1: Semi-structured interview participant demographics.
All participants (designers and their collaborators) followed an agile product development process, which included requirements analysis, design, development, testing, and maintenance [72].
In this section, we report on the requirements analysis, design, and development stages that companies A, B, and C (see Table 2) followed to develop products PA, PB, and PC (see Table 3).
We describe how GDPR influenced the development process of smart home cameras.
We also describe the challenges that UX design activities and smart homes introduced to our participants.
We briefly describe the processes and approaches of the design teams working at companies A, B, and C. All teams combined hardware and software development in agile and iterative design processes.Company A.
A cross-functional team that involved various stakeholders (e.g., senior UX/UI designers, software and mobile application developers, industrial designers, product managers) was in charge of questioning, exploring, defining, and making decisions related to the design of product PA (a camera).
The team ran multiple workshops with designers and developers to explore various ideas and techniques, become familiar with common design patterns, and understand the product's business strategy.
They followed a collaborative UX design process (multi-staged UX [73]).
Company B.
A self-managing agile team was in charge of the design and development of product PB (a doorbell).
The team was composed of different experts (e.g., designers, developers, engineers) who met on a regular basis to share data, communicate, collaborate, and discuss their progress.
No managers were controlling or directing the team because team members decided how to prioritize their work, manage their team, and achieve the goals of the project.
UX designers were in charge of eliciting functional and quality requirements by applying different UX activities related to the use cases of the product.
The requirements were extracted from user needs identified by UX research (e.g., personas, prototypes, interviews).
Company C.
A functional team-operating in a traditional organizational structure-adopted agile mindsets, principles, and practices and was in charge of the development lifecycle of product PC (a camera).
The team consisted of senior UX designers, product managers, and software developers.
The team leader reshuffled team members regularly depending on the project's needs and requirements.
Team members met regularly and were familiar with each other's work processes.
UX designers, developers, and content designers conducted user research and made explicit UX decisions during the early stages of their projects.
The team elicited requirements during the design, development, and implementation phases.
All three companies were required to comply with GDPR [74], which mandated Data Protection by Design (DPbD) [75] practices (as reported by A3, B7, and C15).
In practice, a DPbD approach requires companies to "consider privacy and data protection issues at the design phase of any system, service, or product."
[76] Delayed effect.
GDPR came into force on May 25 th , 2018, after the smart cameras of companies A, B, and C had been developed and released.
Product Counsel C15 said that the devices produced before the enforcement date were noncompliant with GDPR.
Similarly, Legal Counsel B7 said that the company's infrastructure that stored user data was not equipped to deal with GDPR requests.
Security Architect B12 stated that making changes in the existing product architecture required an increased demand for labor, money, and effort.Obtaining consent.
GDPR requires smart home companies to obtain clear and valid consent from users to the use of their data.
Due to the large amount of data exchanged in the ecosystem of company C's products, consenting to all uses of data was described as technically challenging by Product Manager C18.
UX Designer A2, who was familiar with GDPR, stated that asking users to consent to all uses of data in their ecosystem would be detrimental to UX.
A2 said: "I think it would be too overwhelming for users to see every single piece of data that we collect.
"Right to withdraw consent.
Under GDPR rules, smart home users have the right to withdraw their consent at any time, which requires companies to delete user data.
However, in the case of company C, Security Engineer C19 reported that their smart camera was often used with other company products as well as by third-party products (e.g., Amazon Alexa).
C19 explained that the increased number of devices that shared customer data made complying with this regulation demanding.
C19 described that their infrastructure "is not designed to destroy the data just like this, with one click."
C19 also mentioned that lack of control of data collected by third-party devices made complying with this regulation "very challenging."
In particular, C19 stated that it was difficult to determine whether third-party devices; e.g., Amazon Echo, were GDPRcompliant due to the lack of clear guidelines showing how third-parties collected and processed user data.Conflict between business and regulation.
Different and conflicting design goals could arise during the design phase.
Security Manager A3 reported dealing with a tension between commercial and regulatory stakeholders.
The Legal Department wanted some of the data collected from users to remain stored on users' cameras (i.e., offline); however, the Commercial Department requested all data collected to be stored on the company's cloud servers.
A3 explained: "The legal team asked to keep the data local only, but at the same time the commercial team wanted us to collect it.
I guess they wanted to monetize it."
This reported conflict highlights the important role of regulation in smart homes.
Participants reported different challenges during different activities of the UX design process.Identifying pain points.
UX designers (n=2) investigated data monitoring and collection in smart home cameras (through conducting research) to appropriately "identify the main sort of frustrations and pain points" (A2).
A2 interviewed smart camera users to research "acceptable areas of monitoring"; however, they could not identify the pain points resulting from monitoring.
To address this problem, A2 interviewed psychologists and visited existing customers in their house.
A2 was able to identify six major pain points related to video monitoring.
For example, A2 found that customers were concerned about cameras spying on them-by passively collecting data without their knowledge.Making UX design decisions.
UX designers made recommendations, but did not always make design decisions.
UX Designer B11 reported that despite conducting user research and suggesting UX-aware changes, they did not make any final decisions; they were instead made by the project manager.
B11 said: "Ultimately, it's always their decision at the end of the day, but it has always been difficult for my job to ensure that I'm providing the best experience possible.
"Fully understanding user behavior.
UX designers (n=3) mentioned that one persistent UX challenge they faced was to fully understand the security behavior of users.
A6 commented: "The reality is we won't know the exact behavior until the product is out."
The difficulty of understanding real user behavior is a challenge that is well-known to the usable security and privacy community [77].
Making hardware design changes.
Hardware stakeholders (n=2) faced issues when applying UX-related changes to existing products.
In company A, industrial designers faced difficulties when implementing a privacy feature which would visualize the on/off state of smart cameras.
Hardware Designer A5 explained that software developers were more flexible: "while mobile developers are flexible, we have to make early decisions that are not easy to change."
Similarly, Hardware Engineer C17 said there was not enough time to build hardware sprints.
The lack of flexibility (e.g., time, effort) made it difficult to apply UX changes to existing products.
Participants reported two smart home interoperability challenges that occurred during design and development.Heterogeneous devices.
Participants (n=2) stated that the integration between heterogeneous devices and company products was important to companies.
For example, most products in company A supported heterogeneous devices and services, such as Amazon Alexa, If This Then That (IFTTT) applications, and Apple's Siri (A1).
Third-party services (e.g., Amazon Alexa) "improve [d] [user] experience" (A2); however, they created difficulties for security stakeholders.
Security Engineer A4-who worked on encrypting the data exchanged between the company's ecosystem of products and Amazon Alexa-described the process as "complex" and "timeconsuming."
A4 specifically reported dealing with a legacy platform unable to send and receive encrypted messages with the API of Alexa Voice Service [78].
Securing connections between devices.
Security Engineer C19 stressed that their company's smart home devices had "solid security."
The challenge, however, was encrypting data exchanged among the increased number of devices in the company's ecosystem.
C19 explained: "Smart home devices are generally secure. . . The problem is the number of connections between all of those devices, they all have to be protected."
In addition, Product Designer B10 explained that the increased connections between devices, touch points, and objects would add to the complexity of this challenge.
Complexity in smart homes was previously reported in the literature [79].
In this section, we present how design teams applied UX principles and practices to the design of security features that end-users interacted with.
We found that UX was not factored into the design of security solutions due to lack of expertise and the misperception of security being a low-priority technical-only problem.
In addition, we found that GDPR and security audits motivated UX considerations.
Regulations and legal liabilities.
We found that regulation triggered security design considerations.
Although security was not explicitly factored into company B's design phase, regulations and legal liabilities required designers to consider some security requirements.
UX designers in company B attempted to consider regulatory requirements in the design phase although they faced several obstacles (i.e., high-level guidelines).
Legal Counsel B7 mentioned that the introduction of GDPR's "Data Protection by Design" requirements prompted doorbell design teams to implement new security features (i.e., stronger encryption during authentication).
Security audits looking into user behavior.
All three companies conducted security audits to establish how well their information system conformed to security standards and frameworks.
We found that security audits in companies B and C prompted security design considerations.
During a security audit led by Security Architect B12, a security review was conducted to investigate the password strength of the accounts of PB's users.
B12 found several instances of poor password behavior, which prompted an evaluation of password strength as well as the creation of UX-aware password requirements (e.g., the addition of password strength meters).
Design of security features was not explicitly anyone's responsibility.
Among our participants (n=20), no one took responsibility for the design of security features.
Participants (n=5) who handled security design tasks said they were not accountable for security design issues.
Those participants did not have UX design expertise.
For instance, Product Manager A1, who made security design decisions based on their understanding of common security practices, said that the Information Security Team was responsible for all matters related to security, including security design.
However, Security Engineer A4 from the Information Security Team dismissed any responsibilities related to the design of security features.Security design was a low-priority concern.
For some stakeholders (n=2), security design was acknowledged but perceived as low-priority.
As a result, minimal efforts were made to introduce security stakeholders in design teams that handled UX design.
Security Manager A3 explained that the budget of the Information Security Team was limited, and that adding security experts to the design team was a "luxury" they could not afford.
Moreover, Product Designer B10 expressed similar thoughts when discussing the addition of a 'usable security expert' to the design team.Security was seen only as a technical problem.
Many participants (n=15) described security as being only a technical problem that should be addressed from a technical perspective.
As a result, participants expected that security to be exclusively handled by developers and security experts.
This perception gave little to no consideration to social aspects of security.
For instance, when we asked UX Designer B11 why security was not part of the design process, B11 stated that this was "a development question."
Security Engineer A4 had a similar response: "Designers do not have any security expertise and it doesn't make sense to expect them to handle security problems."
This finding is not novel, but confirms the existence of a long-term challenge in HCI where security is treated as a technical problem [80], regardless of ongoing efforts to bridge the gap between social and technical aspects of security design [77].
Security features were not designed by usable security or UX experts.
In company C, sensitive features related to the connectivity of security cameras, firmware upgrades, and registration were designed by Software Developer C20.
Similarly, Product Manager A1 -in company A -did not "see the value" of including security experts in the design team, and chose security features -such as authentication -based on their understanding of common security practices.UX designers had no sight of security requirements.
Security requirements were not always present in the UX design phase.
Experience Designer B9, who played a core part in the design of the doorbell (PB), said that the requirements he was provided with did not include data protection or security requirements.
Similarly, UX Designer A2 explained that the security of registering and processing data was discussed during the design phase.
However, there were no requirements related to security design: "There wasn't specific kind of UX work around data protection or user protection or something like that.
"Security design considerations were ad hoc.
For some participants (n=5), features handling sensitive information (e.g., authentication, software patches, access to video footage) created security design considerations on an ad hoc basis.
For example, Mobile Developer B13 designed the software update development process for the doorbell mobile application.
B13 strongly valued the design of update features because they realized that these features could be used to deliver security updates.
In all five cases, ad hoc design security considerations were triggered by non-experts of security design: management stakeholders (n=3) and development stakeholders (n=2).
This finding confirms Assal and Chiasson's study results [43], which suggested that ad hoc security considerations are fragile because non-experts of security design (e.g., developers) could fail to identify security-sensitive features.Lack of security experts in design teams.
The product design teams of companies A, B, and C did not include security experts.
In company A, the Information Security Team was not involved in the design phase of their smart camera.
Justifying the decision, Security Manager A3 stated that all company employees underwent annual training and followed the company's "information security management framework.
"Security was only considered at the implementation stage.
For some security stakeholders, security design was acknowledged but was not seen as a priority.
The Security Team in company B prioritized working with the Development Team over the Design Team.
Security Architect B12 who worked with the doorbell Development Team said: "I know that we can get involved with designers, but well, it's more efficient to work with the development team.
"Security was reactive and not proactive.
Security stakeholders (n=2) reported that security design was treated as reactive, rather than proactive, in companies A and B. Companies preferred a reactive approach, in which they made security design considerations or changes based on security incidents reported by customers.
For example, Security Architect B12 reported that their security teams had implemented multifactor authentication as an option to secure user accounts after successful account hijacking attacks were reported.
In this section, we present how design teams applied UX principles and practices to the design of privacy features that end-users interacted with.
We found that UX was factored into the design of privacy solutions in companies A and B through considerations of consent, transparency, and user control.
However, in company C, UX was not considered in the design of privacy features due to lack of expertise and relying on a general understanding of privacy issues and product use.
Giving users control.
Companies A and B gave customers more control of their privacy settings, which UX designers reported to increase trust.
For example, both companies implemented a privacy mode in their mobile application in order to allow users to stop camera monitoring.
In company A, designers also aimed to make users "feel in control" by adding (1) a visible on/off feature that showed the current state of cameras and (2) a privacy mode to give users "peace of mind" (A2)-allowing users to automatically or manually disable cameras when using their mobile application.
In company B, UX Designer B11 explained that they added a private mode because their customers shared their cameras with family members: "We do have a privacy feature in our product which allows you to switch the [...] whenever users want to have privacy.
[...] When we interviewed users, we realized that a lot of them share their camera with others, mostly family members.
"Being transparent with users.
Participants (n=7) reported that their company made numerous efforts to be transparent with users.
Legal Counsel B7 described that the Legal Department at company B worked with UX designers to create user-friendly FAQ pages that explained how their company collected and processed user data, as well as the measures they took to protect data.
Further, B7 mentioned that the company constantly reminded users of their right to get their data deleted (by sending regular reminder emails).
On the other hand, company A, which had to deal with multiple security vulnerabilities in the past, had recently updated its data breach incident response plan to inform customers of data breaches (A3).
UX Director A6 helped the company use best practices to ensure that affected users had the best experience possible.Obtaining explicit consent from users.
UX designers (n=2) described different projects that looked into obtaining explicit consent from users in relation to data collection and sharing.
UX Director A6 described an on-going project looking into obtaining consent through visual indicators instead of textheavy documentation that would be difficult for users to read.
UX Designer B11 worked on developing user-friendly consent notifications for the camera's mobile application.
In addition, developments were made to allow customers to change their own privacy settings based on their needs.Ensuring smart home cameras were not 'creepy' or intrusive.
UX designers (n=3) conducted user research with the aim to design smart home products that were not 'creepy' or 'intrusive'.
In company A, the goal of UX designers was to ensure users felt comfortable with their camera (PA), and that it did not make users feel that it was a "tool of surveillance" (A6).
To achieve their goal, UX Designer A2 interviewed psychologists and visited existing customers in their house to identify acceptable and non-intrusive "areas of monitoring.
"In company B, Experience Designer B9 assisted in the design of a feature which allowed cameras to "detect human activity based on geographic location."
B9 explained that the feature allowed users to automatically disable their smart home camera when they were at home and, hence, the device did not feel "creepy."
Designers of privacy features lacked expertise.
In company C, privacy features were designed by stakeholders (n=2) who did not possess design or privacy expertise (e.g., developers, product managers).
Software Developer C20 designed the privacy mode settings of the camera's mobile application during the development process.
C20 made privacy design decisions based on their own understanding of sensitive data.
Similarly, Product Manager C18 made privacy decisions related to a feature that allowed family members to disable video monitoring and notifications.
Both stakeholders did not refer to any design or data protection guidelines.
C18 said: "We didn't follow any requirements, no.
[...] I don't know why, I wasn't aware of any requirements.
"Some privacy solutions were designed based on a general understanding of product use.
Company C's Product Design Team appeared to deal with privacy design based on a general understanding of product use, rather than a thorough investigation of the specific context of use.
For instance, Product Manager C18 believed that privacy concerns of users would better be dealt with by understanding users in a broad and wider context of user-centered design.Privacy was not explicitly discussed during user research.The Product Design Team of company C did not explicitly discuss privacy during the design phase.
Senior UX Designer C16 -who worked with product designers, engineers, and managers -said that privacy was not discussed during the user research phase when user interviews were conducted.
We found that innovation cross-cut UX with security and privacy.
In this section, we describe how innovation seemed to enhance the design of privacy solutions, but also to impede the design of security solutions.
Sixteenth Symposium on Usable Privacy and Security 193 New privacy features were supported by qualitative and quantitative UX research.
UX stakeholders (n=4) working at companies A and B adopted a mixed qualitativequantitative approach to build new features that addressed user privacy (e.g., concerns, pain points, expectations) during the design phase.
To design features related to camera monitoring, UX Designer A2 conducted qualitative interviews with users as well as observed users in their homes to address any privacy concerns.
UX Director A6 conducted quantitative research by collecting and analyzing survey data to design a visible indicator that showed whether a camera was turned on or off.
A6 also used existing quantitative data from Google Analytics to prioritize which privacy features to implement.
Experience Designer B9 created detailed storyboards and personas to visualize how their doorbell would be used in users' homes and whether it would be intrusive.New privacy features were evaluated through usability testing.
UX stakeholders (n=2) conducted usability testing of new privacy features introduced by company B.
This was used to ensure that new privacy features did not negatively affect customer experience.
UX Designer B11 delivered usability testing results to the Product Design Team based on the analysis of mobile application prototypes.
B11 mentioned that among these prototypes, some requirements were related to privacy features.
B11 explained that users were observed interacting with and changing privacy settings.
Similarly, Experience Designer B9 conducted usability testing of the doorbell privacy features and was able to identify issues that prompted design considerations.
Security solutions were tried-and-tested.
Security experts (n=3) mentioned that their companies' Information Security Team did not design their own security solutions.
Instead, they used existing security solutions in their company's security protection paradigms, a practice known as tried-and-tested security.
Additionally, non-experts also made security design choices supported by their own understanding of common security solutions.
Product Manager A1, who worked with the Design Team that did not include security experts, chose the "username and password" authentication mechanism since it was familiar, widely-used, and accepted in industry.New security solutions increased uncertainty.
Participants (n=2) explained that incorporating tried-and-tested security solutions avoided uncertainties that arose out of the introduction of new security features.
Product Manager C18 mentioned that new security solutions were likely to create usability concerns due to lacking information on how users would interact with such features.
Similarly, Security Engineer C19 mentioned that attempts to introduce new security features were discouraged in the Security Team.
C19 explained that introducing new security features would increase security risks due to lacking the knowledge required to design these features.
We found that trust heavily influenced UX design choices: product teams aimed to build customer trust through better privacy experiences, and also aimed to protect trust relationships with their customers through data protection policies.Building and nurturing trust through privacy experiences.
We found concerted efforts in the companies that aimed to build a culture of fostering trust.
In company C, Product Counsel C15 explained that employees were encouraged to take an interest in and care about protecting user privacy.
Similarly, in company B, Legal Counsel B7 described efforts put into creating a customer-first culture, where user privacy was not only seen in development processes but also discussed and encouraged culturally among product teams.Protecting trust relationships through data protection policies.
Product teams (n=5) used data protection policies to protect their company's reputation and build user trust.
Many companies had established policies to deal with security vulnerabilities and attacks.
For example, Security Manager A3 reported that his company adopted an incident response plan in case of a breach, in order to maintain its reputation, which we identified as a powerful motivator for companies to take security measures.
Similarly, Security Architect B12 reported that their Security Team had invested in "developing wellfounded requirements" for responding to security incidents, even when incidents resulted from users' incompetence (e.g., falling for a phishing attack, a compromised home router).
Security Engineer C19 said their company drafted a "responsible disclosure policy" which dealt with managing security vulnerabilities reported by users.Overall, our interview participants identified that customer trust was strongly linked to data protection: security was needed to mitigate loss of trust arising from exploiting security vulnerabilities.
Further, user privacy was used by product teams to build and nurture trust relationships.
All product teams used an agile methodology to drive the development of their smart home products.
We found that the practice of using tried-and-tested security solutions inhibited innovation in security design.
In addition, the perception of security being only a technical problem, for which there were 'best-practice' technical solutions, limited the consideration of social and interactive aspects of security.
In particular, it created a gap between UX considerations and security design (e.g., UX designers had no sight of security requirements).
Despite the gaps that we found in security design, our results show companies innovated in the privacy design space (e.g., company B created a novel geographic-based privacy feature).
Our data shows that UX stakeholders in design teams elicited and handled privacy requirements.
The practice of using UX design principles to respect user privacy (e.g., giving users control, avoiding creepiness and intrusiveness) seemed to encourage innovation in the privacy space.
Moreover, we found that companies were motivated to preserve a trust relationship and build trust with their customers, as privacy or security failures (e.g., intrusive or vulnerable products) would undermine that relationship.
Finally, regulations (e.g., GDPR) legally required design teams to consider data protection by design in their requirements.
Our results uncover complex challenges and limitations that product designers faced: challenges arising from complying with GDPR; the importance and role of building trust; barriers to factoring UX into security design solutions.
In this section, we use our findings to discuss the wider role of innovation in designing security and privacy solutions, as well as the implications of adopting a user-centered agile approach to data protection.
We also highlight areas for future work.
All novel issues related to smart home security and privacy point to significant challenges where innovative solutions are necessary.
Despite recognizing the importance of security in the design process, our results show design and security teams are less innovative due to existing practices and perceptions.
These practices include favoring tried-and-tested security solutions or procuring security solutions from reputable vendors.
This finding highlights a desire to avoid novelty and a preference to 'follow the crowd' in the design of security.Further, the perception of security as only a technical problem, for which there are "best-practice" technical solutions, limits design considerations for security solutions (e.g., authentication consisting of only username and password combinations).
Many smart home devices are designed for operating in privacy-sensitive environments (e.g., personal spaces).
Given the relative immaturity of the smart home device space, tried-and-tested solutions are not particularly suitable, and innovative solutions are required.
For example, current designs do not accommodate the diversity of social aspects of smart home security and privacy (e.g., the nuance between a device being in a shared space in a flat-share vs. being in a shared space in a single-family household).
While we found no evidence of innovation in security design, our results show that efforts have been made to innovate in the privacy space.
For instance, company B created a geographic location privacy feature which could detect human activity and make their doorbell less intrusive.
One reason for this was that companies wanted to preserve their trust relationship with their customers, and privacy failures were seen as potentially "creepy" and "intrusive," which would undermine this relationship.The current efforts of innovation in privacy design are a good first step, but more is needed.
For example, the challenge of communicating and obtaining user consent in smart homes needs to be systematic (e.g., within the same device ecosystem) and coordinated (e.g., among device ecosystems).
However, this is currently not the case and highlights the need for better communication and coordination between stakeholders and product teams.While data protection regulations (e.g., GDPR) appear to be consistent with better UX design for privacy in smart homes, these regulations remain unclear as to whether the same could be true with regards to UX for security design.
Security and privacy qualities of smart homes are not the same; however, both are qualities of data protection.
It is not clear how much responsibility users should have to ensure the secure operation of their devices.
However, some manufacturers blame breaches on users who do not adopt secure practices (e.g., failing to change default passwords).
Regardless of where responsibility lies, manufacturers could put effort into improving security experience, making it easier for users to achieve their desired security outcomes.
One option would be for data protection legislation to explicitly cover security experiences, as currently there are very few incentives for manufacturers to put additional effort into enhancing the UX of security.Regardless of whether regulations should encompass UX aspects of both security and privacy, design standards, guidelines, frameworks, and APIs are other options which have not been explored from an innovation perspective.
The tensions that exist between regulators and UX designers over communicating the use of data (e.g., despite being required by GDPR, UX designers do not typically ask users to consent to all uses of their data because-otherwise-it would be detrimental to UX) should invite us to find innovative solutions that satisfy both parties: regulators and users.
Agile teams have historically treated security as a technical problem, ignoring its social and interaction aspects [81].
With that in mind, we argue that in an agile setting, security would still not be considered during the design stage and would, hence, remain an implementation problem.
In company A, Security Manager A3 described their Information Security Team as "the department of 'no' when it comes to enforcing security."
This problem has been common in the past where security teams blocked progress in agile environments with the attitude of "security says no" [82].
Moreover, agile development does not have built-in steps for explicitly dealing with security issues because it was not designed with security in mind [83].
This might explain our results which show that product design teams who used an agile development process did not explicitly consider security issues during the design phase.
However, our results show that GDPR required design teams to follow DPbD requirements, in order to build legally-compliant products.
We argue that this is a promising step toward better considerations of security design in agile teams, but this is accompanied with noteworthy challenges and barriers, especially in the context of smart home ecosystems.
In this section, we outline areas for future investigation.Innovation without hindering security.
Our results show that tried-and-tested solutions were highly demanded in companies A, B, and C which preferred reliability and assurance (e.g., reusing best-security practices).
Those practices were shown to hinder innovation; however, we believe more research is needed to explore the relationship between UX, innovation, and security.
A key issue to uncover is what aspects of security design can be safely innovated, and how UX can be used to design more effective security experiences.UX-aware data protection guidelines.
Our findings show that data protection regulations (e.g., GDPR) influenced the design phase.
Our participants reported that GDPR touched on facets of product design but often failed to translate into specific requirements, which caused disparities in the design process.
While GDPR requires practitioners to factor security and privacy into the design process, it can bring more confusion to the design table: regulatory requirements have been reported to be high-level and impractical [84].
New techniques and tools are needed to address how data protection regulations and practices can factor the application of UX design principles.Improving communication among different stakeholders.
Our results show poor communication among multistakeholder teams where security design happens.
In the absence of regular communication among stakeholders, the number of implicit assumptions made increases (e.g., in our study, Product Manager A1 selecting security features based on their own knowledge of common practices) [85].
Similarly, tensions among stakeholders also increase.
For example, in Company B, UX Designer B11 was frustrated that they could not make UX-aware decisions.
Expecting largely autonomous groups of stakeholders (e.g., security, legal, design, UX) with different goals, motivations, and constraints to speak the same language is unrealistic.
Therefore, more research into this area should explore how to make different teams communicate effectively about factoring UX into the security and privacy design of smart home products.
Studies and recent events show that security and privacy of smart home products can have detrimental and lifethreatening effects on people (e.g., compromised products have allowed attackers to spy on residents and control home networks).
Design must consider users' motivations, perceptions, and expectations to enable users to effectively protect themselves when using these products.While research suggests that factoring UX into security and privacy design is important, the practices of product designers in this space have not been empirically explored.
To bridge this gap, we conducted three user studies involving 20 interviews with security camera designers.
We analyzed the data using Grounded Theory and found that design teams used UX as a means of innovating in privacy design to address social aspects of privacy, in particular to avoid intrusiveness.
However, UX was seen as undesirable for innovating in security design due to the belief that security was only a technical problem where tried-and-tested solutions were the only option.
Based on our findings, we conclude with recommendations to improve design practices in smart homes:Explicitly aim to innovate through UX of security.
Triedand-tested security solutions are preferred by design teams as they provide a measure of assurance that they are effective and reduce vulnerabilities.
The challenges introduced by smart homes (e.g., diverse social contexts, varying levels of skill and ability, subtle tensions among stakeholders) are not addressed by current tried-and-tested security solutions.
By exploring security through the lens of UX, new ways of simplifying and streamlining interactions can be uncovered.
While these innovations may also lead to new security challenges, it is necessary to innovate in order to design better solutions that will eventually become tried-and-tested for smart homes.Align security and privacy in UX.
Our results show that UX of security design is not distinct in practice from UX of privacy design.
Many technical aspects of security and privacy design have common principles and, thus, could be considered as part of a single UX domain-instead of being broken down into separate components, such that one is in scope and one is not.Factor UX into the practice of data protection compliance.
The compliance aspect of data protection regulations strongly motivates security and privacy considerations (e.g., DPbD).
Our results show that UX can help identify issues with compliance, and suggest more workable alternatives.
2.
Can you describe your product design process?
3.
Do you consider UX when designing security and privacy solutions for your smart home products?
If so, how?
4.
What are the typical challenges that you face when designing smart home products?
Are there any challenges specific to factoring UX into security and privacy design?
5.
Is there anything in the design process that could help address user-centered security and privacy challenges in smart homes?
If so, please elaborate.
This research was supported by the 2018-2019 Information Commissioner's Office's (ICO) Grants Programme.
The authors would like to thank Elie Tom, Martin J. Kraemer, and the anonymous SOUPS reviewers for their valuable input.
George Chalhoub is funded by Fondation Sesam.
The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ICO, or any sponsor.
sOur interviews were semi-structured.
We below describe our study script (divided into several sections).
The last four sections describe specific questions that we asked to employees who had different responsibilities.
C.1 Characterizations1.
Would you tell us about your role in the company that you work at?
(a) When did you join the company?
(b) What are your responsibilities?
(c) What is your specific role in the development or design process of smart home devices?
2.
Would you tell us about the products that you develop?
(a) Is there a specific product that you focus on developing?
3.
Would you tell us about your users (or customers)?
(a) How would you describe the typical customers that use your products?C.
5.
In general, who is responsible for designing the security/privacy features of smart cameras?
6.
How do you update the firmware of smart cameras that you sell?
Who is responsible for this task?
7.
How often does security need to be maintained?
8.
If a security breach happens in the physical products sold to clients, who will take responsibility?
9.
If your company suffers from a data breach, how will you address this?
Do you notify users?
10.
How do you make sure that users who use your products are protected when it comes to breaches?
C.9 Regulatory Stakeholder Questions1.
Who deals with GDPR and Product Liability?
2.
Do you deal with legislation?
3.
How is data protection represented in your organization?
4.
Do you interact with any regulatory bodies (e.g., ICO) when it comes to matters of data protection?
(a) (if yes) What are these matters?
(b) (if no) Do you think it would be useful to do so?
C.10 Management Stakeholder Questions1.
Are there any restrictions (e.g., legal, security, privacy) that make it harder for you to use customer data for product design or making decisions?
2.
What data protection roles/responsibilities are there for:(a) Product management (and data management)?
(b) Product design and development?
(c) UX, usability, and experience-centered jobs?
(d) Marketing and sales?
3.
What does privacy mean in terms of your products?
4.
How do you design for data protection when devices are shared among multiple users?
C.11 Concluding Remarks1.
Do you think there is anything in the design/development process that makes it easier to address user-centered security and privacy challenges in cameras?
2.
We have reached the end of the interview.
Thank you for talking to us!
(a) Do you have any questions?
(b) Do you have any comments you want to add?
Our interviews were semi-structured.
We below describe our study script (divided into several sections).
The last four sections describe specific questions that we asked to employees who had different responsibilities.
1.
Would you tell us about your role in the company that you work at?
(a) When did you join the company?
(b) What are your responsibilities?
(c) What is your specific role in the development or design process of smart home devices?
2.
Would you tell us about the products that you develop?
(a) Is there a specific product that you focus on developing?
3.
Would you tell us about your users (or customers)?
(a) How would you describe the typical customers that use your products?C.
5.
In general, who is responsible for designing the security/privacy features of smart cameras?
6.
How do you update the firmware of smart cameras that you sell?
Who is responsible for this task?
7.
How often does security need to be maintained?
8.
If a security breach happens in the physical products sold to clients, who will take responsibility?
9.
If your company suffers from a data breach, how will you address this?
Do you notify users?
10.
How do you make sure that users who use your products are protected when it comes to breaches?
1.
Who deals with GDPR and Product Liability?
2.
Do you deal with legislation?
3.
How is data protection represented in your organization?
4.
Do you interact with any regulatory bodies (e.g., ICO) when it comes to matters of data protection?
(a) (if yes) What are these matters?
(b) (if no) Do you think it would be useful to do so?
1.
Are there any restrictions (e.g., legal, security, privacy) that make it harder for you to use customer data for product design or making decisions?
2.
What data protection roles/responsibilities are there for:(a) Product management (and data management)?
(b) Product design and development?
(c) UX, usability, and experience-centered jobs?
(d) Marketing and sales?
3.
What does privacy mean in terms of your products?
4.
How do you design for data protection when devices are shared among multiple users?
1.
Do you think there is anything in the design/development process that makes it easier to address user-centered security and privacy challenges in cameras?
2.
We have reached the end of the interview.
Thank you for talking to us!
(a) Do you have any questions?
(b) Do you have any comments you want to add?
