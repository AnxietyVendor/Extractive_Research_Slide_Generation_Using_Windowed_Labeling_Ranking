Providing explanations for deep neural network (DNN) models is crucial for their use in security-sensitive domains.
A plethora of interpretation models have been proposed to help users understand the inner workings of DNNs: how does a DNN arrive at a specific decision for a given input?
The improved interpretability is believed to offer a sense of security by involving human in the decision-making process.
Yet, due to its data-driven nature, the interpretability itself is potentially susceptible to malicious manipulations, about which little is known thus far.
Here we bridge this gap by conducting the first systematic study on the security of interpretable deep learning systems (IDLSes).
We show that existing IDLSes are highly vulnerable to adversarial manipulations.
Specifically, we present ADV 2 , a new class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models.
Through empirical evaluation against four major types of IDLSes on benchmark datasets and in security-critical applications (e.g., skin cancer diagnosis), we demonstrate that with ADV 2 the adversary is able to arbitrarily designate an input's prediction and interpretation.
Further, with both analytical and empirical evidence, we identify the prediction-interpretation gap as one root cause of this vulnerability-a DNN and its interpretation model are often misaligned, resulting in the possibility of exploiting both models simultaneously.
Finally, we explore potential countermeasures against ADV 2 , including leveraging its low transferability and incorporating it in an adversarial training framework.
Our findings shed light on designing and operating IDLSes in a more secure and informative fashion, leading to several promising research directions.
The recent advances in deep learning have led to breakthroughs in many long-standing machine learning tasks (e.g., image classification [22], natural language processing [54], and even playing Go [49]), enabling use cases previously considered strictly experimental.However, the state-of-the-art performance of deep neural network (DNN) models is often achieved at the cost of interpretability.
It is challenging to intuitively understand the inference of complicated DNNs -how does a DNN arrive at a specific decision for a given input -due to their high non-linearity and nested architectures.
This is a major drawback for applications in which the interpretability of decisions is a critical prerequisite, while simple black-box predictions cannot be trusted by default.
Another drawback of DNNs is their inherent vulnerability to adversarial inputs -maliciously crafted samples to trigger target DNNs to malfunction [9,28,56] -which leads to unpredictable model behaviors and hinders their use in security-sensitive domains.The drawbacks have spurred intensive research on improving the DNN interpretability via providing explanations at either model-level [26,45,63] or instance-level [10,16,43,50].
For example, in Figure 1 (a), an attribution map highlights an input's most informative part with respect to its classification, revealing their causal relationship.
Such interpretability helps users understand the inner workings of DNNs, enabling use cases including model debugging [39], digesting security analysis results [20], and detecting adversarial inputs [13].
For instance, in Figure 1 (b), an adversarial input, which causes the target DNN to deviate from its normal behavior, generates an attribution map highly distinguishable from its benign counterpart, and is thus easily detectable.
terpretable deep learning system (IDLS).
The enhanced interpretability of IDLSes is believed to offer a sense of security by involving human in the decision process [57].
However, given its data-driven nature, this interpretability itself is potentially susceptible to malicious manipulations.
Unfortunately, thus far, little is known about the security vulnerability of IDLSes, not to mention mitigating such threats.Our Work.
To bridge the gap, in this paper, we conduct a comprehensive study on the security vulnerability of IDLSes, which leads to the following interesting findings.First, we demonstrate that existing IDLSes are highly vulnerable to adversarial manipulations.
We present ADV 2 , a new class of attacks that generate adversarial inputs not only misleading a target DNN but also deceiving its coupled interpreter.
By empirically evaluating ADV 2 against four major types of IDLSes on benchmark datasets and in securitycritical applications (e.g., skin cancer diagnosis), we show that it is practical to generate adversarial inputs with predictions and interpretations arbitrarily chosen by the adversary.
For example, Figure 1 (c) shows adversarial inputs that are misclassified by target DNNs and also interpreted highly similarly to their benign counterparts.
Thus the interpretability of IDLSes merely provides limited security assurance.Then, we show that one possible root cause of this attack vulnerability lies in the prediction-interpretation gap: the interpreter is often misaligned with the classifier, while the interpreter's interpretation only partially explains the classifier's behavior, allowing the adversary to exploit both models simultaneously.
This finding entails several intriguing questions: (i) what, in turn, is the possible cause of this gap?
(ii) how does this gap vary across different interpreters?
(iii) what is its implication for designing more robust interpreters?
We explore all these key questions in our study.Further, we investigate the transferability of ADV 2 across different interpreters.
We note that it is often difficult to find adversarial inputs transferable across distinct types of interpreters, as they generate interpretations from complementary perspectives (e.g., back-propagation, intermediate representations, input-prediction correspondence).
This finding points to training an ensemble of interpreters as one potential countermeasure against ADV 2 .
Finally, we present adversarial interpretation distillation (AID), an adversarial training framework which integrates ADV 2 in training interpreters.
We show that AID effectively reduces the prediction-interpretation gap and potentially helps improve the robustness of interpreters against ADV 2 .
To our best knowledge, this work represents the first systematic study on the security vulnerability of existing IDLSes.
We believe our findings shed light on designing and operating IDLSes in a more secure and informative manner.Roadmap.
The remainder of the paper proceeds as follows.
§ 2 introduces fundamental concepts; § 3 presents the ADV 2 attack and details its implementation against four major types of interpreters; § 4 empirically evaluates its effectiveness; § 5 explores the fundamental causes of the attack vulnerability and discusses possible countermeasures; § 6 surveys relevant literature; the paper is concluded in § 7.
We begin with introducing a set of fundamental concepts and assumptions.
The symbols and notations used in this paper are summarized in Table 1.
Classifier -In this paper, we primarily focus on predictive tasks (e.g., image classification [12]), in which a DNN f (i.e., classifier) assigns a given input x to one of a set of predefined classes C , f (x) = c ∈ C .
Interpreter -In general, the DNN interpretability can be obtained in two ways: designing interpretable DNNs [45,62] or extracting post-hoc interpretations.
The latter case does not require modifying model architectures or parameters, thereby leading to higher prediction accuracy.
We thus mainly consider post-hoc interpretations in this paper.
More specifically, we focus on instance-level interpretability [10,16,26,37,38,45,48,50,63], which explains how a DNN f classifies a given input x and uncovers the causal relationship between x and f (x).
We assume such interpretations are given in the form of attribution maps.
As shown in Figure 2, the interpreter g generates an attribution map m = g(x; f ), with its i-th element m [i] quantifying the importance of x's i-th feature x [i] with respect to f (x).
Adversarial Attack -DNNs are inherently vulnerable to adversarial inputs, which are maliciously crafted samples to force DNNs to misbehave [36,56].
Typically, an adversarial input x * is generated by modifying a benign input x • via pixel perturbation (e.g., PGD [35]) or spatial transformation (e.g., STADV [60]), with the objective of forcing f to misclassify x * to a target class c t , f (x * ) = c t = f (x • ).
To ensure the attack evasiveness, the modification is often constrained to an 1660 29th USENIX Security Symposium USENIX Association allowed set (e.g., a norm ball B ε (x • ) = {x|x − x • ∞ ≤ ε}).
Consider PGD, a universal first-order adversarial attack, as a concrete case.
At a high level, PGD implements a sequence of project gradient descent on the loss function:x (i+1) = Π B ε (x • ) x (i) − α sgn ∇ x prd f x (i) , c t (1)where Π is the projection operator, α represents the learning rate, the loss function prd measures the difference of the model prediction f (x) and the class c t targeted by the adversary (e.g., cross entropy), and x (0) is initialized as x • .
Threat Model -Following the line of work on adversarial attacks [9,19,35,56], we assume in this paper a white-box setting: the adversary has complete access to the classifier f and the interpreter g, including their architectures and parameters.
This is a conservative and realistic assumption.
Prior work has shown that it is possible to train a surrogate model f given black-box access to a target DNN f [41]; given that the interpreter is often derived directly from the classifier (details in § 3), the adversary may then train a substitution interpreter g based on f .
We consider investigating such black-box attacks as our ongoing work.
The interpretability of IDLSes is believed to offer a sense of security by involving human in the decision process [13,17,20,57]; this belief has yet to be rigorously tested.
We bridge this gap by presenting ADV 2 , a new class of attacks that deceive target DNNs and their interpreters simultaneously.
Below we first give an overview of ADV 2 and then detail its instantiations against four major types of interpreters.
The ADV 2 attack deceives both the DNN f and its coupled interpreter g. Specifically, ADV 2 generates an adversarial input x * by modifying a benign input x • such that• (i) x * is misclassified by f to a target class c t , f (x * ) = c t ;• (ii) x * triggers g to generate a target attribution map m t , g(x * ; f ) = m t ;• (iii) The difference between x * and x • , ∆(x * , x • ), is imperceptible;where the distance function ∆ depends on the concrete modification: for pixel perturbation (e.g., [35]), it is instantiated as L p norm, while for spatial transformation (e.g., [60]), it is defined as the overall spatial distortion.
In other words, the goal is to find sufficiently small perturbation to the benign input that leads to the prediction and interpretation desired by the adversary.At a high level, we formulate ADV 2 using the following optimization framework:min x ∆(x, x • ) s.t.f (x) = c t g(x; f ) = m twhere the constraints ensure that (i) the adversarial input is misclassified as c t and (ii) it triggers g to generate the target attribution map m t .
As the constraints of f (x) = c t and g(x; f ) = m t are highly non-linear for practical DNNs, we reformulate Eqn (2) in a form more suited for optimization: min x prd ( f (x), c t ) + λ int (g(x; f ), m t )s.t. ∆(x, x • ) ≤ ε(3)where the prediction loss prd is the same as in Eqn (1), the interpretation loss int measures the difference of adversarial map g(x; f ) and target map m t , and the hyper-parameter λ balances the two factors.
Below we use adv (x) to denote the overall loss function defined in Eqn (3).
We construct the solver of Eqn (3) upon an adversarial attack framework.
While it is flexible to choose the concrete framework, below we primarily use PGD [35] as the reference and discuss the construction of ADV 2 upon alternative frameworks (e.g., spatial transformation [60]) in § 4.
Under this setting, we define prd ( f (x), c t ) = − log( f c t (x)) (i.e., the negative log likelihood of x with respect to the class c t ), ∆(x, x • ) = x − x • ∞ , and int (g(x; f ), m t ) = g(x; f ) − m t 2 2 .
In general, ADV 2 searches for x * using a sequence of gradient descent updates:x (i+1) = Π B ε (x • ) x (i) − α sgn ∇ x adv x (i)(4)However, directly applying Eqn (4) is often found ineffective, due to the unique characteristics of individual interpreters.
In the following, we detail the instantiations of ADV 2 against the back-propagation-, representation-, model-, and perturbation-guided interpreters, respectively.
This class of interpreters compute the gradient (or its variants) of the model prediction with respect to a given input to derive the importance of each input feature.
The hypothesis is that larger gradient magnitude indicates higher relevance of the feature to the prediction.
We consider gradient saliency (GRAD) [50] as a representative of this class.Intuitively, GRAD considers a linear approximation of the model prediction (probability) f c (x) for a given input x and a given class c, and derives the attribution map m as:m = ∂ f c (x) ∂x(5)To attack GRAD-based IDLSes, we may search for x * using a sequence of gradient descent updates as defined in Eqn (4).
However, according to Eqn (5), computing the gradient of the attribution map g(x; f ) amounts to computing the Hessian matrix of f c (x), which is all-zero for DNNs with ReLU activation functions.
Thus the gradient of the interpretation loss int provides little information for updating x, which makes directly applying Eqn (4) ineffective.
29th USENIX Security Symposium 1661 To overcome this, when performing back-propagation, we smooth the gradient of ReLU, denoted by r(z), with a function h(z) defined as (τ is a small constant, e.g., 10 −4 ):h(z) (z + √ z 2 + τ) = 1 + z/ √ z 2 + τ (z < 0) ( √ z 2 + τ) = z/ √ z 2 + τ (z ≥ 0)Intuitively, h(z) tightly approximates r(z), while its gradient is non-zero everywhere.
Another possibility is the sigmoid function σ(z) = 1/(1 + e −z ).
Figure 3 compares different functions near z = 0.
Our evaluation shows that h(z) significantly outperforms σ(z) and r(z) in attacking GRAD.This attack is extensible to other back-propagation-based interpreters (e.g., DEEPLIFT [48], SMOOTHGRAD [51], and LRP [6]), due to their fundamentally equivalent, gradientcentric formulations [3].
This class of interpreters leverage the feature maps at intermediate layers of DNNs to generate attribution maps.
We consider class activation mapping (CAM) [64] as a representative interpreter of this class.At a high level, CAM performs global average pooling [30] over the feature maps of the last convolutional layer, and uses the outputs as features for a linear layer with softmax activation to approximate the model predictions.
Based on this connectivity structure, CAM computes the attribution maps by projecting the weights of the linear layer back to the convolutional feature maps.Formally, let a k [i, j] denote the activation of the k-th channel of the last convolutional layer at the spatial position (i, j).
The output of global average pooling is defined asA k = ∑ i, j a k [i, j].
Further let w k,c be the weight of the connection between the k-th input and the c-th output of the linear layer.
The input to the softmax function for a class c with respect to a given input x is approximated by:z c (x) ≈ ∑ k w k,c A k = ∑ i, j ∑ k w k,c a k [i, j](6)The class activation map m c is then given by:m c [i, j] = ∑ k w k,c a k [i, j](7)Due to its use of deep representations at intermediate layers, CAM generates attribution maps of high visual quality and limited noise and artifacts [30].
We instantiate g with a DNN that concatenates the part of f up to its last convolutional layer and a linear layer parameterized by {w k,c }.
To attack CAM, we search for x * using a sequence of gradient descent updates as defined in Eqn (4).
This attack can be readily extended to other representationguided interpreters (e.g., GRADCAM [47]).
Instead of relying on deep representations at intermediate layers, model-guided methods train a meta-model to directly predict the attribution map for any given input in a single feed-forward pass.
We consider RTS [10] as a representative method in this category.For a given input x in a class c, RTS finds its attribution map m by solving the following optimization problem:min m λ 1 r tv (m) + λ 2 r av (m) − log ( f c (φ(x; m))) +λ 3 f c (φ(x; 1 − m)) λ 4 s.t. 0 ≤ m ≤ 1 (8)Here r tv (m) denotes the total variation of m, which reduces noise and artifacts in m; r av (m) represents the average value of m, which minimizes the size of retained parts; φ(x; m) is the operator using m as a mask to blend x with random colors and Gaussian blur, which captures the impact of retained parts (where the mask is non-zero) on the model prediction; the hyper-parameters {λ i } 4 i=1 balance these factors.
Intuitively, this formulation finds the sufficient and necessary parts of x, based on which f is able to make the prediction f (x) with high confidence.However, solving Eqn (8) for every input during inference is fairly expensive.
Instead, RTS trains a DNN to directly predict the attribution map for any given input, without accessing to the DNN f after training.
In [44], this is achieved by composing a ResNet [22] pre-trained on ImageNet [12] as the encoder (which extracts feature maps of given inputs at different scales) and a U-NET [44] as the masking model, which is then trained to directly optimize Eqn (8).
We consider the composition of this encoder and this masking model as the interpreter g.To attack RTS, one may directly apply Eqn (4).
However, our evaluation shows that this strategy is often ineffective for finding desirable adversarial inputs.
This is explained by that the encoder enc(·) plays a significant role in generating attribution maps, while solely relying on the outputs of the masking model is insufficient to guide the attack.
We thus add to Eqn (3) an additional loss term enc (enc(x), enc(c t )), which measures the difference of the encoder's outputs for the adversarial input x and the target class c t .
We then search for the adversarial input x * with a sequence of gradient descent updates defined in Eqn (4).
More implementation details are discussed in § 3.6.
The fourth class of interpreters formulate finding the attribution map by perturbing the input with minimum noise and observing the change in the model prediction.
We consider MASK [16] as a representative interpreter in this class.For a given input x, MASK identifies its most informative parts by checking whether changing such parts influences the prediction f (x).
It learns a mask m, where m[i] = 0 if the i-th input feature is retained and m[i] = 1 if the feature is replaced with Gaussian noise.
The optimal mask is found by solving an optimization problem:min m f c (φ(x; m)) + λ1 − m 1 s.t. 0 ≤ m ≤ 1 (9)where c denotes the current prediction c = f (x) and φ(x; m) is the perturbation operator which blends x with Gaussian noise.
The first term finds m that causes the probability of c to decrease significantly, while the second term encourages m to be sparse.
Intuitively, solving Eqn (9) amounts to finding the most informative and necessary parts of x with respect to its prediction f (x).
Note that this formulation may result in significant artifacts in m.
A more refined formulation is given in [16].
Unlike other classes of interpreters, to attack MASK, it is infeasible to directly optimize Eqn (3) with iterative gradient descent (Eqn (4)), because the interpreter g itself is formulated as an optimization procedure.Instead, we reformulate ADV 2 using a bilevel optimization framework.
For given x • , c t , m t , f , and g, we re-define the adversarial loss function as adv (x, m) prd ( f (x), c t ) + λ int (m, m t ) by introducing m as an additional variable.
Let map (m; x) be the objective function defined in Eqn (9).
Note that m * (x) = arg min m map (m; x) is the attribution map found by MASK for a given input x.
We then have the following attack framework:min x adv (x, m * (x)) s.t. m * (x) = arg min m map (m; x)(10)Still, solving the bilevel optimization in Eqn (10) exactly is challenging, as it requires recomputing m * (x) by solving the inner optimization problem whenever x is updated.
We propose an approximate iterative procedure which optimizes x and m by alternating between gradient descent on adv and map respectively.More specifically, at the i-th iteration, given the current input x (i−1) , we compute its attribution map m (i) by updating m (i−1) with gradient descent on map m (i−1) ;x (i−1); we then fix m (i) and obtain x (i) by minimizing adv after a single step of gradient descent with respect to m (i) .
Formally, we define the objective function for updating x (i) as:adv x (i−1) , m (i) − ξ∇ m map m (i) ; x (i−1)where ξ is the learning rate for this virtual gradient descent.
The rationale behind this procedure is as follows.
While it is difficult to directly minimizing adv (x, m * (x)) with respect to x, we use a single-step unrolled map as a surrogate of m * (x).
A similar approach is used in [15].
Essentially, this iterative optimization defines a Stackelberg game [46] between the optimizer for x (leader) and the optimizer for m (follower), which requires the leader to anticipate the follower's next move to reach the equilibrium.
∇ x adv x, m − ξ∇ m map (m; x) ; 5 return x;Algorithm 1 sketches the attack against MASK.
More implementation details are given in § 3.6.
Next we detail the implementation of ADV 2 and present a suite of optimizations to improve the attack effectiveness against specific interpreters.Iterative Optimizer -We build the optimizer based upon PGD [35], which iteratively updates the adversarial input using Eqn (4).
By default, we use L ∞ norm to measure the perturbation magnitude.
It is possible to adopt alternative frameworks if other perturbation metrics are considered.
For instance, instead of modifying pixels directly, one may generate adversarial inputs via spatial transformation [2,60], in which the perturbation magnitude is often measured by the overall spatial distortion.
We detail and evaluate spatial transformationbased ADV 2 in § 4.
Warm Start -It is observed in our evaluation that it is often inefficient to search for adversarial inputs by running the update steps of ADV 2 (Eqn (4)) from scratch.
Rather, first running a fixed number (e.g., 400) of update steps of the regular adversarial attack and then resuming the ADV 2 update steps significantly improves the search efficiency.
Intuitively, this strategy first quickly approaches the manifold of adversarial inputs, and then searches for inputs satisfying both prediction and interpretation constraints.Label Smoothing -Recall that we measure the prediction loss prd ( f (x), c t ) with cross entropy.
When attacking GRAD, ADV 2 may generate intermediate inputs that cause f to make over-confident predictions (e.g., with probability 1).
The allzero gradient of prd prevents the attack from finding inputs with desirable interpretations.
To solve this, we refine cross entropy with label smoothing [55].
We sample y c t from a USENIX Association 29th USENIX Security Symposium 1663 Next we conduct an empirical study of ADV 2 on a variety of DNNs and interpreters from both qualitative and quantitative perspectives.
Specifically, our experiments are designed to answer the following key questions about ADV 2 : Q 1 : Is it effective to deceive target classifiers?
Q 2 : Is it effective to mislead target interpreters?
Q 3 : Is it evasive with respect to attack detection methods?
Q 4 : Is it effective in real security-critical applications?
Q 5 : Is it flexible to adopt alternative attack frameworks?
We first introduce the setting of our empirical evaluation.
Datasets -Our evaluation primarily uses ImageNet [12], which consists of 1.2 million images from 1,000 classes.
Every image is center-cropped to 224×224 pixels.
For a given classifier f , from the validation set of ImageNet, we randomly sample 1,000 images that are classified correctly by f to form our test set.
All the pixels are normalized to [0,1].
Classifiers -We use two state-of-the-art DNNs as the classifiers, ResNet-50 [22] and DenseNet-169 [24], which respectively attain 77.15% and 77.92% top-1 accuracy on ImageNet.
Using two DNNs of distinct capacities (50 layers versus 169 layers) and architectures (residual blocks versus dense blocks), we factor out the influence of the characteristics of individual DNNs.Interpreters -We adopt GRAD [50], CAM [64], RTS [10], and MASK [16] as the representatives of back-propagation-, representation-, model-, and perturbation-guided interpreters respectively.
We adopt their open-source implementation in our evaluation.
As RTS is tightly coupled with its target DNN (i.e., ResNet), we train a new masking model for DenseNet.
To assess the validity of the implementation, we evaluate all the interpreters in a weakly semi-supervised localization task [8] using the benchmark dataset and method in [10].
Table 2 summarizes the results.
The performance of all the interpreters is consistent with that reported in [10], with slight variation due to the difference of underlying DNNs.
Table 2.
Performance of the interpreters in this paper in a weakly semi-supervised localization task (with ResNet as the classifier).
Attacks -We implement all the variants of ADV 2 in § 3 on the PGD framework.
In addition, we also implement ADV 2 on a spatial transformation framework (STADV) [2].
We compare ADV 2 with regular PGD [35], a universal first-order adversarial attack.
For both ADV 2 and PGD, we assume the setting of targeted attacks, in which the adversary attempts to force the DNNs to misclassify the adversarial inputs into randomly designated classes.
The parameter settings of all the attacks are summarized in Appendix B.
We first evaluate the effectiveness of ADV 2 in terms of deceiving target DNNs.
The effectiveness is measured using attack success rate, which is defined as Attack Success Rate (ASR) = # successful trials # total trials and misclassification confidence (MC), which is the probability assigned by the DNN to the target class c t .
Table 3.
Effectiveness of PGD (P) and ADV 2 (A) against different classifiers and interpreters in terms of ASR (MC).
Table 3 summarizes the attack success rate and misclassification confidence of ADV 2 and PGD against different combinations of classifiers and interpreters.
Note that as PGD is only applied on the classifier, its effectiveness is agnostic to the interpreters.
To make fair comparison, we fix the maximum number of iterations as 1,000 for both attacks.
It is 1664 29th USENIX Security Symposium USENIX Association observed that ADV 2 achieves high success rate (above 95%) and misclassification confidence (above 0.98) across all the cases, which is comparable with the regular PGD attack.
We thus have the following conclusion.ResNet DenseNet GRAD CAM MASK RTS GRAD CAM MASK RTS P 100% (1.0) 100% (1.0) 100% 100% 98% 100% 100% 100% 96% 100% A (0.99) (1.0) (0.99) (1.0) (0.98) (1.0) (0.98) (1.0) Despite its dual objectives, ADV 2 is as effective as regular adversarial attacks in deceiving target DNNs.
Next we evaluate the effectiveness of ADV 2 in terms of generating similar interpretations to benign inputs.
Specifically, we compare the interpretations of benign and adversarial inputs, which is crucial for understanding the security implications of using interpretability as a means of defenses [13,57].
Due to the lack of standard metrics for interpretation plausibility, we use a variety of measures in our evaluation.
Visualization -We first qualitatively compare the interpretations of benign and adversarial (PGD, ADV 2 ) inputs.
Fig- ure 4 show a set of sample inputs and their attribution maps with respect to GRAD, CAM, MASK, and RTS (more samples in Appendix C1).
Observe that in all the cases, the ADV 2 inputs generate interpretations perceptually indistinguishable from their benign counterparts.
In comparison, the PGD inputs are easily identifiable by inspecting their attribution maps.
L L L p p p Measure -Besides qualitatively comparing the attribution maps of benign and adversarial inputs, we also measure their similarity quantitatively.
By considering attribution maps as matrices, we measure the L 1 distance between benign and adversarial maps.
Figure 5 summarizes the results (other L p measures in Appendix C1).
For comparison, we normalize all the measures to [0,1] by dividing them by the number of pixels.We have the following observations.
(i) Compared with PGD, ADV 2 generates attribution maps much more similar to benign cases.
The average L 1 measure of ADV 2 is more than 60% lower than PGD across all the interpreters.
(ii) The effectiveness of ADV 2 varies with the target interpreter.
For instance, compared with other interpreters, the difference between PGD and ADV 2 is relatively marginal on GRAD, imply- IoU Test -Another quantitative measure for the similarity of attribution maps is the intersection-over-union (IoU) score.
It is widely used in object detection [21] to compare model predictions with ground-truth bounding boxes.
Formally, the IoU score of a binary-valued map m with respect to a baseline map m • is defined as their Jaccard similarity: Following a typical rule used in the object detection task [21] where a detected region of interest (RoI) is considered positive if its IoU score is above 0.5 with respect to a groundtruth mask, we thus consider an attribution map as plausible if its IoU score exceeds 0.5 with respect to the benign attribution map.
Figure 6 compares the average IoU scores of adversarial maps (PGD, ADV 2 ) with respect to the benign cases.
Observe that ADV 2 achieves IoU scores above 0.5 across all the interpreters, which are more than 40% higher than PGD in all the cases.
Especially on RTS, in which the attribution maps are natively binary-valued, ADV 2 achieves IoU scores above 0.9 on both ResNet and DenseNet.IoU(m) = |O(m) ∩ O(m • )|/|O(m) ∪ O(m • )|,Based on both qualitative and quantitative measures, we have the following conclusion.
Observation 2 ADV 2 is able to generate adversarial inputs with interpretations highly similar to benign cases.
29th USENIX Security Symposium 1665 Intuitively, from the adversary's perspective, ADV 2 entails a search space for adversarial inputs no larger than its underlying adversarial attack (e.g., PGD), as ADV 2 needs to optimize both the prediction loss prd and interpretation loss int , while ADV 2 only needs to optimize prd .
Next we compare PGD and ADV 2 in terms of their evasiveness with respect to adversarial attack detection methods.Basic ADV 2 -To be succinct, we consider feature squeezing (FS) [61] as a concrete detection method.
FS reduces the adversary's search space by coalescing inputs corresponding to different feature vectors into a single input, and detects adversarial inputs by comparing their predictions under original and squeezed settings.
This operation is implemented in the form of a set of "squeezers": bit depth reduction, local smoothing, and non-local smoothing.
Setting Table 4 lists the detection rate of adversarial inputs (PGD, ADV 2 ) using different types of squeezers on ResNet.
Observe that the squeezers seem effective to detect both ADV 2 and PGD inputs.
For instance, local smoothing achieves higher than 97% success rate in detecting both ADV 2 and PGD inputs, with difference less than 2%.
We thus have: The overall detectability of ADV 2 and PGD with respect to feature squeezing is not significantly different.Adaptive ADV 2 -We now adapt ADV 2 to evade the detection of FS.
Related to existing adaptive attacks against FS [23], this optimization is interesting in its own right.
Specifically, for smoothing squeezers, we augment the loss function adv (x) (Eqn (3)) with the term sqz ( f (x), f (ψ(x)), which is the cross entropy of the predictions of original and squeezed inputs (ψ is the squeezer).
Algorithm 2: Adaptive ADV 2 against Feature Squeezing.Input: x • : benign input; c t : target class; f : target DNN; g: target interpreter; ψ: bit depth reduction; i: bit depth Output: x * : adversarial input // augmented adv with sqz w.r.t. smoothing // attack in squeezed space 1 x + ← PGD on ψ(x • ) with target c t and α = 1/2 i ; // attack in original space 2 search forx * = arg min x∈B ε (x • ) adv (x) + λ f (x) − f (x + ) 1 ; 3 return x * ;For bit depth reduction, we use a two-stage strategy.
(i) We first search in the squeezed space for an adversarial input x + that is close to x • 's ε-neighborhood.
To do so, we run PGD over ψ(x • ) with learning rate α = 1/2 i (i is the bit depth).
(ii) We then search in x • 's ε-neighborhood for an adversarial input x * that is classified similarly as x + .
To do so, we augment the loss function adv (x) with a probability loss term f (x) − f (x + ) 1 ( f (x + ) is x + 's probability vector), and then apply PGD to search for x * within x • 's ε-neighborhood.
The overall algorithm is sketched in Algorithm 2.
Table 4 summarizes the detection rate of adversarial inputs generated by adaptive ADV 2 , which drops significantly, compared with the case of basic ADV 2 .
Note that here we only show the possibility of adapting ADV 2 to evade a representative detection method, and consider an in-depth study on this matter as our ongoing work.
Meanwhile, we compare the L 1 measures and IoU scores of the attribution maps generated by basic and adaptive ADV 2 (with respect to the benign maps).
Table 5 shows the results.
Observe that the optimization in adaptive ADV 2 has little impact on its attack effectiveness against the interpreters.
We may thus conclude: Observation 4It is possible to adapt ADV 2 to generate adversarial inputs evasive with respect to feature squeezing.
= " n d l T B Z h G d y f q f c l p d + R D O x Z S b h o = " > A A A C V H i c b V B d T x N B F J 1 d R K E i A j 7 y s r G Y 8 N T s V o 0 + 4 s e D j 5 j Y Q s I U M n t 7 l 0 4 6 H 5 u Z u 0 g z 2 f / B q / w o E v + L D 0 5 L E 7 V 4 k k l O z j 3 3 Y 0 5 Z K + k p z 3 8 m 6 d q j 9 c d P N j Y 7 T 7 e e b T / f 2 d 0 b e t s 4 w A F Y Z d 1 p K T w q a X B A k h S e 1 g 6 F L h W e l N N P 8 / r J F T o v r f l G s x p H W l w a W U k Q F K V z r g V N f B U + f B 6 2 5 / 2 L n W 7 e y x f I H p J i S b p s i e O L 3 S T n Y w u N R k O g h P d n R V 7 T K A h H E h S 2 H d 5 4 r A V M x S W e R W q E R j 8 K i 7 P b 7 F V U x l l l X X y G s o X 6 d 0 c Q 2 v u Z L q N z c e Z q b S 7 + t 1 b q l c 1 U v R 8 F a e q G 0 M D 9 4 q p R G d l s n k k 2 l g 6 B 1 C w S A U 7 G 2 z O Y C C e A Y n I d b v A 7 W K 2 F G Q c O I B 2 0 g U / R m b z 3 F q / 5X w q K T B A U l S e F 4 5 F L p Q e F Z M v s 7 0 s 2 t 0 X l r z g 6 Y V D r W 4 M r K U I C h S n G t B Y 1 + G / r f j 5 r L T z X r Z v N L n I F + A L l t U / 3 I 7 y f j I Q q 3 R E C j h / U W e V T Q M w p E E h U 2 b 1 x 4 r A R N x h R c R G q H R D 8 N 8 6 S b d j c w o L a 2 L x 1 A 6 Z x 9 3 B K G 9 n + o i O u d L L We now evaluate the effectiveness of ADV 2 in real securitycritical applications.
We use the skin cancer screening task from the ISIC 2018 challenge [18] as a case study, in which given skin lesion images are categorized into a seven-disease taxonomy.
We adopt a competition-winning model 1 (with ResNet as its backbone) as the classifier, which attains 82.27% weighted multi-class accuracy on the holdout set (more details in Appendix C2).
We apply ADV 2 on this classifier and measure its effectiveness of generating plausible interpretations.
Figure 7 shows a set of samples and their attribution maps on the four interpreters.
Observe that ADV 2 generates interpretations visually indiscernible from their benign counterparts in all the cases.
Besides the PGD framework, ADV 2 can also be flexibly built upon alternative frameworks.
Here we construct ADV 2 upon STADV [60], a spatial transformation-based adversarial attack.
The implementation details are given in Appendix A1.
Overall we have the following conclusion.
Observation 5 As a general class of attacks, ADV 2 can be flexibly built upon alternative adversarial attack frameworks.
While it is shown in § 4 that ADV 2 is effective against a range of classifiers and interpreters, the cause of this effectiveness is unclear yet.
Next we conduct a study on this root cause from both analytical and empirical perspectives.
Based on our findings, we further discuss potential countermeasures against ADV 2 .
Recall that the formulation of ADV 2 in Eqn (3) defines two seemingly conflicting objectives: (i) maximizing the prediction change while (ii) minimizing the interpretation change.
We thus conjecture that the effectiveness of ADV 2 may stem from the partial independence between a classifier and its interpreter -the interpreter's explanations only partially describe the classifier's predictions, making it practical to exploit both models simultaneously.To validate the existence of this prediction-interpretation gap, we consider ADV 2 targeting randomly generated predictions and interpretations.
For a given input x • , we randomly generate a target class c t and a target interpretation m t , and search for an adversarial input x * that triggers the classifier to misclassify it as c t and also generates an interpretation similar to m t (i.e., f (x * ) = c t and g(x * ; f ) ≈ m t ).
Intuitively, if ADV 2 is able to find such x * , it indicates that the classifier and its interpreter can be manipulated separately; in other words, they are only partially aligned with each other.Random Patch Interpretation -In the first case, for a given input, we define its target attribution map by (i) sampling a patch of random shape (either a rectangle or a circle), random angle, and random position over the input, and (ii) setting the elements inside the patch as '1' and that outside it as '0'.
Typically this target map deviates significantly from its benign counterpart, due to its randomness.We evaluate the effectiveness of ADV 2 under this setting.
fectiveness in terms of deceiving the classifiers, implying that the space of adversarial inputs is sufficiently large to contain ones with targeted interpretations.
We then evaluate the effectiveness of ADV 2 in terms of generating the target interpretations.
For a given benign input x • and a target random patch map m t , ADV 2 attempts to generate an adversarial input c t with the interpretation similar to m t .
Figure 11 visualizes a set of sample results.
Note that in all the cases the ADV 2 maps appear visually similar to the target maps, highlighting the attack effectiveness.
This effectiveness is further validated in Random Class Interpretation -In the second case, for a given input (with c t as the target class), we instantiate its target interpretation with the attribution map of a benign input randomly sampled from another class˜cclass˜ class˜c t .
We particularly enforce c t = ˜ c t ; in other words, the adversarial input is misclassified into one class but interpreted as another one.
The ASR of ADV 2 is summarized in Table 8.
Observe that targeting random class interpretations has little influence on the attack effectiveness of deceiving the classifiers.
Figure 12 visualizes a set of sample target and ADV 2 inputs and their interpretations (DenseNet results in Appendix C4).
Note that the target and ADV 2 inputs are fairly distinct, but with highly similar interpretations.
This is quantitatively validated by their L 1 measures and IoU scores listed in Figure 13.
The experiments above show that it is practical to generate adversarial inputs targeting arbitrary predictions and interpretations.
We can therefore conclude: Observation 6A DNN and its interpreter are often not fully aligned, allowing the adversary to exploit both models simultaneously.
Next we explore the fundamental causes of this predictioninterpretation gap.
We speculate one following possible explanation as: existing interpretation models do not comprehensively capture the dynamics of DNNs, each only describing one aspect of their behavior.Specifically, GRAD solely relies on the gradient information; MASK focuses on the input-prediction correspondence while ignoring the internal representations; CAM leverages the deep representations at intermediate layers, but neglecting the input-prediction correspondence; RTS uses the internal representations in an auxiliary encoder and the inputinterpretation correspondence in the training data, which however may deviate from the true behavior of DNNs.Intuitively the exclusive focus on one aspect (e.g., inputprediction correspondence) of the DNN behavior results in loose constraints: when performing the attack, the adversary only needs to ensure that benign and adversarial inputs cause DNNs to behave similarly from one specific perspective.
We validate this speculation from two observations, low attack 1668 29th USENIX Security Symposium USENIX Association transferability and disparate attack robustness.
Attack Transferability -One intriguing property of adversarial inputs is their transferability: an adversarial input effective against one DNN is often found effective against another DNN, though it is not crafted on the second one [33,36,40].
In this set of experiments, we investigate whether such transferability exists in attacks against interpreters; that is, whether an adversarial input that generates a plausible interpretation against one interpreter is also able to generate a probable interpretation against another interpreter.
Specifically, for each given interpreter g, we randomly select a set of adversarial inputs crafted against g (source) and compute their interpretations on another interpreter g (target).
Figure 14 illustrates the attribution maps of a given adversarial input on g and g .
Further, for each case, we compare the adversarial map (right) against the corresponding benign map (left).
Observe that the interpretation transferability is fairly low: an adversarial input crafted against one interpreter g rarely generates highly plausible interpretation on another interpreter g .
We further quantitatively validate this observation.
Table 9 measures the L 1 distance between the adversarial and benign attribution maps across different interpreters.
For comparison, it also shows the L 1 measure for the adversarial inputs generated by PGD.
Observe that the adversarial inputs crafted on g tends to generate low-quality interpretations on a different interpreter g , with quality comparable to that generated by an interpretation-agnostic attack (i.e., PGD).
We can therefore conclude: Observation 7The transferability of adversarial inputs across different interpreters seems low.Attack Robustness -It is observed in § 4 that the effectiveness of ADV 2 varies with the target interpreter.
As shown in Figure 6, among all the interpreters, ADV 2 attains the lowest IoU scores on GRAD, suggesting that GRAD may be more robust against ADV 2 .
This observation may be explained as follows: GRAD uses the gradient magnitude of each input feature to measure its relevance to the model prediction; meanwhile, ADV 2 heavily uses the gradient information to optimize the prediction loss prd ; it is inherently difficult to minimize prd while keeping the gradient intact.We validate the conjecture by analyzing the robustness of integrated gradient (IG) [53], another back-propagationguided interpreter, against ADV 2 .
Due to their fundamental equivalence [3], the discussion here also generalizes to other back-propagation interpreters (e.g., [48,50,51]).
At a high level, for the i-th feature of a given input x, IG computes its attribution m [i] by aggregating the gradient of f (x) along the path from a baseline input ¯x to x:m[i] = (x[i] − ¯ x[i]) 1 0 ∂ f (tx + (1 − t) ¯ x) ∂x[i] dt(11)Like other back-propagation interpretation models [3], IG satisfies the desirable completeness axiom [48] that the attributions sum up to the difference between f 's predictions for the given input x and the baseline ¯ x. To simplify the exposition, let us assume a binary classification setting with classes C = {+, −}.
The DNN f predicts the probability of x belonging to the positive class as f (x).
Given an input x • from the negative class, the adversary attempts to craft an adversarial input x * to force f to misclassify x * as positive.
We define the prediction loss as prd (x * ) = f (x * )− f (x • ) (i.e., the increase in the probability of positive prediction), which can be computed as:prd (x * ) = 1 0 ∇ f (tx * + (1 − t)x • ) (x * − x • )dt(12)Meanwhile, we define the interpretation loss as int (x * ) = m • − m * 1 , where m • and m * are the attribution maps of x • and x * respectively.
While it is difficult to directly quantify int (x * ), we may use the attribution map of x * with x • as a surrogate baseline:∆m[i] = (x * [i] − x • [i]) 1 0 ∂ f (tx * + (1 − t)x • ) ∂x * [i] dt (13)which quantifies the impact of the i-th input feature on the difference of f (x • ) and f (x * ).
Thus, int (x * ) = ∆m 1 .
Proposition 1.
With IG, the prediction loss is upper bounded by the interpretation loss as:prd (x * ) ≤ int (x * ).
Proof.
We define u as the input difference u = (x * − x • ) and v as the integral vector with its i-th element v[i] defined asv[i] = 1 0 ∂ f (tx * + (1 − t)x • ) ∂x * [i] dt According to the definitions, we have prd (x * ) = u v and int (x * ) = u v 1 , where is the Hadamard product.
We have the following derivation: prd (x * ) = ∑ i u[i]v[i] ≤ ∑ i u[i] · v[i] = int (x * ).
Thus the prediction loss is upperbounded by the interpretation loss.In other words, in order to force x * to be misclassified with high confidence, the difference of benign and adversarial attribution maps needs to be large.
As the objectives of ADV 2 here is to maximize the prediction loss while minimizing the interpretation loss.
The coupling between prediction and interpretation losses results in a fundamental conflict.Note that however this conflict does not preclude effective adversarial attacks.
First, the constraint of prediction and interpretation losses may be loose.
Let γ prd and γ int be the thresholds of effective attacks.
That is, for an effective attack, prd (x * ) ≥ γ prd and int (x * ) ≤ γ int .
There could be cases that γ prd γ int , making ADV 2 still highly effective (e.g., Figure 8).
Second, the adversary may pursue attacks that rely less on the gradient information to circumvent this conflict.Overall, with the evidence of low attack transferability and disparate attack robustness, we can conclude:Observation 8Existing interpreters tend to focus on distinct aspects of DNN behavior, which may result in the predictioninterpretation gap.
Based on our findings, next we discuss potential countermeasures against ADV 2 attacks.Defense 1: Ensemble Interpretation -Motivated by the observation that different interpreters focus on distinct aspects of DNN behavior (e.g., CAM focuses on deep representations while MASK focuses on input-prediction correspondence), a promising direction to defend against ADV 2 is to deploy multiple, complementary interpreters to provide a holistic view of DNN behavior.Yet, two major challenges remain to be addressed.
First, different interpreters may provide disparate interpretations (e.g., Figure 14).
It is challenging to optimally aggregate such interpretations to detect ADV 2 .
Second, the adversary may adapt ADV 2 to the ensemble interpreter (e.g. optimizing the interpretation loss with respect to all the interpreters).
It is crucial to account for such adaptiveness in designing the ensemble interpreter.
We consider developing the ensemble defenses and exploring the adversary's adaptive strategies as our ongoing research directions.Defense 2: Adversarial Interpretation -Along the second direction, we explore the idea of adversarial training.
Recall that ADV 2 exploit the prediction-interpretation gap to generate adversarial inputs.
Here we employ ADV 2 as a drive to minimize this gap during training interpreters.Specifically, we propose an adversarial interpretation distillation (AID) framework.
Let A be the ADV 2 attack.
During training an interpreter g, for a given input x • , besides the regular loss map (x • ), we consider an additional loss term 1 , which is the negative L 1 measure between the attribution maps of x • and its adversarial counterpart A(x • ).
We encourage g to minimize this loss during the training (details in Appendix A2).
aid (x • ) = −g(x • ) − g(A(x • ))To assess the effectiveness of AID to reduce the predictioninterpretation gap, we use RTS as a concrete case study.
Recall that RTS is a model-guided interpreter which directly predicts the interpretation of a given input.
We construct two variants of RTS, a regular one and another with AID training (denoted by RTS A ).
We measure the sensitivity of the two interpreters to the underlying DNN behavior.
Normal Uniform Noise N U In the first case, we inject random noise (either normal or uniform) to the inputs and compare the attribution maps generated by the two interpreters.
We consider two noise levels, which respectively cause 3% and 30% misclassification on the test set.
Figure 15 shows a set of misclassified samples under the two noise levels.
Observe that compared with RTS, RTS A appears much more sensitive to the DNN's behavior change, by generating highly contrastive maps.
This sensitivity is also quantitatively confirmed by the L 1 measures between clean and noisy maps on RTS and RTS A .
The findings also corroborate a similar phenomenon observed in [59]: the representations generated by robust models tend to align better with salient data characteristics.
In the second case, we assess the ADV 2 .
In Figure ??
, we compare th nign and adversarial inputs on RTS that while ADV 2 generates adversari tions fairly similar to benign cases o RTS A : the maps of adversarial inpu able from their benign counterparts.
almost identically to RTS on benign AID training has little impact on be vations are confirmed by the L1 me Overall we have the following co Observation 8It is possible to exploit ADV 2 to r interpretation gap in training int In this section, we survey three vant to this work, namely, adversar transferability, and interpretability.Attacks and Defenses -Due in security-critical domains, mach increasingly becoming the targets Two primary threat models are con soning attacks -the adversary pol eventually compromise the target m attacks -the adversary manipulates ference to trigger target models to mCompared with simple models ( chines), securing deep neural netw ial settings entails more challenges higher model complexity [?]
.
One developing new evasion attacks aga In the second case, we assess the resilience of RTS A aga ADV 2 .
In Figure 16, we compare the attribution maps of nign and adversarial inputs on RTS and RTS A .
It is obser that while ADV 2 generates adversarial inputs with interpr tions fairly similar to benign cases on RTS, it fails to do so RTS A : the maps of adversarial inputs are fairly distingu able from their benign counterparts.
Moreover, RTS A beha almost identically to RTS on benign inputs, indicating that AID training has little impact on benign cases.
These ob vations are confirmed by the L 1 measures as well.
Overall we have the following conclusion.
Observation 8 It is possible to exploit ADV 2 to reduce the predictioninterpretation gap in training interpreters.
In this section, we survey three categories of work r vant to this work, namely, adversarial attacks and defen transferability, and interpretability.Attacks and Defenses -Due to their widespread in security-critical domains, machine learning models In the second case, we assess the resilience of RTS A against 1670 29th USENIX Security Symposium USENIX Association ADV 2 .
In Figure 16, we compare the attribution maps of benign and adversarial inputs on RTS and RTS A .
It is observed that while ADV 2 generates adversarial inputs with interpretations fairly similar to benign cases on RTS, it fails to do so on RTS A : the maps of adversarial inputs are fairly distinguishable from their benign counterparts.
Moreover, RTS A behaves almost identically to RTS on benign inputs, indicating that the AID training has little impact on benign cases.
These findings are confirmed by the L 1 measures as well.Overall we have the following conclusion.
It is possible to exploit ADV 2 to reduce the predictioninterpretation gap during training interpreters.
In this section, we survey three categories of work relevant to this work, namely, adversarial attacks and defenses, transferability, and interpretability.Attacks and Defenses -Due to their widespread use in security-critical domains, machine learning models are increasingly becoming the targets of malicious attacks.
Two primary threat models are considered in literature.
Poisoning attacks -the adversary pollutes the training data to eventually compromise the target models [7]; Evasion attacks -the adversary manipulates the input data during inference to trigger target models to misbehave [11].
Compared with simple models (e.g., support vector machines), securing deep neural networks (DNNs) in adversarial settings entails more challenges due to their significantly higher model complexity [29].
One line of work focuses on developing new evasion attacks against DNNs [19,35,56].
Another line of work attempts to improve DNN resilience against such attacks by inventing new training and inference strategies [34,42,61].
Yet, such defenses are often circumvented by more powerful attacks [9] or adaptively engineered adversarial inputs [5], resulting in a constant arms race between attackers and defenders [31].
This work is among the first to explore attacks against DNNs with interpretability as a means of defense.Transferability -One intriguing property of adversarial attacks is their transferability [56]: adversarial inputs crafted against one DNN is often effective against another one.
This property enables black-box attacks -the adversary generates adversarial inputs based on a surrogate DNN and then applies them on the target model [33,40].
To defend against such attacks, the method of ensemble adversarial training [58] has been proposed, which trains DNNs using data augmented with adversarial inputs crafted on other models.This work complements this line of work by investigating the transferability of adversarial inputs across different interpretation models.Interpretability -A plethora of interpretation models have been proposed to provide interpretability for blackbox DNNs, using techniques based on back-propagation [50,52,53], intermediate representations [14,47,64], input perturbation [16], and meta models [10].
The improved interpretability is believed to offer a sense of security by involving human in the decision-making process.
Existing work has exploited interpretability to debug DNNs [39], digest security analysis results [20], and detect adversarial inputs [32,57].
Intuitively, as adversarial inputs cause unexpected DNN behaviors, the interpretation of DNN dynamics is expected to differ significantly between benign and adversarial inputs.However, recent work empirically shows that some interpretation models seem insensitive to either DNNs or data generation processes [1], while transformation with no effect on DNNs (e.g., constant shift) may significantly affect the behaviors of interpretation models [27].
This work shows the possibility of deceiving DNNs and their coupled interpretation models simultaneously, implying that the improved interpretability only provides limited security assurance, which also complements prior work by examining the reliability of existing interpretation models from the perspective of adversarial vulnerability.
This work represents a systematic study on the security of interpretable deep learning systems (IDLSes).
We present ADV 2 , a general class of attacks that generate adversarial inputs not only misleading target DNNs but also deceiving their coupled interpretation models.
Through extensive empirical evaluation, we show the effectiveness of ADV 2 against a range of DNNs and interpretation models, implying that the interpretability of existing IDLSes may merely offer a false sense of security.
We identify the prediction-interpretation gap as one possible cause of this vulnerability, raising the critical concern about the current assessment metrics of interpretation models.
Further, we discuss potential countermeasures against ADV 2 , which sheds light on designing and operating IDLSes in a more robust and informative fashion.measures the magnitude of spatial transformation and τ is a hyper-parameter controlling its importance.
In implementation, we solve Eqn (14) using an Adam optimizer.
We use RTS as a concrete example to show the implementation of AID.
In RTS, one trains a DNN g (parameterized by θ) to directly predict the attribution map g(x; θ) for a given input x. To train g, one minimizes the interpretation loss:int (θ) λ 1 r tv (g(x; θ)) + λ 2 r av (g(x; θ)) − log ( f c (φ(x; g(x; θ)))) + λ 3 f c (φ(x; 1 − g(x; θ))) λ 4 (15)with all the terms defined similarly as in Eqn (8).
In AID, let A denote the ADV 2 attack.
We further consider an adversarial distillation loss:aid (θ) −g(x; θ) − g(A(x); θ) 1 (16)which measures the difference of attribution maps of benign and adversarial inputs under the current interpreter g(·; θ).
AID trains g by alternating between minimizing int (θ) and minimizing aid (θ) until convergence.
Here we summarize the default parameter setting for the attacks implemented in this paper.
For regular PGD, we set the learning rate α = 1.
/255 and the perturbation threshold ε = 0.031.
Table 11 list the parameter setting of STADV-based ADV 2 .
The Adam optimizer in our experiments uses the hyperparameter setting of (α, β 1 , β 2 ) = (0.01, 0.9, 0.999).
Below we include more experimental results that complement the ones presented in § 4 and § 5.
Figure 17 shows a set of sample inputs (benign and adversarial) and their attribution maps generated by GRAD, CAM, MASK, and RTS on DenseNet.
the attribution maps of benign and adversarial (PGD, ADV 2 ) inputs, which complements the results in Figure 5.
We normalize the L 2 measures by dividing them by the square root of the number of pixels.
ResNet DenseNet Table 12.
L p distance between attribution maps of benign and adversarial (P-PGD, A-ADV 2 ) inputs.
∆ (L 1 ) ∆ (L 2 ) ∆ (L 1 ) ∆ (L 2 ) GRAD P 0.10 In § 4, we use the dataset from the ISIC 2018 challenge 2 , and adopt a competition-winning model [18] (with ResNet as its backbone), which achieves the second place in the challenge.
The confusion matrix in Figure 18 shows the performance of the classifier in our study.
Figure 19 visualizes attribution maps of benign and adversarial (STADV, STADV-based ADV 2 ) inputs on DenseNet.
Figure 21 visualizes attribution maps of target and adversarial (ADV 2 ) inputs on DenseNet, which complements the results shown in Figure 12.
Figure 22 compares the L 1 measures and IoU scores of adversarial maps w.r.t. benign and target cases on DenseNet.
1676 29th USENIX Security Symposium USENIX Association xA.
Implementation Details A1: Details of StAdv-based ADV 2 2 2 We first briefly introduce the concept of spatial transformation.
Let˜xLet˜ Let˜x i be the i-th pixel of adversarial input˜xinput˜ input˜x and ( ˜ u i , ˜ v i ) be its spatial coordinates.
With flow-based transformation, ˜ x is generated from another input x by a per-pixel flow vector r, where r i = (∆u i , ∆v i ).
The corresponding coordinates of˜xof˜ of˜x i in x are given by (u i , v i ) = ( ˜ u i + ∆u i , ˜ v i + ∆v i ).
As (u i , v i ) do not necessarily lie on the integer grid, bilinear interpolation [25] is used to compute˜xcompute˜ compute˜x i :where j iterates over the pixels adjacent to (u i , v i ) in x. With STADV as the underlying attack framework, ADV 2 can be constructed as optimizing the following objective:where flow (r) = ∑ i ∑ j∈N (i) ∆u i − ∆u j 2 2 + ∆v i − ∆v j 2 A. Implementation Details A1: Details of StAdv-based ADV 2 2 2 We first briefly introduce the concept of spatial transformation.
Let˜xLet˜ Let˜x i be the i-th pixel of adversarial input˜xinput˜ input˜x and ( ˜ u i , ˜ v i ) be its spatial coordinates.
With flow-based transformation, ˜ x is generated from another input x by a per-pixel flow vector r, where r i = (∆u i , ∆v i ).
The corresponding coordinates of˜xof˜ of˜x i in x are given by (u i , v i ) = ( ˜ u i + ∆u i , ˜ v i + ∆v i ).
As (u i , v i ) do not necessarily lie on the integer grid, bilinear interpolation [25] is used to compute˜xcompute˜ compute˜x i :where j iterates over the pixels adjacent to (u i , v i ) in x. With STADV as the underlying attack framework, ADV 2 can be constructed as optimizing the following objective:where flow (r) = ∑ i ∑ j∈N (i) ∆u i − ∆u j 2 2 + ∆v i − ∆v j 2
