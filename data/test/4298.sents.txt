The proposed tutorial mixes the introductory and cutting-edge types.
It will offer a gentle introduction to recent advances in structured modeling with discrete latent variables, which were not previously covered in any ACL/EMNLP/IJCNLP/NAACL related tutorial.
The closest related topics covered in recent tutorials at NLP conferences are:• Variational inference and deep generative models (Aziz and Schulz, 2018); 1• Deep latent-variable models of natural language (Kim et al., 2018).
2 Our tutorial offers a complementary perspective in which the latent variables are structured and discrete, corresponding to linguistic structure.
We will briefly discuss the modeling alternatives above in the final discussion.
Below we sketch an outline of the tutorial, which will take three hours, separated by a 30-minutes coffee break.
We aim to provide the first unified perspective into multiple related approaches.
Of the 31 referenced works, only 6 are co-authored by the presenters.
In the outline, the first half presents exclusively work by other researchers and the second half present a mix of our own work and other people's work.
The audience should be comfortable with:• math: basics of differentiability.
• language: basic familiarity with the building blocks of structured prediction problems in NLP, e.g., syntax trees and dependency parsing.
• machine learning: familiarity with neural networks for NLP, basic understanding of backpropagation and computation graphs.
André Tsvetomila Mihaylova 4 is a PhD student in the DeepSPIN project at Instituto de TelecomunicaçTelecomunicaç˜Telecomunicações in Lisbon, Portugal, supervised by André Martins.
She is working on empowering neural networks with a planning mechanism for structural search.
She has a master's degree in Information Retrieval from the Sofia University, where she was also a teaching assistant in Artificial Intelligence.
She is part of the organizers of a shared task in SemEval 2019.
