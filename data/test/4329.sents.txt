End users learn defensive security behaviors from a variety of channels, including a plethora of security advice given in on-line articles.
A great deal of effort is devoted to getting users to follow this advice.
Surprisingly then, little is known about the quality of this advice: Is it comprehensible?
Is it actionable?
Is it effective?
To answer these questions, we first conduct a large-scale, user-driven measurement study to identify 374 unique recommended behaviors contained within 1,264 documents of online security and privacy advice.
Second, we develop and validate measurement approaches for evaluating the quality-comprehensibility, perceived actionability, and perceived efficacy-of security advice.
Third, we deploy these measurement approaches to evaluate the 374 unique pieces of security advice in a user-study with 1,586 users and 41 professional security experts.
Our results suggest a crisis of advice prioritization.
The majority of advice is perceived by the most users to be at least somewhat actionable, and somewhat comprehensible.
Yet, both users and experts struggle to prioritize this advice.
For example, experts perceive 89% of the hundreds of studied behaviors as being effective, and identify 118 of them as being among the "top 5" things users should do, leaving end-users on their own to prioritize and take action to protect themselves.
It is often considered ideal to remove end users from the security loop, reducing both their burden and the chance of potentially harmful errors [12].
However, removing the user entirely has proven difficult, if not impossible.
Users are still responsible for protecting themselves in a variety of situations, from choosing and protecting passwords, to recognizing phishing emails, to applying software updates, and many more.Researchers and practitioners have spent significant time and effort encouraging users to adopt protective behaviors.
Examples include redesigning warnings to make them harder to ignore [7,8,15,61,62], testing scores of alternative authentication methods intended to reduce user burden [5], "nudging" users toward better behavior [1,17], and even using unicorns to promote secure authentication in encrypted messaging [64].
Despite all this encouragement, user adoption of protective behaviors remains inconsistent at best [43,54,67].
If we wish to improve overall outcomes, it is insufficient to consider protective behaviors independently from each other; we must instead consider the cumulative ecosystem of security-behavior messaging and its effect on users.
For example, there are limits to how much time and effort users can spend on protective behaviors [3], and some protective behaviors may require more effort than they are worth [23,47].
Further, recommended behaviors are sometimes conflicting [10,26,50], change over time (e.g., from changing passwords frequently to limiting password changes except in cases of breach [9,20], and (as with any topic on which people provide advice to others) there is likely to be significant misinformation available.It is critical, therefore, to understand where users get their security information, and what they are learning.
Previously, researchers have identified several key sources of security information and advice: friends and family, fictional media, device prompts, and of course, the web [18,41,43,46].
However, the content of this advice has remained largely unexamined.We make three primary contributions:1.
We create the first comprehensive taxonomy of enduser-focused security and privacy advice.
To do so, we scraped 1,264 documents of security advice from the web, identified based on user-generated search queries from 50 users and via recommendations from vetted expert.
We then manually annotated 2,780 specific pieces of advice contained in these 1,264 documents, ultimately identifying 374 unique advice imperatives, 204 of which were documented for the first time in this work [4,10,11,26,35,50].2.
We develop measurement approaches for and validate a novel set of advice quality metrics: perceived actionabil-ity, perceived efficacy, and comprehensibility.
We show that these metrics correlate with the ultimate goal of security advice: end-user adoption of secure behaviors.3.
We conduct a study with 1,586 users and 41 professional security experts to evaluate the quality of the current body of security advice: we evaluate all 374 advice imperatives along these quality axes, examining the relative quality of different topics (e.g., passwords vs. privacy) and advice-givers (e.g., the government vs. popular media), identifying areas needing improvement.Our results suggest the key challenge is not in the quality of security advice, but in the volume and prioritization of that advice.
While users find the majority of the 374 advice imperatives they evaluate fairly actionable and somewhat comprehensible, they struggle to identify which advice is most important, listing 146 pieces of advice as being among the top 3 things they should attempt to do.
Yet, we know that they do not adopt anywhere near this many protective behaviors [43,56,67,68], nor would doing so be practical [3].
We find little evidence that experts are any better off than end-users on the subject of security advice: experts identify 118 pieces of security advice as being among the top 5 things they would recommend to a user, consider 89% of the 374 pieces of advice to be useful, and struggle with internal consistency and alignment with the latest guidelines (for example, claiming that failing to change passwords is harmful, despite the latest NIST advice to the contrary).
Thus, users -whose priority ratings of advice have little to no correlation with expert priority ratings -are left to fend for themselves, navigating through a sea of reasonably well-crafted but poorly organized advice.
These findings suggest that the path forward for security advice is one of data-driven measurement, minimality and practicality: experts should rigorously measure the impact of suggested behaviors on users' risk and ruthlessly identify only the minimal set of highest impact, most practical advice to recommend.
In this section, we review related work on security education and advice, as well as measurement of text quality.Security education and advice.
Users receive security advice from a variety of different sources, including from websites, TV, and peers, depending on their level of expertise, access to resources, and demographics [18,36,43,45,46].
People also learn from negative experiences -their own and others' -through stories about security incidents [41].
The negative experiences that inform these stories are effective but carry undesirable emotional and practical costs.
Some researchers have thus explored comic strips and interactive approaches as effective means of teaching security lessons [13,29,34,53,57,71]; others have used visual media to teach security [2,19].
Rader and Wash [40] found that the types of security information users encounter depends strongly on the source, with websites seeking to impart information from organizations, news articles focusing on large breaches or attacks, and interpersonal stories addressing who is hacking whom and why.
While there are many sources of security information, prior work has shown that websites are one of the most common sources of advice specifically [43].
We therefore aim to characterize advice that is available on the Internet.
Rather than use topic modeling, as in prior work [40], we manually coded each document we collected in order to deeply understand the online security advice ecosystem.In addition to studying where and how people get security advice, researchers have studied what is in that advice.
Ion et al. [26,50] found that experts and non-experts consider different practices to be most important; Busse et al. replicated this work in 2019 and found this was still true [10].
Reeder et al. [50] additionally report on advice imperatives provided by security experts.
We leverage this work as a starting point for our taxonomy, while also examining what users might find by directly seeking security advice.Prioritizing advice is important, because people and organizations have a limited "compliance budget" with which to implement security practices [3].
It has been shown that users make time-benefit tradeoffs when choosing a security behavior [47], and may find it irrational to follow all, or even most, security advice [23].
Further, advice can be difficult to retract once disseminated, creating a continuously increasing burden for users and organizations [24,25].
Text evaluation.
There are many ways to define and measure text quality.
Louis and Nenkova [32], for example, investigate the quality of science journalism articles using both general measures, like grammar or spelling correctness, and domain-specific measures, like the presence of narrative.
Tan et al. define quality using linguistic features -like Jaccard similarity, number of words, and number of first person pronouns -of successfully persuasive arguments on Reddit [63].
Perhaps the most common measure of text quality is comprehensibility: how easy or difficult it is for people to comprehend a document.
Prior work has considered the comprehensibility of three types of security-and privacy-relevant text: privacy policies [33,60], warning messages [21], and data breaches [72].
These investigations have shown that security and privacy content is often difficult to read, and that problems of readability may also be compounded by other factors such as the display constraints of mobile devices [60].
In this work, we consider a broader class of security-relevant documents -security advice from the web -and we apply multiple measures of quality along three axes: comprehensibility, actionability, and accuracy.
There are a number of different mechanisms for measuring the comprehensibility of adult texts.
Redmiles et al. [49] evaluate the validity of these different mechanisms.
We leverage their proposed decision strategy and tools for our measurements (see Section 4.4 for more detail).
We used two approaches to collect text-based security advice aimed at end users: (1) We collected search queries for security advice from 50 crowdworkers and scraped the top 20 articles surfaced by Google for each query, and (2) we collected a list of authoritative security-advice sources from computer security experts and librarians and scraped articles accordingly.User search query generation.
We recruited 50 participants from Amazon Mechanical Turk (AMT) to write search queries for security advice.
To obtain a broad range of queries, we used two different surveys.
The first survey asked participants to list three digital security topics they would be interested in learning more about, then write five search queries for each topic.
Participants in the second survey were shown the title and top two paragraphs of a security-related news article (See Appendix A), then asked if they were interested in learning more about digital security topics related to the article.
If the participant answered yes, they were prompted to provide three associated search queries.
Participants who answered no were asked to read additional articles until they reported interest; if no interest was reported after six articles, the survey ended without creating queries.
Twenty-five people participated in each survey and were compensated $0.25 (first survey, 2.5 min completion time) or $0.50 (second survey, 4 min completion time).
Our protocol was approved by the University of Maryland IRB.From these surveys, we collected 140 security-advice search queries.
After manual cleaning to remove duplicates and off-topic queries, 110 queries remained.
Examples of these queries include, "how safe is my information online?
," "how to block all windows traffic manually?
," and "common malware.
"We then used the Diffbot API 1 to scrape and parse the top twenty Google search results for these queries 2 .
Our collection was conducted in September 2017.
Expert advice recommendations.
To identify the types of articles users might be referred to if they asked an authority figure for advice, we asked 10 people for a list of websites from which they personally get security advice or which they would recommend to others.
These included five people holding or pursuing a Ph.D. in computer security, two employees of our university's IT department who have security-related job responsibilities, and three librarians from our university and local libraries.
Two researchers visited each recommended website and collected URLs for the referenced advice articles.
Manual collection was required, as many of these expert sites required hovering, clicking images, and traversing multiple levels of sub-pages to surface relevant advice.
(An initial attempt to use an automated crawl of all URLs one link deep from each page missed more than 90% of the provided advice.)
As with the search corpus, we then used the Diffbot API to parse and sanitize body elements.Initial corpus & cleaning.
The resulting corpus contained 1,896 documents.
Examples include Apple and Facebook help pages, news articles from Guardian and the New York Times, advice or sales material from McAfee, Avast, or Norton, U.S. CERT pages, FBI articles, and articles from Bruce Schneier's blog.
To ensure that all of the documents in our corpus actually pertained to online security and privacy, we recruited CrowdFlower crowdworkers 3 to review all of the documents and answer the following Yes/No question: "Is this article primarily about online security, privacy, or safety?"
We retained all documents in our corpus for which three of three workers answered 'Yes.'
When two of the three initial workers answered 'Yes,' we recruited an additional two workers to review the document, retaining documents for which four of the five workers answered 'Yes.'
After this cleaning, 1,264 of the initial 1,896 documents were retained in our corpus.Extracting & evaluating advice imperatives.
Next, we decomposed these documents into specific advice imperatives (e.g., "Use a password manager").
Two members of the research team manually annotated each of the 1,264 documents in our corpus to extract the advice imperatives contained within them.We constructed an initial taxonomy of advice imperatives based on prior work that had identified user security behaviors [4,11,26,35].
We manually reviewed each of these articles, made a list of all described behaviors, and reached out to the article authors to ask for any additional behaviors not reported in the papers.
The authors of [26] shared their codebook with us.
After merging duplicates, our initial list contained 196 individual advice imperatives.
We used this taxonomy as a starting point for annotating our security advice corpus.
To ensure validity and consistency of annotation, two researchers double-annotated 165 (13.1%) of the advice documents, adding to the taxonomy as needed.We reached a Krippendorff's alpha agreement of 0.69 (96.36% agreement) across the 12 high-level code categories, which is classified as substantial agreement [30].
Given this substantial agreement, and the large time burden of double annotating all 1,264 documents, the researchers proceeded to independently code the remaining documents.
To evaluate the consistency of our in-dependent annotations, we compute the intraclass correlation (ICC), a commonly used statistical metric [59] for assessing the consistency of measurements such as test results or ratings.
We find that both annotators had an ICC above 0.75 (0.823 for annotator 1 and 0.850 for annotator 2), indicating "good" consistency in their annotations [27].
At the end of the annotation process, the researchers reviewed each other's taxonomies to eliminate redundancies.
Ultimately, our analysis identified 400 unique advice imperatives targeting end users: 204 newly identified in our work, 170 identified in prior literature and also found in our corpus, and 26 from the research literature that did not appear in any of our documents.
The full document corpus, set of advice imperatives, together with linked evaluation metrics, can be found here: https://securityadvice.cs.umd.edu.As part of this process, we also identified two categories of irrelevant documents present in our corpus: 229 documents that were advertisements for security or privacy products and 421 documents (news reports, help pages for specific software, etc.) containing no actionable advice.
To maintain our focus on end-user advice, we also discarded imperatives targeting, e.g., system administrators or corporate IT departments.
This resulted in a final corpus of 614 documents containing security advice.It is important to note that we use manual annotation to analyze this data because (a) we cannot use supervised automated classification, as there exists at present no labeled training data from which to build a classifier (this work establishes such labeled data) and (b) unsupervised modeling of advice "topics" and automated tagging of non-standardized open text with those topics, with a very large number of possible classes as in our case, remains an open, unsolved problem [31].
Twelve topics of security advice from 476 unique web domains.
Our annotation process identified 374 security advice imperatives relevant to end-users.
These pieces of advice occurred 2780 times overall, with an average of 4.53 imperatives per document.
We categorized these pieces of advice into 12 high-level topics, which are summarized in Table 1.
Figure 1 (left) shows the distribution of topics across the documents in our corpus.
We identified 476 unique web domains in our corpus; we manually grouped these domains into broader categories, while retaining certain specific, highfrequency domain owners of interest, such as Google and the Electronic Frontier Foundation (EFF).
Hereafter, we use "domain" to refer to these groupings.
Figure 1 (right) shows the distribution of domains in our corpus.
After identifying and categorizing the broad set of security advice being offered to users, we next evaluated its quality.
Specifically, we measure the perceived actionability and perceived efficacy of the imperatives, as well as the comprehensibility of the documents.
Below we describe our measurement approach, including the novel metrics we developed, the user study (1,586 users and 41 professional security experts) we conducted to instantiate these metrics, and our assessment of the metrics' validity.
Perceived actionability.
We assess perceived actionability by asking users from the general population to report how hard they think it would be to put a given imperative into practice.
In particular, our actionability questionnaire incorporates four sub-metrics:• Confidence: How confident the user was that they could implement this advice.
• Time Consumption: How time consuming the respondent thought it would be to implement this piece of advice.
• Disruption: How disruptive the user thought it would be to implement this advice.
• Difficulty: How difficult the user thought it would be to implement this advice.each evaluated on a 4-point Likert scale from "Not at All" to "Very."
The full questionnaire, which included an example to help respondents distinguish among the different sub-metrics, is included in Appendix C. Each imperative was evaluated by three respondents, and each respondent evaluated five randomly drawn imperatives.
These four sub-metrics align with theoretical foundations relevant to security behavior.
The confidence sub-metric is drawn from Protection Motivation Theory [52], which identifies perceived ability to protect oneself as a key component of protective behavior implementation, and from the Human in the Loop model [12], which identifies knowledge acquisition-knowing what to do with information-as a key component of security behavior change.
The time-consumption and disruption sub-metrics are created to align with the "cost" of the behavior, which has been found to be an important decision-making factor in economic frameworks of secure behavior [3,23,47,48].
Finally, the difficulty sub-metric is used to align with the capabilities component of the Human in the Loop model [12].
Perceived efficacy.
We also use human-generated data to measure the perceived efficacy of the advice imperatives.
We asked professional security experts (see qualification criteria below) to answer an evaluation questionnaire for each piece of security advice.
Each advice imperative was again evaluated by three respondents.
The efficacy questionnaire evaluated, for each advice imperative, Perceived efficacy: whether the expert believed that a typical end user following this advice would experience an improvement in, no effect on, or harm to their four randomly drawn documents.
For measurements conducted with the general population (measurements of actionability and comprehensibility), we recruited users from the survey research firm Cint's 6 survey panel, which allows for the use of quota sampling to ensure our respondents' demographics were representative of the U.S. population within 5% on age, gender, race, education and income.
We recruited a total of 1,586 users in June 2019 to evaluate the actionability and comprehensibility of our security advice.
Participants were compensated in accordance with their agreement with Cint.The efficacy measurements were conducted with professional security experts.
We recruited experts during May and June 2019.
We did so by tweeting from the lead author's Twitter account, asking well-known security Twitter accounts to retweet, and leveraging our personal networks.
We also posted in multiple professional LinkedIn groups and contacted authors of security blogs.
All recruited individuals completed a screening questionnaire to assess their security credentials, including what security certifications they held, whether they had ever participated in a CTF, what security blogs or publications they read, whether they had ever had to write a program that required them to consider security implications, whether they had ever penetration-tested a system, and their current job title.
We also asked them to upload their resume or link to their personal website so that we could verify their credentials.
We considered anyone who had done two or more of: participating in a CTF, penetration testing a system, and writing programs that required them to consider security implications, OR who held security certifications (including computer security professors) to be an expert.
Ultimately, 41 qualified experts evaluated our security advice.
The majority of our experts were practitioners; only three were academics.
Our experts have diverse workplace contexts: engineer through director-level information security professionals for large corporations and government agencies, red team/pen testers, independent security consultants, and privacy-focused professionals at large and well-known non-profit/advocacy organizations.
Experts were paid $1 for each piece of advice they evaluated.
Advice was evaluated in batches of 10; experts were allowed to complete as many batches as desired and were able to skip previously-evaluated pieces of advice.
On average, experts evaluated 38 pieces of advice each.
We evaluate the validity of our measurements in two ways: (1) we check the reliability of ratings provided by our user 6 https://www.cint.com/reach-survey-respondents/ and expert evaluators, again using the ICC metric (see Section 3) and (2) we examine whether these quality measures are discriminant, whether they correlate with behavior adoption (the ultimate goal of security advice), and, where possible, whether we reproduce results of prior work on security advice.
We report on (1) here and point the reader to Section 9 for the results of (2).
Overall, all of our evaluators achieved at least "good" reliability in evaluating our three metrics of advice quality [27].
For actionability, reliability was "very good": ICC = 0.896, 0.854, 0.868, and 0.868 for confidence, time consumption, disruption, and difficulty, respectively.
For efficacy, the experts achieved "very good" reliability, with an ICC of 0.876, and for comprehensibility, our Cloze raters had "excellent" reliability (ICC=0.989), while our ease raters achieved "good" reliability (ICC = 0.757).
As with all measurement and user studies, our work has certain inherent limitations.
First, it is possible that our security advice corpus is not fully representative of the ecosystem of security advice.
We used multiple techniques -soliciting advice recommendations from experts, two methods for collecting search queries from users -to ensure broad coverage of advice in order to mitigate this potential limitation.
Second, it is possible that our manual annotation process was inaccurate.
We conducted a double annotation of over 10% of our documents, achieving "sufficient" inter-annotator agreement before proceeding to annotate independently, to mitigate this risk; further, both coders conducted a full review of each other's taxonomies once annotation was finished, and reached a final, cohesive taxonomy that was applied to all documents.
Third, we cannot capture all possible types of relevant expertise.
To minimize data collection, we screened our experts for expertise but explicitly did not collect demographic data; examining how experts' sociodemographic backgrounds may affect how experts prioritize advice may be an exciting direction for future work.Fourth, due to the volume of advice, experts and users evaluated advice in the abstract and did not evaluate all advice.
We find, through a X 2 proportion test, that there is not a statistically significant difference between the priority ratings of the 26 experts who rated less than 30 pieces of advice and the 15 who rated more advice; however, lack of a full sense of the dataset may still have affected prioritization.Fifth, our instantiations of our metrics may not provide a full picture of the comprehensibility, perceived efficacy, and perceived actionability of our documents.
To mitigate this limitation, we relied upon established, validated tools from library science and NLP [49,65] and constructed questionnaires that we robustly pre-tested using techniques such as cognitive interviewing, following survey methodology best practice for mitigating self-report biases such as social desirability [44].
Table 2: List of the most unactionable advice based on user ratings.
The first four columns indicate advice with median rating of "not at all" confident and"very" time consuming, disruptive, and/or difficult.
The fifth column indicates expert-perceived efficacy and the sixth column provides expert-estimated median risk reduction for efficacious advice (negative for harmful advice).
nance was also considered quite actionable: 94.1% of finance advice was perceived as at most "slightly" time-consuming or disruptive to implement, and more than 80% of this advice was perceived as at most "slightly" difficult to implement.
Advice about passwords scored well on two of the four actionability submetrics: for more than 80% of passwords advice people were at least "somewhat" confident they could implement it and perceived it as at most "slightly" difficult to implement.The least-actionable advice is about data storage and network security.
The topic with the highest proportion of poor (lowest two ratings on Likert scale) actionability ratings, across all four metrics, was data storage.
More than half the data storage imperatives received confidence responses of "slightly" or "not at all," there was no advice about data storage for which people were "very" confident.
Similarly, 58.8%, 41.2%, and 47.1% of the imperatives about data storage were rated at least "somewhat" time consuming, disruptive, and difficult to implement, respectively.
Advice about network security performed nearly as badly on three of the four actionability submetrics; participants were confident they could implement barely half the advice about network security, and they perceived at least 40% of network security advice as "very" time consuming or difficult to implement.Privacy advice polarizing in perceived actionability.
It is additionally interesting to note that the actionability ratings for privacy advice were quite split.
Near-equal proportions of privacy advice were rated as at least "somewhat" time security, and finance achieved at least partial comprehension on average (Cloze scores, mean across the topic, above 50%).
Finance-related documents had particularly low variance in scores, with a standard deviation of 6.22%.
The remaining topics had mean Cloze scores under 50%, indicating that the majority of test takers struggled to comprehend the average text on these topics.
Password-and networksecurity-related documents had particularly low mean scores, with very wide score spreads.
Passwords was the most popular topic in the corpus and also had the highest standard deviation in Cloze scores; we therefore hypothesize that the low scores may be at least partially about quantity.
On the other hand, network security is a particularly technical topic, so the low scores may relate to additional complexity or jargon.There was no significant difference in reading ease perceptions among different topics (p = 0.999, Kruskal-Wallis 13 ).
The most comprehensible sources are general news channels, subject-matter experts (SMEs), non-profits, and security and computer-repair companies.
To understand whether some advice-givers provided more readable advice than others, we examined Cloze scores grouped by domain.
Figure 9 summarizes these results.
The Cloze scores of the domains were significantly different: p < 0.001, ANOVA (all pairwise tests remain significant after Holm-Bonferonni correction).
Of the 30 domain groups we considered, seven scored above 50% (mean across documents): SMEs, general news outlets, how-to websites, non-tech-focused and tech-focused nonprofit organizations, security companies, and computer-repair companies.
Within particular categories, we see that some organizations perform better than others (Appendix B); we discuss the more notable cases of variance below.
As with topics, there was not a significant difference in ease perception by domain (p = 0.999, Kruskal-Wallis).
Government organizations.
Among U.S. government organizations, ic3.gov, whitehouse.gov, ftc.gov, and dhs.gov had average scores mapping to partial comprehension or better; the remaining domains perform worse.
We had only five non-U.S. government domains in our dataset, three of which (csir.co.za, staysmartonline.gov.au, and connectsmart.gov.nz) had mean scores of partial comprehension or above.Child-focused organizations.
Encouragingly, documents from non-profit organizations (both technology focused and not) that were aimed toward children (e.g., childline.org.uk, netsmartz.org, safetynetkids.org.uk) appear to be among the most readable.
That said, content collected from school websites was not particularly readable, with mean Cloze scores indicating low comprehension, suggesting that schools may be better off obtaining content from child-focused nonprofit organizations.Technical non-profits.
Documents from non-profit organizations with technical focus had wider variance.
Documents from the Tor Project, GNU, and techsoup.org had mean Cloze scores of at least partial comprehension.
However, documents from nine other technical non-profits, including Mozilla, Chromium, and Ubuntu as well as organizations focused specifically on helping non-experts (e.g., libraryfreedomproject.org) had mean Cloze scores well below this threshold.
Documents from the EFF and Tactical Techsponsored organizations also had mean Cloze scores mapping to low comprehension.
This is important, as documents from these two organizations make up 21% of our corpus.Corporations.
Security-focused companies and those offering computer-repair services both scored very high on comprehensibility.
We hypothesize that for these companies, which focus on lay users as customers, providing readable materials may be tantamount to a business requirement.
On the other hand, non-security-focused companies -including some frequently under fire for privacy and security issuesscored poorly: mean Cloze scores for Google, Facebook, and Apple were 45.1%, 37.9%, and 41.7%, respectively.Low-comprehension platforms.
Finally, seven of the 30 advice-givers we examined provided particularly difficult to read advice (mean Cloze scores under 40%): SANS (sans.org), security forums (e.g., malwaretips.com, wilderssecurity.com), MOOC platforms (e.g., lynda.com, khanacademy.org), consumer rating sites (e.g., consumerreports.org, av-comparatives.org), Facebook, Technical Q&A websites (e.g., stackoverflow.com, stackexchange.com), and academic publications.While it is not necessarily problematic for more technical content such as that from academic security publications and security forums to be incomprehensible to the the average person, low readability from organizations such as the Library Freedom Project, MOOCs, Facebook Help pages, and Technical Q&A websites may make it difficult for non-experts to stay secure.
This work makes three primary contributions.We create a taxonomy of 374 pieces of security advice.
This work provides a comprehensive point-in-time taxonomy of 374 end-user security behaviors, including 204 pieces of security advice that were not previously catalogued in the literature.
The full set of behaviors can be explored here: https://securityadvice.cs.umd.edu.
This taxonomy provides (i) insight into the scope and quantity of advice received by users, (ii) a tool for researchers to consult when considering what security and privacy behaviors to study or analyze, and (iii) a mechanism for the broader security community to move forward with improving security advice by identifying advice in need of repair or retirement.We develop and evaluate axes of security advice quality.
Our approach to evaluating security advice is in itself a contribution: the axes of quality that we identify (comprehensibility, actionability, and efficacy) and the measurement approaches we designed to assess them can be applied to new advice that is created to ensure that as we move forward in advice-giving, we create higher-quality, more effective advice.
Before we can recommend further use of these evaluation strategies, however, we must be convinced of their validity.
Specifically, do the quality measurements correlate with behavior adoption (the ultimate goal of security advice), are the measurements discriminant, and are the measurements consistent with prior work (where applicable)?
In an initial validation using the results of our work, we find that our metrics indeed correlate with (reported) adoption, lending support for the importance of the advice quality factors we have operationalized.
We find that all four of our actionability sub-metrics correlate with reported behavior adoption by users.
Additionally, we find that priority ranking -one of our metrics of efficacystrongly correlates with reported adoption as well, for both general users and experts.We also find that our quality metrics are indeed discriminant: that is, they measure different components of advice quality.
For example, while network security was least readable and also had low actionability, data storage did quite well on readability while scoring consistently low on actionability.
Similarly, documents containing advice about software security and antivirus were among the more difficult to read, but were not high in implementation difficulty, indicating that readability of the document containing the advice is different from the actionability of the advice itself.Further, we examine whether we can replicate the results of prior studies in which security experts were asked to prioritize 20 pieces of security advice [10,26,50].
We find that our prioritization results replicate these quite closely.
Two of the three behaviors given "number one" priority by our experts overlap with the top three behaviors suggested by experts in both papers: "update system" and "use unique passwords."
The third-most-important behavior identified by both papers "use two-factor auth", is rated as a "top 3" priority by our experts and ranked #25 out of 374 across all of our advice.Of course, this preliminary validation connects these axes of advice quality to reported, rather than actual, behavior.
Replication is necessary to fully validate any new metrics, and to examine how they perform in broader application (e.g., having both users and experts rate the efficacy of the advice).
We rigorously evaluate the comprehensibility, perceived efficacy, and perceived actionability of our corpus.
By applying our metrics to the taxonomy we developed, we provide a thorough and novel characterization of the quality of the security-advice ecosystem.
While prior work focused on expert and user prioritization of a small set of security advice (at most, 20 topics) [10,26,50], we evaluate a much larger set of advice and conduct a more comprehensive evaluation that considers not only prioritization, but also comprehensibility, perceived actionability, perceived efficacy, and how these factors interact.
Further, our metrics allow us (differently from prior work) to characterize both generalized advice imperatives and specific wording within particular documents.Overall, we find that security advice is perceived as fairly actionable -only 49 advice imperatives were rated by users as 'very' unactionable on one of our four metrics -as well as effective.
The majority of security advice (89%) was perceived as effective by professional security experts.Yet, we know that users do not adopt even a fraction of this advice consistently, despite their best intentions [43,56,67,68].
This may be due in part to mis-comprehension of the instructions: the hundreds of documents we evaluate exhibit only low to partial comprehensibility for the general public.
A larger factor, however, appears to be a crisis of advice prioritization.
The 41 professional security experts consulted in this study not only evaluated 89% of the advice in our corpus as accurate, but reported that 118 pieces of advice were in the top 5 items they would recommend to users.
By asking people to implement an infeasible number of behaviors, with little guidance on which is the most important, we slowly chip away at compliance budgets [3], leaving users haphazardly selecting among hundreds of "actionable," "effective," "high-priority" behaviors.
Our results suggest two key directions of focus for moving toward a healthier ecosystem of security advice.Measurement and a new focus on minimality.
We as security experts and advice givers have failed to narrow down a multitude of relatively actionable, but half-heartedly followed, security behaviors to a key, critical set that are most important for keeping users safe.
The U.S. government alone offers 205 unique pieces of advice to end users, while non-technical news media, such as CNN and Forbes, offers over 100 unique pieces of advice to users.
This overload of advice affects a large portion of the user population: prior work [43,45] suggests the government is a primary source of advice for more than 10% of users, while 67.5% of users report getting at least some of their security advice through the news media.Our struggle as experts to distinguish between more and less helpful advice may be due to unfalsifiability: being unable to identify whether a piece of advice is actually useful, or prove when it is not.
Without measurement of impact on actual security, or proven harm, we presume that everything is slightly useful against potential harms.
Fixing this problem will require rigorous measurement (e.g., comparing the effect of different practices on frequency of compromise) to evaluate which behaviors are the most effective, for which users, in which threat scenarios.
It will also require a strong commitment among security advice givers to minimality and practicality: empirically identifying the smallest and most easily actionable set of behaviors to provide the maximum user protection.If we do not make changes to our advice-giving approach, this situation is destined to get worse.
As new attacks continue to emerge, we are likely to continue to issue new, reactive advice without deprecating old advice (that might still be at least somewhat useful) or reevaluating overall priorities [25].
Further, we need to explore how to better disseminate updates to best practices.
For example, many experts in our study were still emphasizing password changes and avoiding storing passwords, despite this advice having been updated and disproven in the most recent NIST standards [20].
Delays in propagating new priorities among experts will surely translate into even more severe lags in end-user behavior.Based on our analysis, the U.S. government is currently the giver of the most advice.
Unifying the voices across the government into a single central authority for both end-users and external experts to turn to for validated best practicessimilar to the role police departments serve for community education on car break-ins, or the role of the surgeon general for health advice -may help to cut down on inconsistent or delayed updates to advice.
A similar effort could be made to reduce redundancy across trusted non-profits and advocacy groups by encouraging such groups to all support a centralized advice repository rather than each providing their own.Fixing existing advice.
While the primary outcome of this work is that we need less advice and more empirical measurement, we do note that a few topics of advice performed consistently worse than others across our evaluations and thus are good candidates for revision and improvement.
Advice about data storage topics (e.g., "Encrypt your hard drive," "Regularly back up your data," "Make sure to overwrite files you want to delete") scored poorly in actionability across our metrics.
This raises questions about whether we should be giving this advice to end users in the first place, and if so, how these technical concepts can better be expressed in an actionable way.
Network-security advice performed nearly as poorly, especially on user ratings of confidence, time consumption and difficulty.
This is perhaps even more concerning, as the advice on network security is far more general (e.g., "Use a password to protect your WiFi," "Secure your router," "Avoid using open Wi-Fi networks for business, banking, shopping").
Privacy advice was more of a mixed bag.
While a quarter of the advice about privacy was rated as unactionable, a significant proportion of the remaining privacy advice scored quite high on actionability.
Experts were less positive toward any privacy advice, with no advice about privacy being rated among the top 3 practices experts would recommend.
As privacy becomes increasingly important, and prominent in users' awareness, there appears to be significant room for improvement.Additionally, across all topics, many advice articles combined a diverse set of advice types that could be appropriate to different users; future work may wish to examine whether this is effective or whether articles focused on a single context are most appropriate.
Relatedly, future work may wish to pursue mechanisms for personalizing advice to users or helping users filter to advice that is most relevant to them, as searches for security advice are likely to surface context-broad advice that may or may not have direct relevance.
Figure 7 and 9 summarize the comprehensibility of the corpus.
Figure 10 summarizes the mean Cloze scores across specific advice providers who are members of the U.S. Government, non-tech non-profits, and technical non-profits.
The questions for this section of the survey are about the following advice: You should create a new email address if your last one is compromised.
An example of this advice might be: "Time for a new email address.
This is the last resort but it will be 100% effective at giving you a clean slate."
We are grateful to the reviewers and especially to our shepherd Mary Ellen Zurko for their feedback and guidance.
This material is based upon work supported by a UMIACS contract under the partnership between the University of Maryland and DoD.
Elissa M. Redmiles additionally wishes to acknowledge support from the National Science Foundation Graduate Research Fellowship Program under Grant No.
DGE 1322106 and a Facebook Fellowship.
xA Search Query Generation Prompt Articles A Search Query Generation Prompt Articles
