Verifiable outsourcing systems offload a large computation to a remote server, but require that the remote server provide a succinct proof, called a SNARK, that proves that the server carried out the computation correctly.
Real-world applications of this approach can be found in several blockchain systems that employ verifiable outsourcing to process a large number of transactions off-chain.
This reduces the on-chain work to simply verifying a succinct proof that transaction processing was done correctly.
In practice, verifiable outsourcing of state updates is done by updating the leaves of a Merkle tree, recomputing the resulting Merkle root, and proving using a SNARK that the state update was done correctly.
In this work, we use a combination of existing and novel techniques to implement an RSA accumulator inside of a SNARK, and use it as a replacement for a Merkle tree.
We specifically optimize the accumulator for compatibility with SNARKs.
Our experiments show that the resulting system reduces costs compared to existing approaches that use Merkle trees for committing to the current state.
These results apply broadly to any system that needs to offload batches of state updates to an untrusted server.
Verifiable outsourcing [4, 13, 15, 16, 21, 32, 45, 47, 49, 52, 56, 61, 78, 79, 96, 106-108, 111-115, 121, 123, 124] is a technique that enables a weak client to outsource a computation to a powerful server.
The server returns the result of the computation along with a proof that the computation was done correctly.
The proof must be succinct, which means that it must be short and cheap to verify.
Verifiable outsourcing is relevant in a number of scenarios, including weak IoT devices, wearables, and low-power devices.More recently, verifiable outsourcing has been deployed in blockchain environments, because on-chain work is expensive-literally.
Here, a batch of k transactions, say k = 1000, is outsourced to an untrusted server, called an aggregator, for processing.
The aggregator (1) verifies that the transactions are valid (e.g., properly signed), (2) computes the updated global state resulting from these transactions, and (3) generates a succinct proof that the aggregator correctly executed steps (1) and (2).
The updated state and the succinct proof are then sent to the blockchain.
In this approach, the (expensive) on-chain work is reduced to only verifying the proof-which is fast, taking time independent of the number of transactions k-and then recording the updated state.
Example systems that operate this way include Rollup [7], Coda [89], Matter [86], and Zexe [29].
The process described above is called verifiable outsourcing of state update [32].
In more detail, the state is a set of elements S = {x 1 , . . . , x M } from some universe X .
The blockchain (or a low-power device) stores only a succinct digest of S, e.g., the root of a Merkle tree whose leaves comprise the elements of S.
The untrusted but powerful aggregator stores the full set S, in the clear.
(Note that we treat S as public data-privacy is orthogonal to our goal, which is scalability).
When processing a batch of transactions as described above, the aggregator updates S to produce a new set S , then computes a new Merkle digest for S that it sends to the blockchain to be verified and recorded.
The aggregator's proof establishes that its starting state S is consistent with the current digest, that correctly applying transactions yields the ending state S , and that the new digest is consistent with S .
The succinct proof needed here is called a SNARK [19], which we define in more detail in the next section.
Constructing efficient SNARKs and optimizing their implementation is a very active area of research [13,15,16,49,64,70,96], with several new systems just in the last year [11,37,43,44,62,63,85,122].
A common thread in all of these systems is that the proving costs are enormous.
In particular, proving imposes multiple-orders-of-magnitude slowdown compared to native execution [96,106,116]; this can be defrayed via parallel execution, e.g., in clusters [45,121] or on GPUs [108,112].
Perhaps more importantly, for widely deployed SNARKs, proving correctness of large computations requires an amount of RAM proportional to the computation's execution time [16,96].
The result is that, even when proving is distributed across hundreds of workers, the largest reachable computation sizes are relatively small: only about 2 billion steps [121].
This imposes a strict upper bound on the number of transactions k that can be processed in a single batch.
This state of affairs has motivated a large body of work on computational primitives that yield efficient proofs.
Examples include arithmetic [79,96,108], control flow [96,108,116], persistent state [4,32,49,56,105], and random-access memory [12,13,16,32,79,116].
Our work continues in this vein, with a focus on reducing proving costs for computations involving persistent state or random-access memory.Our work.
A Merkle tree [90] is an example of an accumulator [17], a cryptographic primitive that lets one commit to a set S, and later prove that an element x is a member of S. Although Merkle trees are used pervasively in today's general-purpose verifiable state update applications, in this work we show that a Merkle tree is not the best choice for large batches of state updates when S is moderately to very large, say |S| ≥ 2 10 .
In particular, we show that replacing Merkle trees with RSA-based accumulators [24,40,81] significantly improves proving time and/or reachable computation size.
Our contributions are:• We define a new operation for RSA accumulators, which we call MultiSwap, that provides a precise sequential semantics for batched verifiable state updates ( §3).
• We synthesize existing and novel techniques for efficiently implementing MultiSwap (and, more generally, RSA accumulators) in the context of SNARKs ( §4).
These techniques include a hash function that outputs provable prime numbers, and a new division-intractable hash function.
Our approach makes use of very recent advances in manipulating RSA accumulators [24].
• We apply our techniques in two contexts ( §5).
The first, called Rollup [7,65,94], is a technique for batching cryptocurrency transactions off-chain in order to save on-chain work.
The second is a general-purpose RAM abstraction with long-lived state (i.e., over many proofs), which builds upon and improves prior work [12,13,16,32,116].
• We implement and evaluate ( §6, §7).
In particular, we compare our RSA accumulator implementation to Merkle trees in two benchmarks: one that measures only set operations, and one that implements a Rollup-style distributed payment application.
We also compare our RAM abstraction with existing work via a cost model analysis.In the set operations benchmark, we find that RSA accumulators surpass 2 20 -element Merkle trees for batches of ≈1,300 operations, and allow for 3.3× more operations to be performed in the largest proof sizes we consider.
In the Rollup application, RSA accumulators surpass 2 20 -element Merkle trees for ≈600 transactions, and allow 1.9× more transactions in the largest proofs.
For RAM, we find that for a RAM of size 2 20 , RSA accumulators surpass Merkle trees for ≈1000-4000 accesses, depending on write load.
Multisets.
A multiset is an unordered collection that may contain multiple copies of any element.
S 1 S 2 denotes the union of multisets S 1 and S 2 , i.e., the multiset S 3 where each element x ∈ S 3 has multiplicity equal to the sum of the multiplicities of x in S 1 and S 2 .
S 1 S 2 denotes the strict difference of multisets S 1 and S 2 , i.e., the multiset S 3 where each element x ∈ S 3 has multiplicity equal to the difference of multiplicities of x in S 1 and S 2 .
Note that S 1 S 2 is only defined if S 2 ⊆ S 1 .
RSA groups.
An RSA group is the group Z × N , i.e., the multiplicative group of invertible integers modulo N, where N is the product of two secret primes.
We define the RSA quotient group for N as the group Z × N /{±1}.
In this group, the elements x and N − x are the same, meaning that all elements can be represented by integers in the interval [1, N /2].
It is believed that this group has no element of known order, other than the identity.Proofs and arguments.
Informally, a proof is a protocol between a prover P and a PPT verifier V by which P convinces V that ∃υ : R(ι, υ) = 1, for a relation R, ι an input from V , and υ a (possibly empty) witness from P .
A proof satisfies the following properties:• Completeness: If ∃υ : R(ι, υ) = 1, then an honest P convinces V except with probability at most ε c 1 /2.
• Soundness: If ∃υ : R(ι, υ) = 1, no cheating prover P convinces V except with probability at most ε s 1 /2.
If soundness holds only against PPT P , this protocol is instead called an argument.
When the witness υ exists, one may also require the proof system to provide knowledge soundness.
Informally this means that whenever P convinces V that ∃υ : R(ι, υ) = 1, υ exists and P "knows" a witness υ (slightly more formally, there exists a PPT algorithm, an extractor, that can produce a witness via oracle access to P ).
Proof of exponentiation.
Let G be a finite group of unknown order.
Wesolowski [120] describes a protocol that allows P to convince V that y = x n in G, namely a protocol for the relation R given by R (n, x, y),· = 1 ⇐⇒ y = x n ∈ G.The protocol is: on input (n, x, y), V sends to P a random chosen from the first 2 λ primes.
1 P sends back Q = x n// ∈ G, and V accepts only if Q · x n mod = y ∈ G holds.
This protocol is complete by inspection.
Wesolowski shows that it is sound if the group G satisfies the adaptive root assumption, roughly, it is infeasible for an adversary to find a random root of an element of G chosen by the adversary.
The RSA quotient group Z × N /{±1} is conjectured to satisfy this assumption when P cannot factor N [23].
Division-intractable hashing.
Recall that a hash function Pocklington primality certificates.
Let p be a prime, and r < p and a be positive integers.
Define p = p · r + 1.
Pocklington's criterion [34] states that if a p·r ≡ 1 mod p and gcd(a r − 1, p ) = 1, then p is prime.
In this case, we say that (p, r, a) is a Pocklington witness for p .
H : X → D is collision resistant if it is infeasible for a PPT adversary to find distinct x 0 , x 1 such that H(x 0 ) = H(x 1 ).
In- formallyPocklington's criterion is useful for constructing primality certificates.
For a prime p n , this certificate comprisesp 0 , {(r i , a i )} 0<i≤nwhere p i = p i−1 · r i + 1.
To check this certificate, first verify the primality of the small prime p 0 (e.g., using a deterministic primality test), then verify the Pocklington witness(p i−1 , r i , a i ) for p i , 0 < i ≤ n.If each r i is nearly as large as p i , the bit lengths double at each step, meaning that the total verification cost is dominated by the cost of the final step.
A cryptographic accumulator [17] commits to a collection of values (e.g., a vector, set, or multiset) as a succinct digest.
This digest is binding, meaning informally that it is computationally infeasible to equivocate about the collection represented by the digest.
In addition, accumulators admit succinct proofs of membership and, in some cases, non-membership.
Merkle trees.
The best-known vector accumulator is the Merkle tree [90].
To review, this is a binary tree that stores a vector in the labels of its leaves; the label associated with an internal node of this tree is the result of applying a collisionresistant hash H to the concatenation of the children's labels; and the digest representing the collection is the label of the root node.
A membership proof for the leaf at index i is a path through the tree, i.e., the labels of the siblings of all nodes between the purported leaf and the root.
Verifying the proof requires computing the node labels along the path and comparing the final value to the digest (the bits of i indicate whether each node is the right or left child of its parent).
Updating a leaf's label is closely related: given a membership proof for the old value, the new digest is computed by swapping the old leaf for the new one, then computing the hashes along the path.
Merkle trees do not support succinct non-membership proofs.The cost of verifying k membership proofs for a vector comprising 2 m values is k · m evaluations of H.
The cost of k leaf updates is 2 · k · m evaluations.
Membership proofs and updates cannot be batched for savings.RSA accumulators.
The RSA multiset accumulator [40,81] represents a multiset S with the digestS = g ∏ s∈S H(s) ∈ G,where g is a fixed member of an RSA quotient group G and H is a division-intractable hash function ( §2).
Inserting a new element s into S thus requires computing S H(s) .
To prove membership of s ∈ S, the prover furnishes the value π = S 1/H(s) , i.e., a H(s)'th root of S.
This proof is verified by checking that π H(s) = S.Non-membership proofs are also possible [81], leveraging the fact that s ∈ S if and only if gcd(H(s ), ∏ s∈S H(s)) = 1.
This means that the Bézout coefficients a, b, i.e., integers satisfyinga · H(s ) + b · ∏ s∈S H(s) = 1are a non-membership witness, since the above implies thatS b · (g a ) H(s ) = gBecause a is large and b is small, the proof (g a , b) is succinct.Insertions, membership proofs, and non-membership proofs can all be batched [24] via Wesolowski proofs ( §2).
For example, since S {s i } = S ∏ i s i , computing an updated digest directly requires an exponentiation by ∏ i s i .
In contrast, checking the corresponding proof only requires computing and then exponentiating by ∏ i s i mod , for a prime of less than 200 bits.
This means that the exponentiation (but not the multiplication) to verify a batch proof has constant size.
Several lines of built systems [13, 15, 16, 21, 32, 47, 49, 61, 79, 96, 106-108, 112, 113, 124] enable the following highlevel model.
2 A verifier V asks a prover P to convince it that y = Ψ(x), where Ψ is a program taking input x and returning output y. To do so, P produces a short certificate that the claimed output is correct.
Completeness holds with ε c = 0; soundness holds as long as P is computationally bounded, with ε s negligible in a security parameter ( §2).
Roughly speaking, these systems comprise two parts.
In the front-end, V compiles Ψ into a system of equations C (X,Y, Z), where X, Y , and Z are (vectors of) formal variables.V constructs C such that z satisfying C (X = x,Y = y, Z = z)exists (that is, the formal variable X is bound to the value x, and so on) if and only if y = Ψ(x).
The back-end comprises cryptographic and complexity-theoretic machinery by which P convinces V that a witness z exists for X = x and Y = y.This paper focuses on compilation in the front-end.
We target back-ends derived from GGPR [64] via Pinocchio [96] (including [15,16,70]), which we briefly describe below.Our work is also compatible with other back-ends, e.g., Zaatar [106], Ligero [2], Bulletproofs [36], Sonic [85], and Aurora [14].
3 GGPR, Pinocchio and their derivatives instantiate zeroknowledge Succinct Non-interactive ARguments of Knowledge with preprocessing (zkSNARKs), which are argument protocols satisfying completeness, knowledge soundness, and zero knowledge ( §2), 4 where knowledge soundness and zero knowledge apply to the assignment to Z.
In addition, these protocols satisfy succinctness: informally, proof length and verification time are both sublinear in |C | (here, proofs are of constant size, while V 's work is O(|X| + |Y |)).
These protocols include a preprocessing phase, in which V (or someone that V trusts) processes C to produce a structured reference string (SRS), which is used by P to prove and V to verify.The cost of the preprocessing phase and the length of the SRS are O(|C |).
The cost of the proving phase is O(|C | log |C |) in time and O(|C |) in space (i.e., prover RAM).
The system of equations C (X,Y, Z) is a rank-1 constraint system (R1CS) over a large finite field F p .
An R1CS is defined by three matrices, A, B,C ∈ F |C |×(1+|X|+|Y |+|Z|) p .
Its satisfiability is defined as follows: for W the column vector of formalvariables [1, X,Y, Z] , C (X,Y, Z) is the system of |C | equa- tions (A ·W ) • (B ·W ) = C ·W , where• denotes the Hadamard (element-wise) product.
In other words, an R1CS C is a conjunction of |C | constraints in |X| + |Y | + |Z| variables, where each constraint has the form "linear combination times linear combination equals linear combination.
"These facts outline a computational setting whose costs differ significantly from those of CPUs.
On a CPU, bit operations are cheap and word-level arithmetic is slightly more costly.
In an R1CS, addition is free, word-level multiplication has unit cost, and bitwise manipulation and many inequality operations are expensive; details are given below.Compiling programs to constraints.
A large body of prior work [13, 16, 32, 79, 96, 106-108, 115, 116] deals with efficiently compiling from programming languages to constraints.An important technique for non-arithmetic operations is the use of advice, variables in Z whose values are provided by the prover.
For example, consider the program fragment x !
= 0, which cannot be concisely expressed in terms of rank-1 constraints.
Since constraints are defined over F p , this assertion might be rewritten as X p−1 = 1, which is true just when X = 0 by Fermat's little theorem.
But this is costly: it requires O(log p) multiplications.
A less expensive way to express this constraint is Z · X = 1; the satisfying assignment to Z is X −1 ∈ F p .
Since every element of F p other than 0 has a multiplicative inverse, this is satisfiable just when X = 0.
Comparisons, modular reductions, and bitwise operations make heavy use of advice from P .
For example, the program fragment y = x1 & x2, where x1 and x2 have bit width b and & is bitwise AND, is represented by the following constraints:Z 1,0 + 2 · Z 1,1 + . . . + 2 b−1 · Z 1,b−1 = X 1 Z 2,0 + 2 · Z 2,1 + . . . + 2 b−1 · Z 2,b−1 = X 2 Z 3,0 + 2 · Z 3,1 + . . . + 2 b−1 · Z 3,b−1 = Y Z 1,0 · (1 − Z 1,0 ) = 0 . . . Z 1,b−1 · (1 − Z 1,b−1 ) = 0 Z 2,0 · (1 − Z 2,0 ) = 0 . . . Z 2,b−1 · (1 − Z 2,b−1 ) = 0 Z 1,0 · Z 2,0 = Z 3,0 . . . Z 1,b−1 · Z 2,b−1 = Z 3,b−1Here, the variables Z 1,0 . . . Z 1 · X 1 = Z 2 (1) Z 3 · (Z 2 − 1) = 0 (2) (1 − Z 3 ) · X 1 = 0 (3) (1 − Z 3 ) · (Y − X 2 − 1) = 0 (4) Z 3 · (Y − 3 · X 2 ) = 0 (5)This works as follows: if X 1 = 0, Z 2 = 0 by (1), so Z 3 = 0 by (2) and Y = X 2 + 1 by (4).
Otherwise, Z 3 = 1 by (3), so = b /b l limbs { ˆ a i }, where a = ∑ η−1 i=0âi=0ˆi=0â i · 2 b l ·i .
For correctness, the compiler must track the maximum value of each number and ensure that C contains constraints that encode a sufficient number of limbs.Multiprecision operations rely heavily on advice from P .
At a high level, P supplies the result of a multiplication or addition, and the compiler emits constraints to check that result.
Subtractions and divisions are checked by verifying the inverse addition or multiplication, respectively.
xJsnark describes a range of optimizations that reduce the required number of constraints.
We leave details to [79], because they are not necessary to understand our further optimizations ( §4.3).
Programs that make use of RAM-in particular, programs whose memory accesses depend on the input, and thus cannot be statically analyzed-present a challenge for compiling to constraints.
Prior work demonstrates three solutions.
We now describe each, and compare costs and functionality below.Linear scan.
The most direct approach to emulating RAM in constraints is to perform a linear scan [79,96].
Concretely, Y = LOAD(Z) compiles to a loop that scans through an array, comparing the loop index to Z and, if they match, setting Y to the corresponding value.
(STORE is analogous.)
The Pantry approach.
In Pantry [32], the authors borrow a technique from the memory-checking literature [20] based on Merkle trees [90] (see also §2.1).
In particular, Pantry stores the contents of RAM in the leaves of a Merkle tree whose root serves as ground truth for the state of memory.For a LOAD, P furnishes advice comprising a purported value from memory, plus a Merkle path authenticating that value.
The corresponding constraints encode verification of the Merkle path, i.e., a sequence of hash function invocations and an equality check against the Merkle root.
For a STORE, P furnishes, and the constraints verify, the same values as for a LOAD.
In addition, the constraints encode a second sequence of hash function invocations that compute a new Merkle root corresponding to the updated memory state.The BCGT approach.
Ben-Sasson et al. [12] introduce, and other work [13,16,79,116] refines, an approach building on the observation [3] that one can check a sequence of RAM operations using an address-ordered transcript, i.e., the sequence of RAM operations sorted by address accessed, breaking ties by execution order.
In such a transcript, each LOAD is preceded either by the corresponding STORE or by another LOAD from the same address; correctness of RAM dictates that this LOAD should return the same value as the preceding operation.
(A LOAD from an address to which no value was previously stored returns a default value, say, 0.)
Leveraging this observation, correctness of memory operations is compiled to constraints as follows.
First, every access to memory appends a tuple (IDX i , OP i , ADDR i , DATA i ) to an execution-ordered transcript; here, IDX i = i is the index of the memory operation and OP i is either LOAD or STORE.
Then P furnishes a purported address-ordered transcript T , and the constraints check its correctness by ensuring that (1) transcript T is a permutation of the execution-ordered transcript, (2) each sequential pair of entries in transcript T is indeed correctly ordered, and (3) each sequential pair of entries in transcript T is coherent, i.e., each LOAD returns the value of the previous STORE (or the default if no such STORE exists).
Check (1) is implemented with a routing network [18,118].
Costs and functionality.
Roughly speaking, for tiny memories linear scan is cheapest; otherwise, BCGT-style RAM is.
5 In more detail, assume a memory of size 2 m , accessed k times.
For a linear scan, each RAM operation costs O(2 m ) constraints.
(i.e., 2 m copies of constraints encoding conditional assignment).
For Pantry, each LOAD entails m copies of constraints encoding a collision-resistant hash function and each STORE entails 2m such copies, where such hash functions entail a few hundred to a few thousand constraints ( §6; [15,32,79]).
For BCGT, each RAM operation costs O(log k) constraints for the routing network, O(m) constraints for address comparison, and O(1) constraints for coherence checking, all with good constants [116, Fig. 5].
Although Pantry-style RAM is costly, it offers functionality that the other two do not: the ability to pass the full state of a large RAM from one computation to another.
Pantry accomplishes this by including in X the Merkle root corresponding to the initial RAM state; this has constant size (usually one element of F p ).
In contrast, BCGT and linear scan would both require 2 m values in X for a 2 m -sized RAM; as discussed above, this would incur 2 m cost for V in verification.
(Prior work [15,16] uses this approach to partially initialize RAM.)
In this section, we define a new primitive, which we call MultiSwap, that exposes a sequential update semantics for RSA accumulators ( §2.1).
MultiSwap takes an accumulator and a list of pairs of elements, removing the first element from each pair and inserting the second.
The key property of this primitive is that it is defined in terms of batched insertions and removals.
In Section 4, we show how these batched operations are efficiently implemented as a system of constraints ( §2.2).
In more detail, let S and S be multisets, and let (x 1 , y 1 ), . . . , (x n , y n ) be a sequence of operations, called swaps, that replaces each x i by y i in order: (x 1 , y 1 ) applied to S produces some new set S 1 = S {x 1 } {y 1 }; then (x 2 , y 2 ) applied to S 1 produces S 2 = S 1 {x 2 } {y 2 }, etc.
Our goal is to verify that when the above sequence is applied to S, the result is S = S n .
Recall from Section 2.1 that RSA accumulators admit efficient batched insertions (deletions are analogous; §4).
Our question is: how can we use this un-ordered primitive to implement one with ordered semantics?Consider the following naïve solution: first verify the deletions, then verify the insertions.
In other words, verify that there exists some S mid such that S {x i } = S mid and S mid {y i } = S .
The problem with this approach is that it does not permit certain valid sequences, i.e., those in which a later swap deletes an item inserted by an earlier swap.
(To see why, notice that S mid only exists if all x i ∈ S.)Instead, our solution first verifies all the insertions, and then verifies all the deletions, irrespective of the order in which the operations are listed.
In other words, it verifies the predicate∃S mid : S {y i } = S mid ∧ S mid {x i } = S (6) (Note that S mid {x i } = S is equivalent to S {x i } = S mid .)
Intuitively, Equation (6) holds just when each element of an unordered multiset of swaps {(x i , y i )} can be applied to S in some order to produce S .
As we discuss below, this multiset may include cycles, subsets of swaps that have no net effect.We now give a precise semantics for MultiSwap.
Let MultiSwap(S, σ, S ) denote the predicate that holds just when Equation (6) is satisfied.
Let σ denote an unordered multiset of swaps {(x i , y i )}.
A swap (x i , y i ) is valid for S if x i ∈ S .
We say that σ is sequentially consistent with respect to S if there exists some ordering on σ such that all swaps are valid when applied in that order starting from S. Furthermore, we say that σ produces S from S if S is the product of such an application order to S, and we say that σ c is a cycle if it comprises {(c 0 , c 1 ), (c 1 , c 2 ), . . . , (c n , c 0 )}.
Lemma 1.
MultiSwap(S, σ, S ) holds if and only if there exist any number of cycles σ c i and cycle-free σ ⊆ σ such that σ = σ i σ c i , σ is sequentially consistent with respect to S, and σ produces S from S. MultiSwap to problems that need sequential semantics for batched verifiable state updates.
In the previous section we described how the MultiSwap primitive is built from batched insertions and removals.
In this section we describe these batched operations, the primitives that they are built on, and how those primitives are implemented as a set of constraints C ( §2.2).
Recall ( §2.1) that RSA accumulators support batched insertions through an interactive protocol whose final check isQ · S ∏ i H ∆ (y i ) mod = S {y i }(7)where · denotes a digest; S, the initial multiset; , a random prime challenge; {y i }, the inserted elements; H ∆ , a divisionintractable hash function; and Q, a witness from P .
Removing elements {x i } is similar, except that S {x i } is regarded as the initial multiset and S the final one.
6 To instantiate this interactive protocol in constraints, we apply the Fiat-Shamir heuristic [55], i.e., C computes the challenge by hashing all of the inputs to the protocol.
7 Figure 1 illustrates the insertion proof's verification procedure.
MultiSwap requires two proofs (one for insertion and one for removal); for this purpose, we hash all inputs to both proofs to derive a common challenge, as is standard [50].1 . . . . . . . . . . . .
H ∆ × mod y 1 H ∆ × mod y 2 H ∆ × mod y k exp G exp G × G H p S SIn the rest of this section we explain how to efficiently implement the blocks of Figure 1 in constraints.
In particular, we explain how to implement H p , the prime hash function used to compute ( §4.1) and H ∆ , the division-intractable hash function used to hash each element ( §4.2).
We also describe optimizations for multiprecision operations ( §4.3).
Finally, we discuss P 's cost for generating the witness input Z to C ( §2.2), notably, the digests S {y i } and S {x i } and the corresponding witnesses Q for insertion and removal ( §4.4).
The hash function H p (Fig. 1) generates the challenge used in the Wesolowski proofs of batch insertion and removal.These proofs are sound when P has negligible probability of guessing the factors of before evaluating H p [120].
In the non-interactive setting, one way to ensure this is by choosing at random from the first 2 2λ primes (Fn.
1, §2).
In our context, however, a more efficient approach is for H p to output slightly larger primes that are guaranteed by construction to have 2λ bits of entropy.
8 Soundness is identical.In standard settings (i.e., outside of constraints), a typical approach ( §8) for hashing to a random prime is rejection sampling.
Here, the input is fed to a collision-resistant hash whose output seeds a pseudorandom generator (PRG), then the PRG's outputs are tested in sequence until a prime is found.
Verifying correct execution requires, at the very least, testing primality of the purported output.
This is typically done with a probabilistic primality test like Miller-Rabin [98].
Such tests, however, generally require many iterations for soundness, where each iteration involves an exponentiation modulo the prime being tested.
This would be far too costly if implemented directly in constraints.Instead, we take advantage of advice from P ( §2.2).
At a high level, P helps to recursively construct a Pocklington certificate ( §2) for H p 's output, where each intermediate prime p i is the result of hashing H p 's input.
(This is related to prior approaches; see §8.)
This strategy is economical when implemented in constraints, because it uses very little pseudorandomness and requires only one exponentiation modulo the resulting prime, plus a few smaller exponentiations.We now describe the recursive step used to construct p i from p i−1 .
Further below, we describe the base case and give implementation details.
Recall ( §2) that a Pocklington witness for p i comprises (p i−1 , r i , a i ) such that p i = p i−1 · r i + 1.
(If p i is prime, some a i must exist.)
Notice that, given p i−1 , one can find p i by testing candidate r i values until p i−1 · r i + 1 is prime.
To implement this in constraints, we let r i = 2 b n i · h i + n i , where n i is a b n i -bit number provided by P as advice and h i is a b h i -bit pseudorandom number (we discuss its generation below).
P furnishes a corresponding a i and C includes constraints that compute p i and r i , and check the witness.The base case is p 0 = 2 b n 0 · h 0 + n 0 , for h 0 a pseudorandom number and n 0 supplied by P .
We fix b n 0 + b h 0 = 32, i.e., p 0 < 2 32 , and the constraints test primality of p 0 using a deterministic 3-round Miller-Rabin test that works for all values up to 2 32 [73].
This test requires 3 exponentiations modulo p 0 with exponents less than 32 bits; these are inexpensive.We choose bit widths b n i such that a valid n i exists with overwhelming probability, then choose b h i subject to the constraint that b h i + b n i < log p i−1 , which ensures that r i < p i−1 as required ( §2).
rounds suffice for 256 bits of entropy using the parameters listed in Figure 2.
C generates h i by hashing the input to H p with a hash function H modeled as a random oracle.
Each iteration yields a prime approximately twice as wide as the prior iteration's; meanwhile, the cost of each iteration is dominated by an exponentiation.
This means that our approach has cost less that that of two exponentiations modulo the final prime.
In contrast, using Miller-Rabin to check a 264-bit prime (which has roughly 256 bits of entropy) would require 80 exponentiations modulo that prime to give ≈2 −80 probability of outputting a composite (because Miller-Rabin is a probabilistic primality test).
Our approach thus saves more than an order of magnitude and provably outputs a prime.One final optimization is to force the most significant bit of each h i to 1; this establishes a lower bound on each p i and on (which is the final p i ).
As we discuss in Section 4.3, having this lower bound reduces the cost of modular reductions.
The tradeoff is a small loss in entropy, namely, 1 bit per iteration.
Even so, four rounds suffice to produce a 322-bit prime 9 with 256 bits of entropy.
Coron and Naccache show [48] that a hash function H that outputs sufficiently large integers is division intractable when modeled as a random oracle.
Informally, this is because in any randomly-selected set of large (say, 2000 bit) numbers, each element has a distinct, moderately sized (say, 200 bit) prime factor with high probability.Security of this hash function rests on the fact that the density of integers in the interval [0, α) with factors all less than µ approaches β −β+o(1) as α → ∞, where β = log α log µ .
We conjecture that this density also holds for a large interval around α, namely, α, α + α 1/8 .
(This is closely related to a conjecture on which the elliptic curve factorization method relies; there, the interval isα − √ α, α + √ α [71].)
Our hash function is defined as follows: let ∆ be a public 2048-bit integer chosen at random, and let H be a hash function with codomain 0, 2 256 with 128-bit collision resistance.
Then H ∆ (x) = H(x) + ∆.
Security of this construction follows from the analysis of [48] in the random oracle model, 9 Even though the prime comprises only 322 bits, C represents it with 352 ( Fig. 3), which is the next multiple of the limb width b l (32 bits; §2.2).
assuming the conjecture stated above.
Concretely, we conjecture that an adversary making q queries to H ∆ has probability roughly q · 2 −128 of breaking division intractability.H ∆ 's advantage over prior work is that its implementation in constraints is much smaller.
The system parameter ∆ is baked into the constraints, and the only dynamic values to compute are the base hash H(x) and the sum H(x) + ∆; using known techniques [79], this sum is inexpensive.
Moreover, since all hashes must be reduced modulo the challenge (Eq.
(7)) and H ∆ (x) mod = (H(x) + (∆ mod )) mod , the (costly) reduction ∆ mod can be checked once in the constraints and the result can be re-used for each H ∆ (x).
We note that while this approach gives smaller C than hashing to primes (because H ∆ and modular reductions are cheaper), it increases P 's work (because H ∆ 's bit length is longer; §4.4).
We describe two optimizations for multiprecision arithmetic in constraints, building on ideas described in Section 2.2.
Computing greatest common divisor.
We observe that addition and multiplication checks can be leveraged to verify a statement gcd(x, y) = d by checking three equations over Z:∃a, b a · x + b · y = d (8) ∃x x · d = x ∃y y · d = yIn constraints, the existential variables above correspond to advice provided by P .
Verifying coprimality (gcd(x, y) = 1) reduces to condition (8), i.e., materializing the multiplicative inverse of x modulo y.
We use this simplification in Section 4.1 to verify a Pocklington witness ( §2).
Optimizing division and modular reduction.
Prior work implements division and modular reduction for a dividend x and divisor d by having the prover provide, as advice, the quotient q and remainder r < d such that x = q·d +r; this equality is then checked with multiprecision arithmetic ( §2.2).
For correctness, C must enforce upper bounds on the bit widths of q and r via bit splitting ( §2.2), which requires as many constraints as the sum of the bit widths of q and r.Since r can range from 0 to d − 1, its width is just that of d.
The width of q, however, is slightly more subtle.
Since q's value is x /d, a conservative choice is to assume q is as wide as x.
But this choice is imprecise: q is only as wide as log 2 ( x max/d min ), where x max denotes x's maximum possible value, and d min denotes d's minimum possible value.
(Intuitively, this is because q is small when d is large.)
As in prior work [79], our system uses a dataflow analysis to track the maximum value of each number, in order to determine the required representation size.
To bound q's width more tightly using the above expression, we augment this dataflow analysis to also track minimum values.
The prior sections have treated P as an advice oracle.
We now discuss P 's cost in computing this advice.
Prior work [116,121] shows that P 's (single-threaded) cost per constraint is ≈100 µs or more (this includes, e.g., an elliptic curve point multiplication per constraint [16,64,70,96]).
Computing most advice values-including for multiprecision operations and prime hashing-is negligible by comparison.
Possible exceptions are the witnesses for Wesolowski proofs ( §2) used by batch insertion and removal operations ( §2.1).
(Recall that one of each operation is required for a MultiSwap; §3.)
The witness for a batch insertion S {y i } = S ∏ i H ∆ (y i ) is the value S (∏ i H ∆ (y i ))// .
This exponent has length ≈2048 · k bits for k elements inserted.
In microbenchmarks, GMP [66] computes a 2048-bit exponentiation modulo a 2048-bit N in ≈2.5 milliseconds (i.e., roughly 25× P 's per-constraint proving cost), so computing this value costs roughly the same as 25 · k constraints, which is inconsequential ( §5, Fig. 3).
Batch removal is much more expensive.
To prove that removing the elements {x i } from the multiset S yields a new multiset S , P must prove that S = S∏ i H ∆ (x i ) , where S = S {x i } = g ∏ s∈S{x i } H ∆ (s)(9)No known method for computing S is faster than directly evaluating this expression because the order of G is unknown (recall that this computation is in G = Z × N /{±1} where N has unknown factorization; §2).
Meanwhile, this exponent has bit length ≈2048 · M, for M the total size of the multiset S , i.e., it costs roughly the same as 25 · M constraints.
(As discussed in the prior paragraph, given S it is inexpensive to compute the witness for batch removal, namely, S (∏ i H ∆ (x i ))// ).
Even for large accumulators, this cost may be reasonable: as we show in Section 7, MultiSwap can easily save tens of millions of constraints compared to Merkle trees.
On the other hand, proof generation can be parallelized [121], whereas at first glance the exponentiation in (9) appears to be strictly serial [22,101].
We observe, however, that since g is fixed, a pre-computation phase can be used to sidestep this issue [33].
Specifically, for some upper bound 2 m on the maximum size of the accumulator, the above exponent is at most 2 2048·2 m , so pre-computing the values g i = g 2 i·2 m , 0 ≤ i < 2048 (via successive squaring) turns the above exponentiation into a 2048-way multi-exponentiation [91] (which can be computed in parallel): for each g i , the exponent is a 2 m -bit chunk of the value ∏ s∈S{x i } H ∆ (s).
Further parallelism is possible simply by computing more g i with closer spacing.This precomputation also enables a time-space tradeoff, via windowed multi-exponentiation [91,110].
In brief, when computing a multi-exponentiation over many bases, first split the bases into groups of size t and compute for each group a table of size 2 t .
This turns t multiplications into a table lookup and one multiplication, for a factor of t speedup.
t = 20 is rea-sonable, and reduces the cost of computing the exponentiation in (9) to roughly the equivalent of 1.25 · M constraints.The above pre-computation is a serial process that requires ≈2048 · 2 m squarings in G. Assuming that 2048 squarings takes ≈2.5 milliseconds (i.e., the same amount of time as a general 2048-bit exponentiation; this is pessimistic), this precomputation takes ≈2 m · 2.5 milliseconds.
For m = 20, this is ≈45 minutes; for m = 25, it is ≈1 day.
Note, however, that this pre-computation is entirely untrusted, so it can be done once by anyone and reused indefinitely for the same g. Finally, the above precomputation requires materializing∏ s∈S{x i } H ∆ (s), which is 2 31 bits when M = 2 20 .
This product can be expressed as a highly parallel computation; the final step is a multiplication of two, 2 30 -bit values, which can itself be parallelized via a Karatsuba-like approach.We evaluate P 's witness generation costs in Section 7.1.
In this section we discuss two applications of MultiSwap and compare constraint costs for these applications when implemented using Merkle swaps and MultiSwaps.MultiSwap Costs.
The first two rows of Figure 3 model the costs of Merkle swaps and swaps computed via MultiSwap.A Merkle swap requires hashing the old and new values and Merkle path verifications for each ( §2.1), so the number of hash invocations is logarithmic in the number of leaves.For a MultiSwap, each swap requires a H ∆ invocation ( §4.2), which comprises an invocation of the underlying hash H and multiprecision arithmetic to compute the result and multiply it mod ( §4, Fig. 1).
In addition, each swap is an input to H p , which requires another hash invocation.
All of these costs are independent of the number of elements in the accumulator.
MultiSwap also costs a large constant overhead, however; this is to generate ( §4.1) and check two Wesolowski proofs via modular exponentiations ( §2, §4).
Blockchain systems [26] like Ethereum [53] enable smart contracts: computations defined by a blockchain's users and executed as part of the block validation procedure.
One application of smart contracts is implementing a form of verifiable state update ( §1): for global state Γ (stored on the blockchain) and a transaction γ (submitted by a user), the computation (1) checks that γ is valid according to some predicate, and if so (2) updates the global state to a new value Γ .
Consider, for example, a distributed payment system where Γ comprises a list of users and their public keys and balances.
Transactions let users send payments to one another.
When Alice wishes to send a payment, she constructs a transaction γ that includes (1) the target user; (2) the amount to send; and (3) a digital signature over the prior two items; she submits this to the smart contract, which verifies it and updates Γ.A major practical limitation of this approach is that computation, storage, and network traffic are extremely expensive for smart contracts.
10 One solution to this issue, Rollup [7,65,94], is an instance of verifiable computation ( §2.2): the smart contract delegates the work of checking transactions to an untrusted aggregator, and then checks a proof that this work was done correctly.
11 To effect this, users submit transactions γ i to the aggregator rather than directly to the smart contract.
The aggregator assembles these transactions into a batch {γ i }, then generates a proof π certifying the correct execution of a computation Ψ that verifies the batch and updates the global state from Γ to Γ .
Finally, the aggregator submits π and Γ to the smart contract, which verifies the proof and stores the updated state.
Checking this proof is substantially cheaper for the smart contract than verifying each transaction individually, and the exorbitant cost of smart contract execution justifies the aggregator's cost in generating the proof [115].
In more detail, the constraints C corresponding to Ψ ( §2.2) take the current state Γ as the input X and the updated state Γ as the output Y .
P (i.e., the aggregator) supplies the batch {γ i } as part of the witness (i.e., the advice vector Z), meaning that the smart contract can verify the proof without reading {γ i }.
This saves both computation and network traffic.Notably, though, even reading Γ and Γ is too expensive for the smart contract, as is storing Γ on the blockchain.
(Recall that verifying a proof requires work proportional to the size of the inputs and outputs; §2.2.)
The original Rollup design [7] addresses this by storing Γ in a Merkle tree ( §2.1).
The inputs and outputs of C are just Merkle roots, and only this root is stored on the blockchain.
Each leaf of this tree contains a tuple (pk, bal, #tx) comprising a user's public key, their balance, and a transaction count (which prevents replaying past transactions).
The constraints that verify a transaction in C thus require two Merkle tree updates, one each for payer and payee.
(Each update comprises two Merkle paths; §2.1).
We observe that a single MultiSwap ( §3) can replace all of the Merkle tree updates for a batch of transactions.
In particular, MultiSwap's semantics guarantee sequential consistency of the transactions with respect to Γ and Γ .
And whereas the per-swap cost of Merkle swaps increase logarithmically with the number of accounts stored in Γ, the per-swap cost of MultiSwap is essentially independent of the number of users.
This means that for large batches of transactions and/or large numbers of users, a MultiSwap-based Rollup requires far fewer constraints than a Merkle-based one.Costs.
The middle two rows of Figure 3 show costs for Rollup using Merkle and MultiSwap.
Both cases pay to ver- Per-Operation Costs Per-Proof Costs The approximate value of each parameter in our implementation ( §6, §7) is given in parentheses.
See Section 5 for discussion.Merkle swap 2(c H e + m · c H ) MultiSwap ( §3, §4) 2(c H e + c H in + c split + c + ( f ) + c × ) 4c e G (||) + 2c × G + c H p + c mod (b H ∆ )ify the payer's signature and ensure that the payer's balance is sufficient.
The difference is in the swap costs, which are discussed above ( §5); Rollup requires two swaps per transaction, one each to update the payer's and payee's accounts.
Recall from Section 2.2 that Pantry-style RAM, while expensive, offers unique functionality: the ability to pass the full state of RAM from one proof to another.
This enables computations over persistent state [32], recursively verifiable state machine execution [15,89], and other useful applications.
Unfortunately, the high cost (in constraints) of hash functions ( §6) limits the number of Pantry-style RAM operations that can be used in a computation-especially for large RAMs [32,79,116].
In this section, we show how to use the batched RSA accumulator construction of Section 4 to address this issue.
Our design yields a persistent RAM abstraction whose per-access constraint cost is lower than Pantry's even at modest RAM sizes, and is nearly insensitive to RAM size.To begin, notice that Pantry's RAM abstraction essentially stores memory values in a fixed-size Merkle tree, executing a membership proof for each LOAD and a swap for each STORE.
Moreover, since our goal is efficiency, our design will ideally check all memory operations using a small number of batched accumulator operations ( §4).
This seems to suggest the following (incorrect) approach.First, replace the Merkle tree with an RSA accumulator, representing memory locations as addr, data tuples.
Then, verify all LOAD and STORE operations in a batch using MultiSwap ( §3) as follows.
For each LOAD from address δ, P supplies as advice the value ν purportedly stored at δ, and the constraints encode a swap that replaces the tuple δ, ν with itself.
For each STORE of the value ν to address δ, P supplies as advice the value ν purportedly being overwritten, and the constraints encode the swap (δ, ν, δ, ν ).
The reason this approach is incorrect is that it does not enforce the consistency of LOAD operations with program execution.
In particular, recall ( §3) that MultiSwap(S, σ, S ) only guarantees that S is produced by a sequentially-consistent cycle-free subsequence σ ⊆ σ.
Since LOAD operations are self-cycles, they are not included in σ .
This use of MultiSwap thus only guarantees that σ correctly encodes STORE operations-LOADs can return any value.We might attempt to fix this issue by checking LOAD operations using membership proofs.
But this is inefficient: checking such a proof requires the constraints to materialize an accumulator that contains the value being loaded; meanwhile, the LOAD might correspond to a prior STORE, in which case the accumulator against which the proof must be checked would first have to be computed.
In other words, this strategy makes batching accumulator operations impossible.Our key insight is that a hybrid of the Pantry and BCGT approaches solves this issue.
At a high level, our design enforces the correctness of LOAD and STORE operations using an address-ordered transcript ( §2.2) while ensuring that this transcript is consistent with the initial and final state of RAM using batched accumulator operations.
As above, each memory location is stored in the accumulator as an addr, data tuple.
As in BCGT-style RAM, the constraints build an executionordered transcript, P supplies an address-ordered transcript T , and the constraints ensure that T is correctly ordered, coherent, and a permutation of the execution-ordered transcript.For the initial state of RAM, the constraints enforce consistency by ensuring that the first time an address δ is accessed in T , the tuple δ, ν is removed from the accumulator.
If the first access is a LOAD, ν is the corresponding DATA value from T .
Otherwise, P supplies as advice a claimed ν value such that δ, ν is in the accumulator.
(For now, we assume that memory location δ has some corresponding tuple in the accumulator; we discuss uninitialized memory below.)
Observe that this ensures consistency, because a removal is only possible if δ, ν is indeed in the accumulator.For the final state of RAM, the constraints enforce consistency by ensuring that the last time an address δ is accessed in T , the tuple δ, ν is inserted into the accumulator.
The value ν is the corresponding DATA value from T .
Together with the above, this ensures that all of the accesses to address δ collectively result in the swap (δ, ν, δ, ν ).
Constraints for the above checks work as follows.
First, for entry i in T , the constraints computeh i,del = H ∆ (ADDR i , ν)and h i,ins = H ∆ (ADDR i , ν ) ( §4.2).
Then, for each sequential pair of entries i, i + 1 in T , if ADDR i = ADDR i+1 , then entry i must be the last access to ADDR i and entry i + 1 must be the first access to ADDR i+1 .
Finally, the constraints compute ∏ i∈F h i,del mod and ∏ i∈L h i,ins mod ( §4), the values inserted into and removed from the accumulator, respectively, for F the first-accessor set and L the last-accessor set.Handling uninitialized memory.
A remaining issue is how to handle the case where memory is uninitialized.
Recall that in the BCGT approach, a LOAD not preceded by a STORE to the same address is serviced with a default value, say, 0.
That does not work here, because this approach relies crucially on swapping old values for new ones, to ensure consistency with both the initial and final accumulators.
A straightforward solution is to ensure that every memory location is initialized, by executing a setup phase that constructs an accumulator containing the tuple δ, 0 for every address δ.
The cost of constructing this accumulator is high when the address space is large, since it amounts to one exponentiation per entry in RAM.
Note, however, that this computation can be parallelized using the pre-computed values described in Section 4.4, and admits the same time-space tradeoff described in that section.
12 Costs.
The constraint costs of memory accesses are shown in the bottom two rows of Figure 3.
The Merkle-based RAM requires two proofs of membership for each STORE, but only only one for each LOAD [32], so it is slightly cheaper than a Merkle swap-but logarithmic in RAM size.The RSA accumulator-based RAM uses one MultiSwap for all LOADs and STOREs, with attendant per-operation costs (which are independent of RAM size; §5).
It also incurs extra per-operation costs to check T as described above; these are logarithmic in the number of accesses but concretely very inexpensive ( §2.2, [116, Fig. 5; 79, Appx.
B-A]).
We implement a library comprising multiprecision arithmetic, Pocklington prime certification, RSA accumulators, and Merkle trees.
This library extends Bellman [9], a library for building constraint systems and generating proofs using the pairing-based argument due to Groth [70].
Based on this library, we implement two end-to-end applications: one that verifies a sequence of swaps, and one that verifies a batch of transactions for a distributed payment system ( §5.1).
We also implement or adapt four hash functions: MiMC [1], which costs 731 constraints (91 rounds of the x 7 permutation); Poseidon [69], which costs 316 constraints; Pedersen [72,97], which costs 2753 constraints (based on the JubJub elliptic curve [28]), and SHA-256 [57], which costs 45567 constraints.
We adapt the latter three hashes from Sapling [104].
13 Finally, we implement custom Bellman constraint synthesizers (ConstraintSystems, in the jargon of Bellman) that allow us to quickly measure a constraint system's size and P 's cost computing a corresponding witness.We use a 2048-bit RSA quotient group ( §2) modulo the RSA-2048 challenge number [76,102], and choose a random 2048-bit ∆ to define the division-intractable hash function H ∆ ( §4.2); we give concrete values in Appendix B.
We synthesize all constraints over the BLS12-381 [27] curve.In total, our implementation comprises ≈11,300 lines of Rust.
We have released it under an open-source license [10].
We evaluate our MultiSwap implementation, comparing it to Merkle trees by answering the following questions:δ for which no value exists, P supplies a proof of non-membership ( §2.1) for valid [δ], plus a default value.
This obviates the setup phase, but requires additional constraints to (1) compute H ∆ (valid[ADDR i ]) for each entry in T , (2) check a batched non-membership proof, (3) check a batched insertion of valid [·] values (which can be combined with the swap check), and (4) enforce correctness of the default value.
Further exploration is future work.
13 The costs of MiMC, Poseidon, and JubJub depend on the underlying elliptic curve; we target BLS12-381 [27].
The cost of SHA-256 is ≈30% higher in Sapling than in prior work [1], but even the best reported costs are more than 10× the other hashes' costs.
This discrepancy does not change our results: we focus on Poseidon, which is the best case for Merkle trees ( §7.1).
(1) How does the cost of a MultiSwap compare to the cost of Merkle swaps for a batch of swaps?
In particular, what is the break-even point (i.e., the number of operations beyond which MultiSwap is cheaper), and how do costs compare for a fixed (large) constraint budget?
(2) What is the effect of hash function cost on the tradeoff between RSA accumulators and Merkle trees?We answer the first question by synthesizing constraint systems for both MultiSwap and Merkle swaps, at varying set and batch sizes ( §7.1).
We also synthesize constraints for the Rollup application ( §7.2) and compare the persistent RAM application using a cost model ( §7.3).
Our cost metric is number of constraints; to validate this metric, we measure end-to-end times for MultiSwap and Merkle swaps ( §7.1).
For the second question, we evaluate the break-even point for MultiSwap versus the cost of the underlying hash function, for four different hash functions ( §7.1).
In sum, we find that MultiSwap breaks even for batch sizes of at most several thousand operations; for large sets, this value is several hundred.
We also find that MultiSwap's advantage is greater when hashing is more expensive.Baseline.
Our baselines are constraint systems ( §2.2) that use Merkle trees ( §2.1) to store state.
For each baseline, we fix capacity to be M = 2 m , for a range of m values.
In all experiments except persistent RAM, the basic Merkle tree operation is a swap ( §5, Fig. 3).
Merkle-based RAMs use a mix of membership proofs and swaps ( §2.1, §2.2); we discuss further in Section 7.3.
Setup.
Except in the hash cost experiment ( §7.1), both Merkle and MultiSwap fix the hash function H ( §4.1, §4.2) as our Poseidon [69] implementation ( §6).
As we show in Section 7.1, this is the most favorable choice for the Merkle baseline, because Poseidon is inexpensive in constraints.For execution time ( §7.1), our testbed has two Intel Xeon E5-2687Wv4 CPUs (12 physical cores per socket, 48 threads total) and 128 GiB of RAM, and runs Ubuntu 18.04.
We compile with Rust 1.41-nightly (c9290dcee 2020-02-04) [103].
Method.
Our primary cost metric is number of constraints, which we measure with a custom Bellman synthesizer ( §6).
We use this metric because P 's costs (both time and space) are dominated by constraint count in the back-ends we target ( §2.2).
V 's costs are small and essentially constant.To validate this metric, in Section 7.1 we measure P 's and V 's time for MultiSwap and Merkle swaps, for 2 20 -element sets.
Limitations of the underlying Bellman and Sapling libraries ( §6) cause our MultiSwap and Merkle implementations to unnecessarily resynthesize all constraints when generating proofs.
To sidestep this, for each experiment we measure total proving time (synthesis, witness computation, and proof generation), separately measure just synthesis time, and report the difference.
Fixing this issue (by rewriting Bellman/Sapling) is future work.
Benchmark.
This experiment compares the costs of MultiSwap and Merkle trees for a computation comprising only swaps, varying the number of swaps and set size.Constraint costs.
Figure 4 shows the results.
The cost of Merkle trees varies with set size, because the number of hash invocations depends on this value ( §2.1; §5, Fig. 3).
In contrast, the constraint cost of MultiSwap is independent of the number of elements in the set; for moderately sized sets (≈2 10 elements), the per-swap cost is less than for Merkle trees.
On the other hand, MultiSwap pays a large overhead (≈11 million constraints) to evaluate H p and verify two Wesolowski proofs ( §4; §5, Fig. 3).
Thus, MultiSwap requires some minimum batch size before it breaks even.
For small sets (say, 2 5 elements) there is no break-even point; for sets with 2 10 or more elements, the break-even point is at most a few thousand swaps, and decreases with set size.
Figure 5a shows the number of swaps that fit in 10 9 constraints, for different accumulators.
(We compare at this size because it is close to the largest that prior work can handle [121].)
Depending on set size, MultiSwap improves reachable batch sizes by up to ≈3.3×.
Proving and verifying time.
Figure 6 shows proving times (witness computation plus proof generation) for MultiSwap and Merkle with sets having 2 20 elements, for varying batch sizes.
Verification costs ≈7 ms in all cases.
MultiSwap has longer proving times for small batches but shorter times for large batches, and the break-even point between 1200 and 1600 swaps.
This is slightly larger than in Figure 4 because of the added cost of computing the new accumulator digest ( §4.4).
For an accumulator with 2 20 elements, computing a new digest after batch removal takes ≈43 seconds and uses ≈4 GiB of RAM via the preprocessing approach described in Section 4.4.
For smaller accumulators this cost is correspondingly smaller.
Larger accumulators have slower witness generation, which affects break-even batch size; we discuss in Section 9.
Effect of hash cost.
Figure 7 shows the effect of hash cost on MultiSwap's break-even point for sets of 2 20 elements (other set sizes are analogous; note that the axes are logarithmic).
We measure MiMC, Poseidon, Pedersen/Jubjub, 14 and SHA-256 ( §6).
As expected, in all cases Merkle trees are cheaper for small numbers of operations.
For the least expensive hash (Poseidon), MultiSwap's break-even point is the highest; as hash cost increases, so does MultiSwap's advantage.
(We report results in all other experiments with Poseidon, which is the worst case for MultiSwap.)
Benchmark.
This experiment compares the costs of MultiSwap and Merkle trees for the Rollup application described in Section 5.1.
We measure cost versus the number of transactions (a signature verification, a validity check, and two swaps).
Signatures use the scheme from ZCash [72].
Results.
Figure 8 shows the results.
In contrast with the previous experiment, here all accumulator types pay a fixed overhead per transaction (this is dominated by signature verification), which reduces MultiSwap's per-transaction advantage.
In this application, set size corresponds to the number of accounts.
As in Section 7.1, MultiSwap does not break even for the smallest set size.
The break-even point for 2 10 accounts is ≈2000 transactions, and ≈600 for 2 20 accounts.
Figure 5b shows the number of transactions that fit in 10 9 constraints, for different accumulators.
MultiSwap's advantage is as large as ≈1.9×, depending on set size.
Benchmark.
This experiment compares the costs of MultiSwap-based and Pantry's [32] Merkle-based persistent RAM 5.2.
We compare using the cost model of Figure 3 ( §5), which is derived from prior work [79,116]; future work is to port Buffet's RAM compiler to Bellman and synthesize.
We report cost versus RAM size.Results.
Figure 9 shows the results.
For Merkle-based RAM, bands in the figure represent varying write loads, from 0 (lowest cost) to 100% (highest cost).
As in prior experiments, MultiSwap's cheaper per-operation cost yields a breakeven point of several thousand operations for a large RAM.
This model includes the cost of memory consistency checks ( §2.2, §5.2, Fig. 3); these cost fewer than 100 constraints per operation and are thus negligible.
Verifiable computation.
The literature on verifiable computation is both broad and deep; a somewhat recent survey [119] gives a thorough treatment of the area's beginnings.Our work builds most directly on xJsnark's [79] multiprecision arithmetic and on the RAM primitives first described by Ben-Sasson et al. [12] and further refined by Ben-Sasson et al. [13,16], in Buffet [116], and in xJsnark.
Buffet and xJsnark both extend lines of work concerned with efficiently compiling high-level programs to constraints, including Pepper [107], Ginger [108], Pinocchio [96], and Pantry [32].
Several other works in this area deal with persistent state.
Pantry [32] was the first to use Merkle trees for stateful computations, and its persistent RAM primitive inspired ours ( §5.2).
vSQL [123] builds a verifiable subset of SQL, building on the interactive proofs of Goldwasser et al. [67], Cormode et al. [47], and Thaler [111], and on the polynomial commitments of Papamanthou et al. [95], which build on the work of Kate et al. [77].
In contrast to the persistent RAM and multiset abstractions we develop, vSQL exposes a database abstraction; queries operate on all rows in parallel.ADSNARK [4] extends the Pinocchio [96] SNARK to support operations on authenticated data provided by a third party.
Geppetto [49] also extends Pinocchio, allowing the verifier to commit to inputs for a specific computation and later verify a proof against that commitment, and also enabling data transfer between separate constraint systems bundled into one proof.
Fiore et al. [56] take Geppetto's commitments to inputs a step further, making them computation independent.
In contrast to a multiset or persistent RAM abstraction, however, all of these systems require a number of constraints sufficient to read every input value-in other words, a multiset of size M implies at least M constraints.
Further, they do not efficiently support programs whose multiset or RAM accesses depend on inputs and thus cannot be statically analyzed ( §2.2).
Spice [105] aims to enable zero-knowledge auditing of concurrent services.
Spice's amortized cost per state operation is ≈2× lower than ours for large batches, but its approach differs from ours in two key ways.
First, Spice's core state verification primitive requires a number of constraints linear in the total size of the state; this cost is amortized over a batch of requests, each containing one or more state operations.
In contrast, MultiSwap operations ( §3) have constraint costs that depend only on the number of state updates, not on total state size.
Second, verification costs in Spice scale with the number of requests in a batch; in our work, verification cost is independent of batch size.
Piperine [80] optimizes Spice's state verification primitive and saves verification work by combining all requests from a batch into one proof; this yields verification cost independent of batch size.Accumulators.
Cryptographic accumulators [17] based on RSA have a long history [5,40,81,84].
The recent work of Boneh et al. [24] builds upon work by Wesolowski [120] to construct batched membership and non-membership proofs for these accumulators.
Our work builds directly on this line.Merkle-based accumulators have also seen extensive study [38,90], and related structures have seen applications, e.g., in the blockchain [99] and PKI contexts [100].
These works all rely crucially on collision-resistant hashing, which is expensive when expressed as constraints ( §6, §7).
Two other lines of work build accumulators [39,42,51,93] and vector commitments [41,82,83] from bilinear maps.
Elliptic curve operations and pairings appear to be very expensive when compiled to constraints [15], but these lines may nevertheless be an interesting direction for further study.Prime generation.
A long line of work [30,31,68,74,75] aims to efficiently generate pseudorandom prime numbers.
In some cases, uniformly distributed primes [59] are desirable.All of these proceed in "guess-and-check" fashion, which is inefficient when implemented in constraints (see §4.1).
Most closely, Maurer [87,88] and Shawe-Taylor [109] describe prime generation methods based on Pocklington certificates; Clavier et al. [46] optimize for embedded devices.
To our knowledge, no prior work tackles this problem in our context.
We have shown that in verifiable state applications with moderate to large state, accessed thousands of times, RSA accumulators are less costly than Merkle trees.There are two caveats: first, RSA accumulators require a trusted setup.
In practice, most SNARKs [15,64,70,96] also require a trusted setup, so this is not a significant burden.
Moreover, it is possible to mitigate trust requirements by generating an RSA modulus using a multiparty computation [25,60].
A conjectured alternative that avoids trusted setup is a class group of imaginary quadratic order [24,35]; exploring efficient constraint implementations is future work.Second, for very large sets (say, > 2 25 ) P 's cost (in time) for advice generation is high ( §4.4).
For small batch sizes, this cost overwhelms the time saved because of reduced constraint count.
Note, however, that there will be some batch size at which RSA breaks even, since per-swap cost is smaller than Merkle for 2 10 elements.
Moreover, reducing the number of constraints also reduces P 's RAM requirements; meanwhile, P 's advice generation task requires little memory.
This means that even if an RSA accumulator requires greater total proving time than a Merkle tree, the RSA accumulator's use may still be justified because it reduces the amount of RAM P needs to generate a proof.
Since RAM is a major bottleneck [116,121] ( §1), such a time-space tradeoff may have significant practical benefit.
Exploring this tradeoff is future work.
This work was supported in part by the NSF, the ONR, the Simons Foundation, the Stanford Center for Blockchain Research, and the Ripple Foundation.
The authors thank Justin Drake, Srinath Setty, and Justin Thaler for helpful comments.
yLet σ denote a multiset of swaps.
Let in σ denote {y : (x, y) ∈ σ} and let rm σ denote {x : (x, y) ∈ σ}.
Claim 1.
Let σ be a multiset of swaps and σ c be a cycle.
MultiSwap(S, σ σ c , S ) holds if and only if MultiSwap(S, σ, S ) does.Proof: We prove both directions simultaneously, by illustrating a bidirectional chain of mutually implicating equalities.
We start with the definition of MultiSwap(S, σ σ c , S ):Since σ c is a cycle, we have that in σ c = rm σ c , so rm σ c ⊆ S in σ c , and the removal of rm σ c can be moved earlierThis last line is exactly our goal: the statement that MultiSwap(S, σ, S ) holds.
Claim 2.
If σ contains no cycles and MultiSwap(S, σ, S ) holds, then σ is sequentially consistent with respect to S, producing S .
Proof: Let n be the number of swaps in σ.
For a set S and multiset of swaps τ, define the directed multigraph G S,τ as a multigraph where the vertices are the universe of multiset elements, the edges point from each removal to its corresponding insertion, and each vertex is labeled with a multiplicity equal to to the multiplicity of that vertex's element in S, minus the out-degree, plus the in-degree.
Observe that in G = G S,σ , the multiplicity of each vertex is equal to the multiplicity of that element in S .
Furthermore, by the predicate MultiSwap(S, σ, S ) and the soundness of the proofs of insertions and removal, all multiplicities in G are non-negative.
We now construct the sequentially valid ordering of σ.
Since σ has no swap cycles, G has no edge cycles.
Thus, the edges of G can be topologically sorted such that all edges to a vertex occur before any edge from that vertex.
We lift this edge order to a swap order, observing that in this swap order, all swaps inserting an element occur before all swaps removing it.It suffices to show that when σ is applied to S in this order, each swap is valid.
Let σ i denote the first i elements of σ in the aforementioned order.
Thus, G S,σ n is equal to G. Furthermore, the order ensures for all i > j and for all vertices v, the multiplicity of v in G S,σ i is at most the multiplicity of v in G S,σ j .
Suppose that the i th element of this order, (x i , y i ) were invalid, where i ≤ n.
This implies that the multiplicity of x i in G S,σ i is negative.
This would imply that the multiplicity of x i in G S,σ n = G were negative, a contradiction.
Thus no swap (x i , y i ) is invalid in this order.
Proof of Lemma 1.
The reverse direction follows immediately from the definition of MultiSwap.We prove the forward direction by (strong) induction on the size of σ.
Say that σ has no cycles.
Then the lemma follows from Claim 2.
Otherwise, let τ be a multiset of swaps and let σ c be a cycle such that σ = τ σ c .
By Claim 1, MultiSwap(S, τ, S ) holds.
Then, by the inductive hypothesis, τ can be decomposed into cycle-free τ and cycles τ c i such that τ = τ i τ c i and τ is sequentially consistent with respect to S, producing S .
By observing that τ ( i τ c i ) σ c is a decomposition of σ into a cycle-free swap multiset and cycles, we conclude this direction of the proof.
B Parameter ValuesOur RSA accumulators work in G = Z × N /{±1}, where N is the RSA-2048 challenge number [102] , N=0xc7970ceedcc3 b0754490201a7aa613cd73911081c790f5f1a8726f46355 0bb5b7ff0db8e1ea1189ec72f93d1650011bd721aeeacc2 acde32a04107f0648c2813a31f5b0b7765ff8b44b4b6ffc 93384b646eb09c7cf5e8592d40ea33c80039f35b4f14a04 b51f7bfd781be4d1673164ba8eb991c2c4d730bbbe35f59 2bdef524af7e8daefd26c66fc02c479af89d64d373f4427 09439de66ceb955f3ea37d5159f6135809f85334b5cb181 3addc80cd05609f10ac6a95ad65872c909525bdad32bc72 9592642920f24c61dc5b3c3b7923e56b16a4d9d373d8721 f24a3fc0f1b3131f55615172866bccc30f95054c824e733 a5eb6817f7bc16399d48c6361cc7e5.We randomly selected a 2048-bit offset ∆ for our divisionintractable hash H ∆ ( §4.2); we use the value ∆=0xf3709c40 772816d668926cae548ffea31f49034ab1b30fb84b595ca 6c126a6646a4341abea2f8b07bf8d366801ac293e5a286a bb43accdec39ac8f0bc599519cf1e532f9c70b5406c4b65 2ca7da4e1cb102b69953841ae20d4bcab055c5338487ba0 0fe95e821abd381b191dfb77bae3e022ccd818d4064882d 28481ffa2db45093a4deab05f6ebfbadcf11afe7369caea aaf1f02572348a17f0510b333b8a2d56e67d892f1e1182b 26301d9347ae0a900cff2a0979caddb1a86e04a6cbc9704 d6549e5b3aef0d5c3dc4aba648ed421b0ba37c3f8e8edc1 2ef42b86d8e5fbc0dbd903238ca2e9ed6873ccb68e8103b 5d01b4249bfbe8e70cb4f4983f41df8c8f.
Our evaluation ( §7) builds on the BLS12-381 elliptic curve [27], which is the Barreto-Lynn-Scott curve [6] with parameter z = -0xd201000000010000 whose subgroup order is p = 0x73eda753299d7d483339d80809a1d80553bda40 2fffe5bfeffffffff00000001.
This is the characteristic of the field F p for which we synthesize constraints.
Let σ denote a multiset of swaps.
Let in σ denote {y : (x, y) ∈ σ} and let rm σ denote {x : (x, y) ∈ σ}.
Claim 1.
Let σ be a multiset of swaps and σ c be a cycle.
MultiSwap(S, σ σ c , S ) holds if and only if MultiSwap(S, σ, S ) does.Proof: We prove both directions simultaneously, by illustrating a bidirectional chain of mutually implicating equalities.
We start with the definition of MultiSwap(S, σ σ c , S ):Since σ c is a cycle, we have that in σ c = rm σ c , so rm σ c ⊆ S in σ c , and the removal of rm σ c can be moved earlierThis last line is exactly our goal: the statement that MultiSwap(S, σ, S ) holds.
Claim 2.
If σ contains no cycles and MultiSwap(S, σ, S ) holds, then σ is sequentially consistent with respect to S, producing S .
Proof: Let n be the number of swaps in σ.
For a set S and multiset of swaps τ, define the directed multigraph G S,τ as a multigraph where the vertices are the universe of multiset elements, the edges point from each removal to its corresponding insertion, and each vertex is labeled with a multiplicity equal to to the multiplicity of that vertex's element in S, minus the out-degree, plus the in-degree.
Observe that in G = G S,σ , the multiplicity of each vertex is equal to the multiplicity of that element in S .
Furthermore, by the predicate MultiSwap(S, σ, S ) and the soundness of the proofs of insertions and removal, all multiplicities in G are non-negative.
We now construct the sequentially valid ordering of σ.
Since σ has no swap cycles, G has no edge cycles.
Thus, the edges of G can be topologically sorted such that all edges to a vertex occur before any edge from that vertex.
We lift this edge order to a swap order, observing that in this swap order, all swaps inserting an element occur before all swaps removing it.It suffices to show that when σ is applied to S in this order, each swap is valid.
Let σ i denote the first i elements of σ in the aforementioned order.
Thus, G S,σ n is equal to G. Furthermore, the order ensures for all i > j and for all vertices v, the multiplicity of v in G S,σ i is at most the multiplicity of v in G S,σ j .
Suppose that the i th element of this order, (x i , y i ) were invalid, where i ≤ n.
This implies that the multiplicity of x i in G S,σ i is negative.
This would imply that the multiplicity of x i in G S,σ n = G were negative, a contradiction.
Thus no swap (x i , y i ) is invalid in this order.
Proof of Lemma 1.
The reverse direction follows immediately from the definition of MultiSwap.We prove the forward direction by (strong) induction on the size of σ.
Say that σ has no cycles.
Then the lemma follows from Claim 2.
Otherwise, let τ be a multiset of swaps and let σ c be a cycle such that σ = τ σ c .
By Claim 1, MultiSwap(S, τ, S ) holds.
Then, by the inductive hypothesis, τ can be decomposed into cycle-free τ and cycles τ c i such that τ = τ i τ c i and τ is sequentially consistent with respect to S, producing S .
By observing that τ ( i τ c i ) σ c is a decomposition of σ into a cycle-free swap multiset and cycles, we conclude this direction of the proof.
Our RSA accumulators work in G = Z × N /{±1}, where N is the RSA-2048 challenge number [102] , N=0xc7970ceedcc3 b0754490201a7aa613cd73911081c790f5f1a8726f46355 0bb5b7ff0db8e1ea1189ec72f93d1650011bd721aeeacc2 acde32a04107f0648c2813a31f5b0b7765ff8b44b4b6ffc 93384b646eb09c7cf5e8592d40ea33c80039f35b4f14a04 b51f7bfd781be4d1673164ba8eb991c2c4d730bbbe35f59 2bdef524af7e8daefd26c66fc02c479af89d64d373f4427 09439de66ceb955f3ea37d5159f6135809f85334b5cb181 3addc80cd05609f10ac6a95ad65872c909525bdad32bc72 9592642920f24c61dc5b3c3b7923e56b16a4d9d373d8721 f24a3fc0f1b3131f55615172866bccc30f95054c824e733 a5eb6817f7bc16399d48c6361cc7e5.We randomly selected a 2048-bit offset ∆ for our divisionintractable hash H ∆ ( §4.2); we use the value ∆=0xf3709c40 772816d668926cae548ffea31f49034ab1b30fb84b595ca 6c126a6646a4341abea2f8b07bf8d366801ac293e5a286a bb43accdec39ac8f0bc599519cf1e532f9c70b5406c4b65 2ca7da4e1cb102b69953841ae20d4bcab055c5338487ba0 0fe95e821abd381b191dfb77bae3e022ccd818d4064882d 28481ffa2db45093a4deab05f6ebfbadcf11afe7369caea aaf1f02572348a17f0510b333b8a2d56e67d892f1e1182b 26301d9347ae0a900cff2a0979caddb1a86e04a6cbc9704 d6549e5b3aef0d5c3dc4aba648ed421b0ba37c3f8e8edc1 2ef42b86d8e5fbc0dbd903238ca2e9ed6873ccb68e8103b 5d01b4249bfbe8e70cb4f4983f41df8c8f.
Our evaluation ( §7) builds on the BLS12-381 elliptic curve [27], which is the Barreto-Lynn-Scott curve [6] with parameter z = -0xd201000000010000 whose subgroup order is p = 0x73eda753299d7d483339d80809a1d80553bda40 2fffe5bfeffffffff00000001.
This is the characteristic of the field F p for which we synthesize constraints.
