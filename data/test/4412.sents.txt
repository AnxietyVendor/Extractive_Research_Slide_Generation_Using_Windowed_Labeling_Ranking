The goal of the BioASQ challenge is to engage researchers into creating cutting-edge biomedical information systems.
Specifically, it aims at the promotion of systems and methodologies that are able to deal with a plethora of different tasks in the biomedical domain.
This is achieved through the organization of challenges.
The fifth challenge consisted of three tasks: semantic indexing, question answering and a new task on information extraction.
In total, 29 teams with more than 95 systems participated in the challenge.
Overall, as in previous years, the best systems were able to outperform the strong baselines.
This suggests that state-of-the art systems are continuously improving , pushing the frontier of research.
The aim of this paper is twofold.
First, we aim to give an overview of the data issued during the BioASQ challenge in 2017.
In addition, we aim to present the systems that participated in the challenge and evaluate their performance.
To achieve these goals, we begin by giving a brief overview of the tasks, which took place from February to May 2017, and the challenge's data.
Thereafter, we provide an overview of the systems that participated in the challenge.
Detailed descriptions of some of the systems are given in workshop proceedings.
The evaluation of the systems, which was carried out using state-of-the-art measures or manual assessment, is the last focal point of this paper, with remarks regarding the results of each task.
The conclusions sum up this year's challenge.
The challenge comprised three tasks: (1) a largescale semantic indexing task (Task 5a), (2) a question answering task (Task 5b) and (3) a funding information extraction task (Task 5c), described in more detail in the following sections.
In Task 5a the goal is to classify documents from the PubMed digital library into concepts of the MeSH hierarchy.
Here, new PubMed articles that are not yet annotated by MEDLINE indexers are collected and used as test sets for the evaluation of the participating systems.
In contrast to previous years, articles from all journals were included in the test data sets of task 5a.
As soon as the annotations are available from the MEDLINE indexers, the performance of each system is calculated using standard flat information retrieval measures, as well as, hierarchical ones.
As in previous years, an on-line and large-scale scenario was provided, dividing the task into three independent batches of 5 weekly test sets each.
Participants had 21 hours to provide their answers for each test set.
Table 1 shows the number of articles in each test set of each batch of the challenge.
12,834,585 articles with 27,773 labels were provided as training data to the participants.
The goal of Task 5b was to provide a largescale question answering challenge where the systems had to cope with all the stages of a question answering task for four types of biomedical questions: yes/no, factoid, list and summary questions ( Balikas et al., 2013).
As in previous years, the task comprised two phases: In phase A, BioASQ released 100 questions and participants were asked to respond with relevant elements from specific resources, including relevant MEDLINE articles, relevant snippets extracted from the articles, relevant concepts and relevant RDF triples.
In phase B, the released questions were enhanced with relevant articles and snippets selected manu- ally and the participants had to respond with exact answers, as well as with summaries in natural language (dubbed ideal answers).
The task was split into five independent batches and the two phases for each batch were run with a time gap of 24 hours.
In each phase, the participants received 100 questions and had 24 hours to submit their answers.
Table 2: Statistics on the training and test datasets of Task 5b.
All the numbers for the documents and snippets refer to averages.
Task 5c was introduced for the first time this year and the challenge at hand was to extract grant information from Biomedical articles.
Funding information can be very useful; in order to estimate, for example, the impact of an agency's funding in the biomedical scientific literature or to identify agencies actively supporting specific directions in research.
MEDLINE citations are annotated with information about funding from specified agencies 1 .
This funding information is either provided by the author manuscript submission systems or extracted manually from the full text of articles during the indexing process.
In particular, NLM human indexers identify the grant ID and the funding agencies can be extracted from the string of the grant ID 2 .
In some cases, only the funding agency is mentioned in the article, without the grant ID.In this task funding information from MED-LINE was used, as golden data, in order to train and evaluate systems.
The systems were asked to extract grant information mentioned in the full text, but author-provided information is not necessarily mentioned in the article.
Therefore, grant IDs not mentioned in the article were filtered out.
This filtering also excluded grant IDs deviating from NLM's general policy of storing grant IDs as published, without any normalization.
When an agency was mentioned in the text without a grant ID, it was kept only if it appeared in the list of agencies and abbreviations provided by NLM.
Cases of misspellings or alternative naming of agencies were removed.
In addition, information for funding agencies that are no longer indexed by NLM was omitted.
Consequently, the golden data used in the task consisted of a subset of all funding information mentioned in the articles.During the challenge, a training and a test dataset were prepared.
The test set of MED-LINE documents with their full-text available in PubMed Central was released and the participants were asked to extract grant IDs and grant agencies mentioned in each test article.
The participating systems were evaluated on (a) the extraction of grant IDs, (b) the extraction of grant agencies and (c) full-grant extraction, i.e. the combination of grant ID and the corresponding funding agency.
3.1 Task 5aFor this task, 10 teams participated and results from 31 different systems were submitted.
In the following paragraphs we describe those systems for which a description was obtained, stressing their key characteristics.
An overview of the systems and their approaches can be seen in Table 4.
Table 4: Systems and approaches for Task 5a.
Systems for which no description was available at the time of writing are omitted.
The "Search system" and its variants were developed as a UIMA-based text and data mining workflow, where different search strategies were adopted to automatically annotate documents with MeSH terms.
On the other hand, the "MZ" systems applied Binary Relevance (BR) classification, using TF-IDF features, and Latent Dirichlet allocation (LDA) models with label frequencies per journal as prior frequencies, using regression for threshold prediction.
A different approach is adopted by the "Sequencer" systems, developed by the team from the Technical University of Darmstadt, that considers the task as a sequenceto-sequence prediction problem and use recurrent neural networks based algorithm to cope with it.The "DeepMeSH" systems implement document to vector (d2v) and tf-idf feature embeddings (Peng et al., 2016), alongside the MESHLabeler system ( Liu et al., 2015) that achieved the best scores overall, integrating multiple evidence using learning to rank (LTR).
A similar approach, with regards to the d2v and tf-idf representations of the text, is followed by the "AUTH" team.
Regarding the learning algorithms they've extended their previous system ( Papagiannopoulou et al., 2016), improving the Labeled LDA and SVM base models, as well as introducing a new ensemble methodology based on label frequencies and multi-label stacking.
Last but not least, the team from the University of Vigo developed the "Iria" systems.
Building upon their previous approach ( Ribadas et al., 2014) that uses an Apache Lucene Index to provide most similar citations, they developed two systems that follow a multilabel k-NN approach.
They also incorporated token bigrams and PMI scores to capture relevant multiword terms through a voting ensemble scheme and the ConceptMapper annotator tool, from the Apache UIMA project (Tanenblatt et al., 2010), to match subject headings with the citation's abstract text.Baselines: During the challenge, two systems served as baselines.
The first baseline is a stateof-the-art method called Medical Text Indexer (MTI) (Mork et al., 2014) with recent improvements incorporated as described in ( Zavorin et al., 2016).
MTI is developed by the National Library of Medicine (NLM) and serves as a classification system for articles of MEDLINE, assisting the indexers in the annotation process.
The second baseline is an extension of the system MTI, incorporating features of the winning system of the first BioASQ challenge ( Tsoumakas et al., 2013).
The question answering task was tackled by 51 different systems, developed by 17 teams.
In the first phase, which concerns the retrieval of information required to answer a question, 9 teams with 25 systems participated.
In the second phase, where teams are requested to submit exact and ideal answers, 10 teams with 29 different systems participated.
Two of the teams participated in both phases.
An overview of the technologies employed by each team can be seen in Table 5.
The "Basic QA pipeline" approach is one of the two that participated in both Phases.
It uses MetaMap for query expansion, taking into account Table 5: Systems and approaches for Task 5b.
Systems for which no information was available at the time of writing are omitted.the text and the title of each article, and the BM25 probabilistic model ( Robertson et al., 1995) in order to match questions with documents, snippets etc.
The same goes for phase B, except for the exact answers, where stop words were removed and the top-k most frequent words were selected.
"Olelo" is the second approach that tackles both phases of task B.
It is built on top of the SAP HANA database and uses various NLP components, such as question processing, document and passage retrieval, answer processing and multidocument summarization based on previous approaches ( Schulze et al., 2016) to develop a comprehensive system that retrieves relevant information and provides both exact and ideal answers for biomedical questions.
Semantic role labeling (SRL) based extensions were also investigated.
One of the teams that participated only in phase A, is "USTB" who combined different strategies to enrich query terms.
Specifically, sequential dependence models (Metzler and Croft, 2005), pseudorelevance feedback models, fielded sequential dependence models and divergence from randomness models are used on the training data to create better search queries.
The "fdu" systems, as in previous years ( , use a language model in order to retrieve relevant documents and keyword scoring with word similarity for snippet extraction.
The "UNCC" team on the other hand, focused mainly on the retrieval of relevant concepts and articles using the Stanford Parser (Chen and Manning, 2014) and semantic indexing.In Phase B, the Macquarie University (MQU) team focused on ideal answers (Molla, 2017), submitting different models ranging from a "trivial baseline" of relevant snippets to deep learning under regression settings ( Malakasiotis et al., 2015) and neural networks with word embeddings.
The Carnegie Mellon University team ("OAQA"), focused also on ideal answer generation, building upon previous versions of the "OAQA" system.
They used extractive summarization techniques and experimented with different biomedical ontologies and algorithms including agglomerative clustering, Maximum Marginal Relevance and sentence compression.
They also introduced a novel similarity metric that incorporates both semantic information (using word embeddings) and tf-idf statistics for each sentence/question.
Many systems used a modular approach breaking the problem down to question analysis, candidate answer generation and answer ranking.
The "LabZhu" systems, followed this approach, based on previous years' methodologies ( ).
In particular, they applied rule-based question type analysis and used Standford POS tool and PubTator for candidate answer generation.
They also used word frequencies for candidate answer ranking.
The "DeepQA" systems focused on factoid and list questions, using an extractive QA model, restricting the system to output substrings of the provided text snippets.
At the core of their system stands a state-of-the-art neural QA system, namely FastQA ( Weissenborn et al., 2017), extended with biomedical word embeddings.
The model was pre-trained on a large-scale opendomain QA dataset, SQuAD ( Rajpurkar et al., 2016), and then the parameters were fine-tuned on the BioASQ training set.
Finally, the "sarrouti" system, from Morocco's USMBA, uses among others a dictionary approach, term frequencies of UMLS metathesaurus' concepts and the BM25 model.
OAQA system proposed by (Yang et al., 2016) for BioASQ4 was used as a strong baseline.
This system, as well as its previous version (Yang et al., 2015) for BioASQ3, had achieved top performance in producing exact answers.
The system uses an UIMA based framework to combine different components.
Question and snippet parsing is based on ClearNLP.
MetaMap, TmTool, CValue and LingPipe are used for concept identification and UMLS Terminology Services (UTS) for concept retrieval.
In addition, identification of concept, document and snippet relevance is based on classifier components and scoring, ranking and reranking techniques are also applied in the final steps.
In this inaugural year for task c, 3 teams participated with a total of 11 systems.
A brief outline of the techniques used by the participating systems is provided in table 6.
Simple regions of interest, SVM, regular expressions, hand-made rules, char-distances, ensemble DZG regions of interest, SVM, tf-idf of bigrams, HMMs, MaxEnt, CRFs, ensemble AUTH regions of interest, regular expressions Table 6: Overview of the methodologies used by the participating systems in Task 5c.
The Fudan University team, participated with a series of similar systems ("Simple" systems) as well as their ensemble.
The general approach included the following steps: First, the articles were parsed and some sections, such as affiliation or references, were removed.
Then, using NLP techniques, alongside pre-defined rules, each paragraph was split into sentences.
These sentences were classified as positive (i.e. containing grant information) or not, using a linear SVM.
The positive sentences were scanned for grant IDs and agencies through the use of regular expressions and hand-made rules.
Finally, multiple classifiers were trained in order to merge grant IDs and agencies into suitable pairs, based on a wide range of features, such as character-level features of the grant ID, the agency in the sentence and the distance between the grant ID and the agency in the sentence.
The "DZG" systems followed a similar methodology, in order to classify snippets of text as possible grant information sources, implementing a linear SVM with tf-idf vectors of bigrams as input features.
However, their methodology differed from that of Fudan in two ways.
Firstly, they used an in-house-created dataset consisting of more than 1,600 articles with grant information in order to train their systems.
Secondly, the systems deployed were based on a variety of sequential learning models namely conditional random fields ( Finkel et al., 2005), hidden markov models (Collins, 2002) and maximum entropy models (Ratnaparkhi, 1998).
The final system deployed was a pooling ensemble of these three approaches, in order to maximize recall and exploit complementarity between predictions of different models.
Likewise, the AUTH team, with systems "Asclepius", "Gallen" and "Hippocrates" emphasized on specific sections of the text that could contain grant support information and extracted grant IDs and agencies using regular expressions.Baselines: For this challenge a baseline was provided by NLM ("BioASQ Filtering") which is based on a two-step procedure.
First, the system classifies snippets from the full-text, as possible grant support "zones" based on the average probability ratio, generated separately by Naive Bayes ( Zhang et al., 2009) and SVM ( Kim et al., 2009).
Then, the system identified grant IDs and agencies in these selected grant support "zones", using mainly heuristic rules, such as regular expressions, especially for detecting uncommon and irregularly formatted grant IDs.
Each of the three batches of task 5a was evaluated independently.
The classification performance of the systems was measured using flat and hierarchical evaluation measures ( Balikas et al., 2013).
The micro F-measure (MiF) and the Lowest Common Ancestor F-measure (LCA-F) were used to choose the winners for each batch ( Kosmopoulos et al., 2013).
According to (Demsar, 2006) the appropriate way to compare multiple classification systems over multiple datasets is based on their average rank across all the datasets.
On each dataset the system with the best performance gets rank 1.0, Table 7: Average system ranks across the batches of the Task 5a.
A hyphenation symbol (-) is used whenever the system participated in fewer than 4 tests in the batch.
Systems with fewer than 4 participations in all batches are omitted.the second best rank 2.0 and so on.
In case two or more systems tie, they all receive the average rank.
Table 7 presents the average rank (according to MiF and LCA-F) of each system over all the test sets for the corresponding batches.
Note, that the average ranks are calculated for the 4 best results of each system in the batch according to the rules of the challenge.On both test batches and for both flat and hierarchical measures, the DeepMeSH systems ( Peng et al., 2016) and the AUTH systems outperform the strong baselines, indicating the importance of the methodologies proposed, including d2v and tf-idf transformations to generate feature embeddings, for semantic indexing.
More detailed results can be found in the online results page 3 .3 http://participants-area.bioasq.org/ results/5a/ Phase A: For phase A and for each of the four types of annotations: documents, concepts, snippets and RDF triples, we rank the systems according to the Mean Average Precision (MAP) measure.
The final ranking for each batch is calculated as the average of the individual rankings in the different categories.
In tables 8 and 9 some indicative results from batch 3 are presented.
Full results are available in the online results page of task 5b, phase A 4 .
It is worth noting that document and snippet retrieval for the given questions were the most popular part of the task.
Moreover, for different evaluation metrics, there are different systems performing best, indicating that different approaches to the task may be preferable depending on the target Table 9: Results for document retrieval in batch 3 of phase A of Task 5b.
outcome.
For example, one can see that the UNCC System 1 performed the best on some unordered measures, namely mean precision and f-measure, however using MAP or GMAP to consider the order of retrieved elements, it is out preformed by other systems, such as the ustb-prir.
Additionally, the combination of some of these approaches seem like a promising direction for future research.Phase B: In phase B of Task 5b the systems were asked to produce exact and ideal answers.
For ideal answers, the systems will eventually be ranked according to manual evaluation by the BioASQ experts ( Balikas et al., 2013).
Regarding exact answers 5 , the systems were ranked according to accuracy for the yes/no questions, mean reciprocal rank (MRR) for the factoids and mean Table 10: Results for batch 4 for exact answers in phase B of Task 5b.F-measure for the list questions.
Table 10 shows the results for exact answers for the fourth batch of task 5b.
The symbol (-) is used when systems don't provide exact answers for a particular type of question.
The full results of phase B of task 5b are available online 6 .
From the results presented in Table 10, it can be seen that systems achieve high scores in the yes/no questions.
This was especially in the first batches, where a high imbalance in yes-no classes leaded to trivial baseline solutions being very strong.
This was amended in the later batches, as shown in the table for batch 4, where the best systems outper-6 http://participants-area.bioasq.org/ results/5b/phaseB/ form baseline approaches.On the other hand, the performance in factoid and list questions indicates that there is more room for improvement in these types of answer.
Table 11: Micro Recall (MR) results on the test set of Task 5c.
task were selected as winners.The results of the participating systems can be seen in Table 11.
Firstly, it can be seen that the grant ID extraction task is harder compared to the agency extraction.
Moreover, the overall performance of the participants was very good, and certainly better than the baseline system.
This indicates that the currently deployed techniques can be improved and as discussed in section 3.3, this can be done through the use of multiple methodologies.
Finally, these results, despite being obtained on a filtered subset of the data available, could serve as a springboard to enhance and redeploy the currently implemented systems.
In this paper, an overview of the fifth BioASQ challenge is presented.
The challenge consisted of three tasks: semantic indexing, question answering and funding information extraction.
Overall, as in previous years, the best systems were able to outperform the strong baselines provided by the organizers.
This suggests that advances over the state of the art were achieved through the BioASQ challenge but also that the benchmark in itself is challenging.
Consequently, we believe that the challenge is successfully towards pushing the research frontier in on biomedical information systems.In future editions of the challenge, we aim to provide even more benchmark data derived from a community-driven acquisition process and design a multi-batch scenario for Task 5c similar to the other tasks.
Finally, as a concluding remark, it is worth mentioning that the increase in challenge participation this year 7 highlights the healthy growth of the BioASQ community, gathering attention from different teams around the globe and constituting a reference point for biomedical semantic indexing and question answering.
Regarding the evaluation of Task 5c and taking into account the fact that only a subset of grant IDs and agencies mentioned in the full text were included in the ground truth data sets, both for training and testing, micro-recall was the evaluation measure used for all three sub-tasks.
This means that each system was assigned a micro-recall score for grant IDs, agencies and full-grants independently and the top-two contenders for each sub- The fifth edition of BioASQ is supported by a conference grant from the NIH/NLM (number 1R13LM012214-01) and sponsored by the Atypon Systems inc.
BioASQ is grateful to NLM for providing baselines for tasks 5a and 5c and the CMU team for providing the baselines for task 5b.
Finally, we would also like to thank all teams for their participation.
