Membership inference (MI) attacks exploit the fact that machine learning algorithms sometimes leak information about their training data through the learned model.
In this work, we study membership inference in the white-box setting in order to exploit the internals of a model, which have not been effectively utilized by previous work.
Leveraging new insights about how overfitting occurs in deep neural networks, we show how a model's idiosyncratic use of features can provide evidence for membership to white-box attackers-even when the model's black-box behavior appears to generalize well-and demonstrate that this attack outperforms prior black-box methods.
Taking the position that an effective attack should have the ability to provide confident positive inferences, we find that previous attacks do not often provide a meaningful basis for confidently inferring membership, whereas our attack can be effectively calibrated for high precision.
Finally, we examine popular defenses against MI attacks, finding that (1) smaller generalization error is not sufficient to prevent attacks on real models, and (2) while small-ε-differential privacy reduces the attack's effectiveness, this often comes at a significant cost to the model's accuracy; and for larger ε that are sometimes used in practice (e.g., ε = 16 [43]), the attack can achieve nearly the same accuracy as on the unprotected model.
Many compelling applications of machine learning involve the collection and processing of sensitive personal data, giving rise to concerns about privacy [2,4,7,10,11,26,33,38,45,46].
In particular, when machine learning algorithms are applied to private training data, the resulting models might unwittingly leak information about that data through their behavior or representation.Membership inference (MI) attacks aim to determine whether a given data point was present in the training set used to build a model.
This can be a privacy threat in itself, but vulnerability to MI has also come to be seen as a more general indicator of whether a model leaks private information [27,38,47], and is closely related to the guarantee provided by differential privacy [26].
To date, most MI attacks follow the so-called shadow model approach [38].
This approach casts the attack as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status.
To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from data derived from the shadow models' outputs on the points used to train each shadow model and points not previously seen by each shadow model.
Subsequently, Nasr et al. extended this attack to the white-box setting [33] by including activation and gradient information obtained from the target model as features for the attack model.
However, Nasr et al. find that a simple extension of the shadow model approach to the white-box setting does not produce an effective attack [33] (we discuss why in Section 4); thus, their white-box attack deviates from the threat model common to most work on MI, and instead assumes that the adversary already knows a significant portion of the target model's training data.
Features to train the attack model are obtained directly from the target model, using the gradients, activations, and outputs obtained by evaluating on known member/non-member points.
In this paper, we present an effective white-box MI attack that operates without access to any of the target model's training data.
Crucially, our analysis uncovers a more intimate understanding of how overfitting takes place in a model, which we leverage to create our attack.Finding Evidence of Membership.
In this paper, we take a fresh look at the problem of white-box membership inference.
We begin with the intuitive observation that while overfitting leads to privacy issues because the model "memorizes" certain aspects of the training data, this is not necessarily manifested in the model's output behavior.
Instead, it is likely to show up in the way that the model uses features-both those that are given explicitly and that are learned in internal layers.Intuitively, we posit that idiosyncratic features present in the training data, which are predictive only for the training data but not the sampling distribution, are oftentimes encoded [25] for three test instances.
The explanations show that the model uses Tony Blair's face to classify these instances, as we might expect.
Meanwhile, (c) shows the explanation for the image with the distinctive pink background from the training set, where we see that the model is using the pink background to infer that the image is of Tony Blair.in the model during training.
Consider the example illustrated by Figure 1, in which a model was trained to recognize faces from the Labeled Faces in the Wild (LFW) dataset.
Figure 1a shows 12 instances sampled from the training set of the model.
The top right corner of Figure 1a depicts an image of Tony Blair with a distinctive pink background.
Supposing that the background is unique to this training instance, an overfit model may use the background as a feature for classifying Tony Blair, identifying the instance as a member of the training set via the uncharacteristic way in which the model correctly labels it.
In such a setting, the model's use of the pink background could be viewed as evidence of membership.
Figures 1b and 1c show this phenomenon on a convolutional neural network trained on this dataset.
Figures 1b and 1c visualize the regions of the image most influential [25] towards the classification of "Tony Blair" on three test instances, and on the aforementioned training instance with the pink background.
While the model is influenced most by Tony Blair's face for classification on the test instances, on the training instance it relies on the distinctive pink background.We show that this evidence-based approach can be used on a variety of real datasets to infer membership, and leverage it to develop a new attack (Sections 3 and 4) that outperforms previous attacks (Section 5).
Calibrating Confidence.
By far the simplest MI attack, which we dub the "naive" attack, follows from the fact that generalization error necessarily leads to membership vulnerability [47].
Given a data point and its true label, the attacker runs the model and observes whether its predicted label is correct.
If it is, then the attacker concludes that the point was in the training data; otherwise, the point is presumed a non-member.
Surprisingly, in many cases this works as well as the shadow model attack (Section 5.5, Figure 10).
As a practical attack, the naive method has a significant drawback even when it appears yield reasonable accuracy.
Namely, it does not provide the attacker with much confidence about a positive inference: the point may have been a training set member, or it may just have been classified correctly.
After all, this is how the model is intended to behave on test points, so it may not be sensible to base a membership inference on a correct prediction result.Initially, it may seem that shadow model attacks do not inherit this limitation, as the attack model can be trained to emit a confidence score with its prediction.
If this score is well-calibrated, then an attacker could use it to make more confident inferences.
Unfortunately, we find shadow attacks are not typically well-calibrated; in fact, Figure 11 (Section 5.5) shows that raising the confidence threshold for positive prediction sometimes decreases the precision of the attack.
In short, like the naive attack, the shadow model attack often produces little consistently useful information to characterize the likelihood that a positive inference is correct.We posit that if the adversary confidently identifies even one training point, then it is reasonable to say that a privacy violation occurred.
We therefore propose that an effective attack should have the ability to make confident inferences, underscoring the need for attacks with high precision.
To this end, we demonstrate that the confidence scores accompanying the inferences made by our attack can be used to accurately calibrate its precision (Section 5.5, Figure 11).
Evaluating Defenses.
A number of defenses have been proposed for membership inference.
Differential privacy (DP) [8], in addition to regularization methods like dropout [41] in deep nets are two commonly-proposed defenses.
While differential privacy gives a theoretical guarantee against membership inference [47], a meaningful guarantee-one that bounds the probability of attack success below 1-requires an ε that is considerably smaller than what is often used in practice.
Nonetheless, common wisdom conjectures that large-ε-DP may provide a practical defense, particularly if the privacy budget analysis only gives a loose bound on ε.Unfortunately, we find that this is not necessarily the case.
We test our attack on deep models trained with (ε,δ)-differential privacy using the moments accountant method [1] (Section 6), and find that training with a large ε sometimes provides little defense against our attack when compared against its effectiveness on non-private models.
These results demonstrate that practical MI attacks like the one described in this paper can serve as a heuristic measure to evaluate paramater choices in private learning, while also emphasizing the need for more research in this area.Organization.
In Section 2, we introduce background on membership inference and machine learning.
Section 3 describes the evidence-based attack, beginning in an idealized setting that can be rigorously analyzed to motivate the intuition behind the attack (Section 3.2).
Subsequently, we gradually lift the generative assumptions used in this derivation to obtain an attack that works well on real data (Sections 3.3 and 3.4).
Section 3.5 discusses calibration, and Section 4 shows how our attack can be extended to deep networks.
Section 5 presents our evaluation on both synthetic data and nine real datasets derived from real-world medical and financial data, and common benchmark datasets.
Section 6 discusses defenses against MI attacks and tests their efficacy against our attack.
Section 7 covers related work, and Section 8 concludes the paper.
Membership inference (MI) attacks aim to determine whether a given data point was present in the dataset used to train a given target model.
In this section, we begin by introducing the necessary background needed to formally define membership inference, as well as explicitly defining the threat model used in our analysis.
We assume data from some universeU = X × Y ⊂ R n ×[C], drawn from a distribution, D * .
Consistent with the typical supervised learning setting, x ∈ X is a vector of n features and y ∈ Y is a label or classification target, corresponding to C distinct classes.
Given a loss function, L : X × Y → R, the goal of supervised learning is to construct a model, g, that minimizes L(g(x),y) on future unseen samples, x, drawn from D * .
This is achieved by minimizing L(g(x), y) on a finite training set, S, drawn i.i.d. from D * .
A membership inference attack operates on a particular target model, ˆ g.
In this work, we consider target models that are expressed as feed-forward neural networks; i.e., they consist of successive linear transformations, or layers, where each layer, , is parameterized by a matrix of weights and biases W , B , followed by the application of a non-linear activation function.Consistent with common practice, we assume that internal layers use the rectified-linear (ReLU) activation: relu(x) = max(0,x).
We assume that the final layer has one component for each label in [C] and uses the softmax activation: softmax(x) j = e x j /∑ i e x i .
The use of the softmax function is standard in machine learning for multi-class classification.
Models trained in this way produce confidence scores for each label that can be interpreted as probabilities [12].
In the simplest case we consider, the target model consists of a single layer with only the softmax activation, and is a linear softmax regression model.
We will sometimes refer to this type of model by its parameterization, ˆ W , ˆ b.
Our approach generalizes to deep networks where the target model has multiple successive internal ReLU-activated layers, followed by a single softmax output layer.
We adpot a formulation of Membership Inference attacks similar to that of Yeom et al. [47].
First a value, b, is chosen uniformly at random from {0, 1}.
If b = 1, the attacker, A, is then given an instance (x,y) from the general population; otherwise, if b = 0, (x, y) is sampled uniformly at random from the elements of the training set, S, used to generate target model, ˆ g.
The attacker then attempts to predict b given (x,y) and some additional knowledge, aux( ˆ g), aboutˆgaboutˆ aboutˆg determined by the threat model (see below).
Threat Model.
Prior work [38,47] has focused primarily on the so-called black-box model where the adversary has access to D * , the learning algorithm used to producê g (including hyperparameters), the size of the training set, and the ability to queryˆgqueryˆ queryˆg arbitrarily on new points.
In practice, having access to D * amounts to knowing a finite data set, ˜S (distinct from S), sampled i.i.d. from D * .
In this work, we replace black-box access tô g with white-box access.
Rather than only being able to query the target model, the attacker has access to the exact representation ofˆgofˆ ofˆg that was produced by the learning algorithm and used by the model owner to make inferences on new data.
For the target models commonly used in practice, e.g. neural networks and linear classifiers, this amounts to a set of floating-point weight matrices and biases, in addition to the linear operators and activation functions used at each layer.This threat model reflects the growing number of publiclyavailable models on websites like Model Zoo [21], as well as the fact that white box representations may fall into the hands of an adversary via other means (e.g., a security breach).
Additionally, even in situations where the requirements for a whitebox attack may not be practical for an adversary, the ability to mount a more powerful attack could be useful for a defender, as it provides a more conservative estimate of the potential threat.Metrics.
The accuracy of an attack is the probability that A's prediction is equal to b, taken over the randomness of b, (x, y), and A. Because an adversary that guesses randomly achieves 50% accuracy, we will often opt to describe the advantage of an attack [47], given by Equation 1 in terms of attack, A. Advantage scales accuracy to the 50% baseline to yield a measure between -1 and 1.
advantage(A) = 2Pr A (x,y), aux( ˆ g) = b −1 (1)While advantage is an indicator of the degree to which private information is leaked by the model, it does not necessarily capture the severity of the threat posed to any given individual in the training set.
From this perspective, a privacy violation occurs if any of the points can be confidently identified by the adversary-this is arguably a greater threat than if the adversary were to identify every training member with very low confidence.
Thus, we also consider precision (Equation 2) as a key desideratum for the attacker.
In order for an attacker to reach confident inferences, precision must be appreciably greater than 1/2.
If no points are predicted to be members, we define precision to be 1/2.
precision(A) = Pr b = 1|A (x,y), aux( ˆ g) = 1(2)Finally, we include recall (Equation 3) as a metric in our evaluation as it has been reported in prior work.
However, we place less emphasis on this metric, as an attack with high recall is not necessarily effective in practice if it fails to return confident inferences on any points.
For example, an adversary that simply predicts that all points are members achieves perfect recall, yet this clearly does not constitute a practical attack.recall(A) = Pr A (x,y), aux( ˆ g) = 1|b = 1 (3)Logistic Attack Models.
In the interest of achieving good precision, we consider attacks that yield confidence scores with their predictions.
Thus, we can think of membership inference as a binary logistic regression [32] problem, in which a logistic (sigmoid) function models confidence with respect to the binary dependent variable (i.e., membership or non-membership).
The sigmoid function, ¯ s, is is given by ¯ s (x) = 1 1+e −x , and can be thought of as converting the log-odds of the dependent variable to a probability.
The use of the sigmoid function for binary classification is standard in machine learning, and has been applied in prior membership inference attacks as well [38].
In this section, we introduce our core membership inference attack.
Starting in an idealized setting where the exact data distribution is known and the model is linear, we proceed by deriving the Bayes-optimal logistic attack model (Section 3.2).
We show that when the data-generating assumptions hold, the confidence scores produced by this attack correspond to the true membership probability, and can thus be used for effective, accurate calibration towards high-precision attacks.
Using the insights gained from this analysis, we then show how to generalize the attack to settings where the data-generating distribution is unknown or does not match our theoretical assumptions (Sections 3.3 and 3.4), and discuss calibration in this setting (Section 3.5).
In Section 4 we extend the attack to deep models.
Our attack works from the intuition that when models overfit to their training data, they potentially leak membership information through anomalous behavior at test time.
However, while this behavior may manifest itself in the form of prediction errors on unseen points, this need not be the case, and a more nuanced look at how memorization occurs yields new insights that can be used in an attack.Models use features to distinguish between classes, and while some features may be truly discriminative (i.e., function as good predictors on unseen data), others may be discriminative only on the particular training set merely by coincidence.When the model applies features of the latter type to make a prediction, this can be thought of as "evidence" of overfitting regardless of whether the prediction is correct; the salience of a feature coincidental to the training data is suggestive on its own.
Similarly, there may be features that are discriminative on the data in general, but not on the training data.For example, consider a hypothetical model trained to recognize celebrity faces.
Suppose that in reality, each celebrity is wearing sunglasses in 10% of his or her respective pictures, so the presence of sunglasses is not an informative feature for this task.
However, if the training data used to construct the model contained images of a particular subject wearing sunglasses with greater frequency, say 30%, then the model might learn a feature that detects sunglasses in an internal layer, and weight this feature towards prediction of that subject.
Knowing that the presence of sunglasses is not predictive of identity on the true distribution, an attacker would infer that, all else being equal, a picture of this subject wearing sunglasses is more likely to be a training set member.While this may not be conclusive evidence of membership, it can be aggregated with other aspects of the model's behavior on an instance to make a final determination with greater confidence than would be possible using only black-box information.
To see why this is the case, consider that another model trained on a different sample, e.g. one that reflects a "normal" frequency of subjects wearing sunglasses, may learn to make the same numerical predictions using a different set of features.
A black-box attacker would be unable to distinguish these cases, and thus be deprived of the feature-based evidence available through an examination of the model's use of internal features.This example highlights the intuition that membership information is leaked via a target model's idiosyncratic use of features.
Essentially, features that are distributed differently in the training data from how they are distributed in the true distribution can provide evidence either for or against membership.
Our attack works by deriving a set of parameters that profile idiosyncratic feature use, which are then used to construct a logistic attack model.
To motivate this intuition more formally, we begin by showing how to mount this evidence-based attack in an idealized setting where data is distributed according to a known distrubution.
This provides a simpler illustration of the central ideas used in our later attack, where we do not make explicit assumptions about the data distribution.
We show that the attack in this setting leads to Bayes-optimal membership predictions on points from that distribution, which suggests that even when the strict assumptions made here are violated, the approach may nonetheless be a strong heuristic even if it cannot be proved optimal.Generative Assumptions.
Recall the setting described in Section 2: a model, ˆ g, trained on S ∼ D * , and an adversary that leverages white-box access tô g to create an attack model, m, Figure 2: Example of two Gaussian distributions, η * andˆηandˆ andˆη.
The point x has a higher probability of being generated byˆηbyˆ byˆη than by η * .
Given a prior probability of 1 2 for being drawn from either distribution, the decision boundary for predicting which distribution a given point was drawn from would be at the intersection of the two curves, and x would be predicted to have been drawn fromˆηfromˆ fromˆη.x x P(x) η * ˆ ηthat predicts whether an instance, (x,y) ∈ U, belongs to S.
We show how the example above can be extended to this setting by introducing some assumptions aboutˆgaboutˆ aboutˆg and D * .
First we assume that D * is given by parameters, µ * y , Σ * , and p * = (p * 1 ,..., p * C ), such that the labels, y, are distributed according to a Categorical distribution with parameter p * , and the features, x, are multivariate Gaussians with mean µ * y for each label y, and covariance matrix, Σ * .
y ∼ Categorical(p * ) x ∼ N (µ * y ,Σ * )(4)Furthermore, assume that Σ * is a diagonal matrix, i.e., the distribution of x satisfies the naive-Bayes assumption of the features being independent conditioned on the class.
We will therefore write Σ * j j as σ * 2 j .
Recall that S is drawn i.i.d. from D * , so its samples are also distributed according to Equation 4.
However, the empirical means and variance of S will not match those of D * exactly, except in expectation.
Therefore, we denote byˆDbyˆ byˆD the empirical distribution of the training data, S. LetˆpLetˆ Letˆp be the empirical class prior for S, ˆ µ y be the empirical mean of the features in S with class y, andˆΣandˆ andˆΣ be the empirical covariance matrix of the features in S.
We make the analogous assumption thatˆΣthatˆ thatˆΣ is a diagonal matrix, and that the empirical distribution function can be modeled as a normal distribution, N (ˆ µ, ˆ Σ).
Intuitively, we can now think of m as determining whether (x,y) is more likely to have been drawn fromˆDfromˆfromˆD (i.e., (x,y) ∈ S), or D * .
If we momentarily assume that the attacker knows D * and ˆ D, then we can proceed to derive an attack model purely in terms of their respective parameters, namely µ * y , ˆ µ y , Σ * , andˆΣandˆ andˆΣ.Attack Model.
Consider two Gaussian distributions,η * = N (µ * , σ * ) andˆηandˆ andˆη = N (ˆ µ, ˆ σ).
For x ∈ R,x is more likely to have been generated byˆηbyˆ byˆη than by η * whenN (x | ˆ µ, ˆ σ) > N (x | µ * , σ * ).
An example of this is shown pictorially in Figure 2.
Assuming a prior probability of 1/2 for being drawn from either distribution, we could construct a simple model that predicts whether x was drawn fromˆηfromˆ fromˆη rather than η * by solving for x in this inequality.
When the variances, σ * andˆσandˆ andˆσ, are the same, this produces a linear decision boundary as a function of µ * − ˆ µ and σ * .
Our setting is more complicated than this simple Gaussian example, but as we demonstrate below, the same principle can be applied to mount an attack.
Let (X, Y ) be random variables drawn from eitherˆDeitherˆ eitherˆD or D * (as defined above), with probability t of drawing fromˆDfromˆ fromˆD.
Let T be the event (X,Y ) ∈ S, i.e., that a point drawn according to this process was in the training set.
Thus, Pr [T ] =t.
In keeping with the MI definition presented in Section 2, we will assume that t = 1 2 .
We want an attack model, m y (x), to give us the probability that point (x,y) is a member of the training set, S.Because we know t and the parameters of D * andˆDandˆ andˆD, we can derive an estimator for this quantity by applying Bayes' rule and algebraically manipulating the result to fit a logistic function of the log odds.
We then make use of the naive-Bayes assumption, allowing us to write the probability of observing x given its label as the product of the probabilities of observing each of x's features independently.
The result is linear in the target feature values whenˆσwhenˆ whenˆσ = σ * , as detailed in Theorem 1.
The proof for Theorem 1 is given in Appendix A. by Equation 4 with parameters (p * ,µ * y ,Σ * ), and S be drawn i.i.d.from D * , with empirical distribution function, ˆ D, modeled as y ∈ S ∼ Categorical( ˆ p), x ∈ S ∼ N (ˆ µ y , ˆ Σ).
Further, assume thatˆΣthatˆ thatˆΣ = Σ * is diagonal andˆpandˆ andˆp = p * .
Then the Bayes-optimal predictor for membership is given by Equation 5.
m y (x) = ¯ s w yT x+b y(5)wherew y = ˆ µ y −µ * y σ 2 b y = ∑ j µ * 2 y j − ˆ µ 2 y j 2σ 2 jNotice that the magnitude of the attack model weights given in Theorem 1 is large only on features whose mean on the training data differs significantly from its mean in the distribution, D * , relative to that feature's variance.
This is a manifestation of the intuition described in the previous section, as the attack model effectively treats those features as its primary "evidence" for deciding membership.
We also point out that the attack model detailed in Theorem 1 defines a different set of parameters for each class label, y.
This follows from the generative assumptions, as each class may have a distinct mean, and thus must be distinguished using separate critera.
As a practical matter this is not an impediment, as our setting assumes that the true class label is given to the adversary, so there is no ambiguity as to which set of parameters should be applied.Summary.
Features that are more likely in the empirical training distribution, ˆ D, than in the true "general population" distribution, D * , serve as evidence for membership.
Theorem 1 shows how this evidence can be compiled into a linear attack model, w y ,b y , that achieves Bayes-optimality for membership inference when both distributions are known precisely.
In Section 3.3, we show how to obtain approximate values for w y and b y when the distributions are unknown.
29th USENIX Security Symposium 1609 In practice, it is unrealistic to know the exact parameters defining the distributions D * andˆDandˆ andˆD.
In particular, our threat model assumes that the attacker has no a priori knowledge of the parameters ofˆDofˆ ofˆD or the elements of S, only that S was drawn from D * .
While we assume white-box access to the target model, ˆ g, we cannot expect that it will explicitly modeîmodeî D; indeed,proxy dataset, ˜ S, which is drawn i.i.d. from D * (but distinct from S)to train a proxy model, ˜ g, which is then compared withˆg withˆ withˆg.
To control for differences in the learned weights resulting from the learning algorithm, rather than from differences betweenˆDbetweenˆ betweenˆD and D * , the proxy model is trained using the same algorithm and hyperparameters asˆgasˆ asˆg (note that this information is assumed to be known in our threat model).
This process can be repeated on many different˜Sdifferent˜ different˜S, using bootstrap sampling when the available data is limited.In more detail, we continue with the assumption that data is generated according to Equation 4.
Note that our target is a linear model, ˆ W , ˆ b, that minimizes 0-1 loss on S for the predictions given by argmax c∈ [C] {softmax( ˆ W T x + ˆ b) c }.
This is a convex optimization problem that, under our generative assumptions, is minimized whenˆWwhenˆ whenˆW andˆbandˆ andˆb are given by Equation 6 1 .
ˆ W jy = ˆ µ y j ˆ σ 2 j ˆ b y = ∑ j −ˆ µ 2 y j 2 ˆ σ 2 j +log( ˆ p)(6)Plugging this, and the analogous equation for the proxy model, ˜ W , ˜ b, into Equation 5 from Theorem 1, we see that the weights and biases of the attack model m y are approximated by w y ≈ ˆ W :y − ˜ W :y and b y ≈ ˆ b y − ˜ b y respectively, assuming that˜µthat˜ that˜µ ≈ µ * .
This is summarized in Observation 1, which leads to a natural attack as shown in Algorithm 1.
We call this the bayes-wb attack.
wherew y = ˆ W :y − ˜ W :y b y = ˆ b y − ˜ b yNotice that Observation 1 gives the weights and biases of m y in terms of only the observable parameters of the target and proxy 1 see Murphy, Slide 20 [31] for details.Algorithm 1: The Linear bayes-wb MI Attackdef createAttackModel ( ˆ g, ˜ S): ˜ g ← trainProxy( ˜ S) w y ← ˆ g.W :y − ˜ g.W :y ∀y ∈ [C] b y ← ˆ g.b :y − ˜ g.b :y ∀y ∈ [C] return λ(x,y) : ¯ s w yT x+b y def predictMembership (m, x, y): return 1 if m y (x) > 1 2 else 0 ˆ W : ˆ W i ˜ W : ˜ W i x : d f W : W i , ¯ sFigure 3: Illustration of the generalized attack model.
A learned displacement function, d, is applied element-wise to the weights of the target and proxy model to produce attack model weights,W .
The inner product ofW and x is then used to make the membership prediction.
Not pictured: d is also applied to the biases, ˆ b and˜band˜ and˜b, to produce b, which is added to the result of the inner product.
models.
This is therefore possible even when the distributions, D * andˆDandˆ andˆD, are unknown.
Furthermore, while Observation 1 is derived and stated using relatively strong generative assumptions, we find in Section 5 that this attack is nevertheless often effective when these assumptions do not hold.
In Section 3.4 we show how to further relax these generative assumptions.
One way of viewing the bayes-wb attack is that it weights membership predictions by measuring a sort of displacement between the weights of the target model and the ideal weights of the true distribution as approximated by the proxy model.
Let d f : R×R → R be a displacement function that is applied element-wise to the weights of the model -for vectors x and y, let D(x,y) = (d f (x 1 ,y 1 ),...,d f (x n ,y n )).
We can express the bayes-wb attack via a such a displacement function, namely,w y = D( ˆ W :y , ˜ W :y ) and b y = D( ˆ b y , ˜ b y ), by letting d f (x,y) = x−y, i.e., by setting D to be element-wise subtraction.As per Observation 1, element-wise subtraction is optimal for membership inference under the Gaussian naive-Bayes assumption, but it may be that for other distributions, a different displacement function is more appropriate.
More generally, we can represent the displacement function as a neural network, and train it using whatever data is at hand.
for i ∈ [N] dõdõ S 1 i , ˜ S 0 i ← split i ( ˜ S) ˇ g i ← trainShadow( ˜ S 1 i ) ˜ g i ← trainProxy( ˜ S 0 i ) T ← ( ˇ g i .
W :y , ˜ g i .
W :y , ˇ g i .
b y , ˜ g i .
b y , x, ) ∀(x,y )∈ ˜ S i :y =y, ∀y∈[C], ∀∈{0,1}, ∀i∈[N] D ← argmin D E ( ˆ w, ˜ w, ˆ b, ˜ b,x,)∈T L(¯ s(D ( ˆ w, ˜ w) T x+D ( ˆ b, ˜ b)),) ˜ g ← trainProxy( ˜ S) return λ(x,y) : ¯ s D( ˆ g.W :y , ˜ g.W :y ) T x+D( ˆ g.b y , ˜ g.b y ) def predictMembership (m, x, y): return 1 if m y (x) > 1 2 else 0initial layer has a kernel size and strides of 2 (i.e., the kernel is applied to one element ofˆWofˆ ofˆW :y and one element of˜Wof˜ of˜W :y ), and subsequent layers have a kernel size and stride of 1.
In order to learn the weights of D, we partitioñ S into an "in" dataset, ˜ S 1 , and an "out" dataset, ˜ S 0 .
We train a shadow target model, ˇ g, oñ S 1 and a proxy model, ˜ g, oñ S 0 .
We then create a labeled dataset, T , where the features are the weights and biases ofˇgofˇ ofˇg, the weights and biases of˜gof˜ of˜g, and x; and the labels are 1 for x belonging tõ S 1 and 0 for x belonging tõ S 0 .
Finally we train to find the parameters to D that minimize the 0-1 loss, L, of the general-wb attack on T .
We can increase the size of T to improve the generalization of the attack by repeating over multiple in/out splits of˜Sof˜ of˜S.
This procedure is described in Algorithm 2.
Recall the "naive" attack that predicts that an instance, x, is a member of the training set if and only if x was classified correctly.
In practice, this naive approach is not a pragmatic attack because, while it will achieve advantage equal to the target model's generalization error (and close to that of prior blackbox approaches [38]), the only way to evaluate the confidence of the inference is to use the target model's own confidence score.
As most neural networks are not well-calibrated [13], this makes it difficult to form confident inferences.
On the other hand, the derivation in Section 3.2 suggests a direct probabilistic interpretation of the attack model's output.
While the maximum likelihood estimator, which predicts x is a member of the training set when Pr[T | X = x,Y = y] > 1 2 , maximizes accuracy, the precision, and therefore confidence in positive inferences, is increased by increasing the decision threshold above 1 2 .
Under the Gaussian Naive Bayes assumption, the probability given by m is exact, and there is no issue with calibration by this approach.
As a matter of practice, there are two main concerns.
First, the training set is finite, so the recall will drop to zero at some point as the threshold is raised for greater precision.
Second, if the generative assumptions are violated, Algorithm 3: Calibrating the Decision Threshold def calibrateThreshold (m, ˜ S, α):˜ S ← sample( ˜ S) ˜ P y ← [m y (x ) for (x ,y ) ∈ ˜ S : y = y] ∀y ∈ [C] τ y ← sort( ˜ P y ) α|˜Pα|˜ α|˜P y | ∀y ∈ [C] return τ def predictMembership (m, x, y, τ): return 1 if m y (x) > τ y else 0the confidence may not correspond to an exact probability.
We must therefore be careful when selecting a decision threshold.
Calibrating the decision threshold for the desired precision/recall trade-off requires access to the training set, S. However, the attack model is obtained using˜Susing˜ using˜S, which is disjoint from S. Instead, we can stipulate that the elements of˜Sof˜ of˜S are to be classified as non-members for the purpose of calibration, and use the following heuristic: given a false-positive tolerance parameter α, set the threshold τ y for each class y as the α th -percentile confidence score of a sample of˜Sof˜ of˜S belonging to class y.
This is detailed in Algorithm 3.
In Section 5.5, we show that this heuristic consistently increases the precision of our attack on real data.
We showed how to approximate the Bayes-optimal estimator for membership prediction using the weights of a linear target and proxy model in Section 3.3.
In this section, we extend the same reasoning to deep models.
However, as deep networks learn novel intermediate representations, the semantic meaning of an internal feature at a given index-i.e., the data characteristic that it associates with-will not necessarily line up with the semantic meaning of the corresponding internal feature in another model [3,48].
This holds even when the models share identical architectures, training data, and hyper-parameters, as long as the randomization in the gradient descent is unique.
In general, the only features for which two models will necessarily agree are the models' inputs and outputs, as these are not defined by the training process.This poses a challenge for any white-box attack that attempts to extend the "shadow model" approach [38] developed for black-box membership inference.
Consider such an approach, which learns properties of internal features that indicate membership-involving activations, gradients, or any other quantity-from shadow models.
Any such property must make reference to specific internal features within the shadow model, but even if the target model contains internal features that match these properties, they are unlikely to reside at exactly the same location within the network as they do in the shadow model.
This is why previous white-box attacks [33] require large amounts of the target model's training data; rather than learning attack models from shadow models, they are forced to learn them from the target model itself and its training data.
Algorithm 4: The Deep bayes-wb MI Attackdef createAttackModel ( ˆ g• ˆ h, ˜ S): ˜ S ← [( ˆ h(x),y) for (x,y) ∈ ˜ S] ˜ g ← trainProxy( ˜ S ) w y ← λ(z) : χ( ˆ g• ˆ h,P z 0 ) y −χ( ˜ g• ˆ h,P z 0 ) y ∀y ∈ [C] b y ← ˆ g(0) y − ˜ g(0) y ∀y ∈ [C] return λ(x,y) : ¯ s w y ( ˆ h(x)) T ˆ h(x)+b y def predictMembership (m, x, y): return 1 if m y (x) > 1 2 else 0To circumvent this limitation, one must either construct a mapping between internal features in the shadow and target models, or fix the feature representation in the shadow model to preserve semantic meaning between the two.
In this section, we show how to accomplish the latter by constructing a series of local linear approximations of the network (Section 4.1), one for each internal layer, that operate on the feature representation of the target model.
Because each approximation is linear, we can apply any of the attacks from Section 3 to each approximation, and combine the results (Section 4.2) to form an attack model for the full network.
We define a local linear approximation in terms of a slice, g,h, which decomposes a deep network, f , into two functions, g and h, such that f = g • h. Intuitively, a slice corresponds to a layer, , of the network, where h computes the features that are input to layer , and g computes the output of the model from these features.For the slice at the top layer of the network, g is simply a linear model acting on features computed by the rest of the model.
In this case no local approximation is needed and the bayes-wb (Algorithm 1) and general-wb (Algorithm 2) attacks can by applied directly to g using internal features that are precomputed by h.For slices lower in the network, g is no longer linear, but we can approximate the way in which g makes use of its features at a particular point by constructing a linear model that agrees with it at that point.
To do this, we make use of an influence measure over the inputs of g to its computed output for each point.
Given a model, f , a point, x, and feature, j, the influence χ j ( f ,x) of x j on f is a quantitative measure of x j 's contribution to the output of f .
A growing body of work on influence measures [25,40,42] provides several choices for χ, each with different properties.For this approximation, we propose using an influence measure that (1) works on internal features, (2) weights features according to their individual marginal contribution to the model's output, (3) satisfies linear agreement, and (4) is efficient with respect to a chosen baseline.
Linear agreement requires that when f is linear, the influence of feature x j is simply the corresponding weight, W j .
Thus, the influence measure generalizes the notion of weights in a linear model, and we can use the influence of a feature in place of the corresponding weight in Equation 7, while obtaining the same result.
However, in order for this substitution to work at a particular internal point, z = h(x), we also require that g(z) = ¯ W T x z+ ¯ b, where ¯ W x captures how each of the features, z j , are used to obtain the model's output, which is semantically meaningful, at point, x.
This follows if χ is efficient with respect to a baseline point z 0 , as defined in Equation 8.
∑ j χ j (g•h,z)(z j −z 0 j ) = g(z)−g(z 0 )(8)When (8) holds, we can set z 0 to zero to arrive at the desired local linear approximation, noting that efficiency with respect to the zero baseline implies g(z) = χ(g•h,z) T z+g(0).
The unique influence measure satisfying the first three properties is internal influence [25], given by Equation 9.
Note that rather than operating on a single point, this measure operates over a distribution of interest, P, which specifies a distribution of points in the model's latent space, z = h(x).
χ j (g•h,P) = z∈h(X ) ∂g ∂z j z P(z)dz(9)When we set P to the uniform distribution over the line from a baseline z 0 to z, denoted P z z 0, then this measure also satisfies efficiency in exactly the manner described above.
We can therefore locally approximate g at z as ¯ g(z) = ¯ W T x z+ ¯ b, where ¯ W x = χ(g•h,P z 0 ) and b = g(0).
Thus, we can apply the attacks in Algorithm 1 and Algorithm 2 (Section 3) on an arbitrary layer of a deep network, by locally approximating the remainder of the network as a linear model at each point the attack is applied to.
Note that this gives a separate set of weights for each input, x (hence why we call the approximation "local"); however, our attacks are parametric in the weights of the target model, so only a single attack model is necessary.
The modification of Algorithm 1 for an arbitrary slice, ˆ g, ˆ h, of a target deep network, ˆ f , is detailed in Algorithm 4.
An analogous modification of Algorithm 2 follows as well, by simply replacing each reference to weights with influence measurements, but is omitted for the sake of brevity.Summary.
We can generalize the attacks given by Algorithms 1 and 2 to apply to an arbitrary layer of a deep target network by replacing the weights with their natural generalization, influence.
Because influence allows us to create a faithful local linear approximation of the model for any given point, this generalized attack follows from the same analysis on linear models from Section 3.
In Section 4.2, we suggest a method for combining attacks on each individual layer to create an attack that utilizes white-box information from all the layers of a deep network.
The results of Section 4.1 allow us to leverage overfitting in each learned representation employed by the target model towards membership inference.
Attacks on different layers may pick up on different signals, but because the model's internal representations are not independent across layers, we cannot simply concatenate the approximated weights of each layer and treat it as an attack on a single model.
Instead, we make use of a meta model, which learns how to combine the logistic outputs of the individual layer-wise attacks.
The meta model takes the confidences of the attack defined in Section 4.1 applied to each layer, and outputs a single decision.To train a meta model, m , to attack target model, f , we partitioñ S into two parts, ˜ S 1 and˜Sand˜ and˜S 0 .
We train a shadow target model, ˇ f , oñ S 1 .
Then, for each layer, , in f , we train an attack model, m , on the th layer ofˇfofˇ ofˇf , as described in Section 4.1.
We then construct a training set,T = T 1 ∪ T 0 , such that (x ,y ) ∈ T 1 is constructed as (x ,y ) = (m y (x),1) for (x,y) ∈ ˜ S 1 , and (x , y ) ∈ T 0 is constructed as (x , y ) = (m y (x), 0) for (x,y) ∈ ˜ S 0 .
We can increase the size of T by creating multiple random partitions of˜Sof˜ of˜S.
Finally, we train m on T .
When building a meta model for the general-wb attack, we can train m jointly with the displacement metric, d, rather than first learning a general-wb attack on each layer.
We also use a separate distance metric, d for each layer, , of f .
In this section, we aim to answer several questions about the attacks described in Sections 3 and 4 using empirical results on several real and synthetic datasets.
Section 6 presents additional experimental results having to do with the efficacy of several popular defenses against our attacks.How sensitive are our attacks to the data assumptions made in Section 3, hyperparameter choices, and amount of data?
In Section 5.2, we find that the learning-based attack described in Section 3.4 (general-wb) recovers nearly all of the advantage of the optimal "omniscient" attack, despite making no generative assumptions.
Additionally, we show how the hyperparameters used in this attack can be effectively tuned using validation data.
Finally, Section 5.3 discusses attack performance as more or less data is available both for training and to the attacker.Do certain layers leak more training information than others?
Section 5.4 explores the effectiveness of the meta attack model described in Section 4.2 at combining predictions from attacks on each layer of the model.
Our results show that while all layers play a role in leaking information, in some cases attacks which use combined information from different layers have greater efficacy than the corresponding sum of layer-wise independent attacks.Relative to prior attacks on real data: (1) are the bayeswb and general-wb attacks more effective in terms of overall accuracy?
(2) does the calibration step (Section 3.5) consistently lead to more confident inferences?
(3) do our attacks work on well-generalized models?
Our results in Section 5.5 indicate that bayes-wb and general-wb improve on the performance of prior black-box attacks, both in terms of accuracy and to a larger extent precision.
Moreover, even on models low generalization error (< 2%), our attack can be calibrated make high-confidence inferences, which we find is not possible with prior approaches.
We now present details on the datasets, target models, methodology, and attack methods used in our experiments.Datasets.
We performed experiments over both synthetic data and nine classification datasets derived from real data.
In general, we chose datasets from domains, such as medicine and finance, for which membership inference is likely to be a real concern.
To facilitate a baseline for comparison against prior work, we also included three common image datasets (MNIST, CIFAR10, and CIFAR100) that are less-plausibly connected to privacy, but serve as effective benchmarks, particularly because they have been studied in nearly all published membership inference experiments.The synthetic data were generated with 10 classes, 75 features, and 400, 800, or 1,600, records, with an equal number of records per class.
The features, x j , of the synthetic data were drawn randomly from a multivariate Gaussian distribution with parameters, µ y (for each class, y) and Σ, where µ y j was drawn uniformly at random from [0,1], and Σ was a diagonal matrix with Σ j j drawn uniformly at random from [0.5,1.5].
Among the classification datasets were Adult, Pima Diabetes (obtained from the UCI Machine Learning Repository); Breast Cancer Wisconsin, Hepatitis, German Credit, Labeled Faces in the Wild (obtained from scikit-learn's datasets API); MNIST [24], CIFAR10, and CIFAR100 [23].
Figure 4 shows the characteristics of each of these datasets.Target Models.
The target models we used to conduct our experiments include linear models, multi-layer perceptrons, and convolutional neural networks.
Each model was trained until convergence with categorical cross-entropy loss, using SGD with a learning rate of 0.1, a decay rate of 10 −4 , and Nesterov momentum.Linear models were implemented as a single-layer network in Keras [6] using a softmax activation.
We used linear models only for the synthetic data.
For non-image real data, we used a multi-layer perceptron (MLP) with one hidden layer and ReLU non-linearities, implemented in Keras.
For datasets with n features, we employed 2n hidden units, followed by a softmax layer with one unit per class.
For image data, we used a CNN architecture based on LeNet, with two convolutional layers with 5×5 filters and 20 and 50 output channels respectively (each convolutional layer is followed by a max pooling layer), followed by a fully connected layer with 500 neurons.
We trained CNNs with a 25% dropout rate following each pooling layer, and a 50% dropout rate following the fully connected layer.Each target model is a pair containing an architecture and a dataset.
We refer to each target model by its dataset abbreviation given in Figure 4.
each of the target models used in our evaluation are given in the final two columns of Figure 4.
Methodology.
When evaluating each attack, we randomly split the data into three disjoint groups: train, test, and hold-out.
The train and test groups were each comprised of one fourth of the total number of instances, and the hold-out group contained the remaining one half of the instances.
The target model was trained on the train group, while the attacks were allowed to make use of the hold-out group only.
The attack model's predictions were evaluated on the train group (members) and the test group (non-members).
Each experiment was repeated 10 times over different random samplings of the data split, and the results were averaged.Attack Methods.
Throughout our evaluation, we assess four different attacks: naive, bayes-wb, general-wb, and shadow-bb.
The naive attack refers to the simple attack introduced in Section 1, in which the attack model predicts an instance, x, is a member of the training set if and only if x was classified correctly.
For the bayes-wb attack (introduced in Section 3.3), we trained 10 proxy models on random samples from the hold-out group, and took the mean of their approximated weights at each point for added robustness.
When attacking MLP models, we performed the attack on the final layer of the MLP using Algorithm 1.
When attacking LeNet models, we used a meta attack model (described in Section 4.2) that was trained on data from 10 shadow models trained on 10 samples from the hold-out group.
We used a MLP with 16 internal neurons for the meta model and trained it for 32 epochs with Adam [20].
For the general-wb attack (introduced in Section 3.4), we construct an attack model that learns a displacement function, D (Algorithm 2), for each layer, , of the network, and combines the results with a meta attack model, M.
The attack model was trained for 32 epochs with Adam, using data from 10 shadow models trained on the hold-out group.
As suggested in Section 3.4, we modeled each D as a convolutional neural network.
In each experiment, the networks modeling M and each D had at most one hidden layer, with n M and n D hidden units, respectively (in our experiments each D used the same architecture, though this need not be the case in general).
In order to determine n M and n D for each dataset, we created a validation set using 10 shadow models trained on Comparison of the bayes-wb and general-wb attacks to an omniscient attack, which has knowledge ofˆµofˆ ofˆµ, µ * , and σ, and thus can use Theorem 1 directly without the use of a proxy model.
In one case, the general-wb attack was given the minimum capacity to reproduce the bayes-wb attack, i.e., d is simply a weighted sum ofˆWofˆ ofˆW i and˜Wand˜ and˜W i .
In another case, the general-wb attack was given excess capacity, with 16 hidden units in d. Three target models, trained on synthetic Gaussian naive-Bayes data with training set sizes of 100, 200, and 400, were attacked.
different random splits of the hold-out group, and performed a parameter sweep over n M ,n D .
We then took the n M and n D yielding the highest validation accuracy for each target model.
We find that because the attack model is highly regularized via its restrictive architecture, the validation accuracy is a reasonably good indicator of the test accuracy, making it a useful tool for hyper-parameter tuning (see Figure 6).
The shadow-bb attack refers to the black-box shadow model attack [38], explained briefly in Section 7.
In each experiment, the shadow-bb attack was trained using 10 shadow models trained on 10 samples from the hold-out group.
In Section 3.2, we derive the Bayes-optimal membership inference attack on Gaussian data satisfying the naive-Bayes condition.
The weights of the optimal membership predictor for this case, given by Theorem 1, are a function of the empirical training distribution parameters and true distribution of the data, which, of course, would be unknown to an attacker.
Section 3.3 describes how to address this, using a proxy model to capture the difference between the data used to train the target model and the general population.
Figure 5 demonstrates the effectiveness of the proxy model in our attack, by comparing our bayes-wb attack using a proxy model to an "omniscient" attack, which uses Equation 6 directly, with knowledge of the train and general distribution.
We can consider the omniscient attack as giving an upper bound on the expected accuracy of a white-box attack on Gaussian naive-Bayes data, as it is the true Bayes-optimal attack (while bayes-wb is the approximate Bayes-optimal attack according to Proposition 1).
Our attack achieves on average 84% of the advantage of the omniscient attack, suggesting that the proxy model was able to approximately capture the general distribution as necessary for the purpose of detecting the target model's idiosyncratic use of features.In Section 3.4, we further generalize the bayes-wb attack to use a learned displacement function that may be more appropriate for distributions that don't resemble the Gaussian naive-Bayes assumption.
While we find that this general-wb attack often generalizes to arbitrary distributions better than the bayes-wb attack, because its displacement function is learned, it is possible for the general-wb attack to overfit.
Figure 5 also shows the accuracy of the general-wb attack on Gaussian naive-Bayes data.
When the neural network representing the displacement function is given exactly enough capacity to reproduce the bayes-wb attack, general-wb recovers on average 94% of the advantage of the bayes-wb attack.
Upon inspecting the weights of the displacement network, we find that general-wb learns almost exactly element-wise subtraction, demonstrating its potential to learn the optimal displacement function.
When given excess capacity, the general-wb attack performs only marginally worse, achieving on average 92% of the minimal general-wb attack's advantage (86% of bayes-wb), suggesting that general-wb is not highly prone to overfitting.Tuning the general-wb Attack.
As mentioned, even an over-parameterized displacement function may be able to perform nearly optimally on models trained on simple datasets, like the Synthetic dataset.
However, as the general-wb attack involves several hyper-parameters, it may be useful to tune these parameters in a reliable way.
We note that an arbitrary number of shadow models can be produced by sampling from the hold-out data, allowing us to construct a validation set on which to evaluate various architectures for implementing the distance function, D , and meta model, M, comprising the general-wb attack.
Figure 6 shows an example of the validation accuracy obtained using various architectures for D and M, along with the corresponding test accuracy (unknown to the attacker).
We see that the test accuracy fairly closely follows the validation accuracy, with the maximum for both metrics occurring for the same architecture.
This suggests that the validation accuracy is a reasonably good indicator of the test accuracy making it a useful tool for hyper-parameter tuning.
This is perhaps not too surprising, as the attack model is highly regularized via its restrictive architecture.
The "omniscient" attack developed in Section 3.2 relies on measuring a difference between the parameters of the true expectationˆDexpectationˆ expectationˆD = D * ; that is, as the number of samples in the training set goes to infinity, the true and empirical distributions will converge, rendering even the optimal attack ineffective (0 advantage).
We would therefore expect that for a sufficiently large training set, the success of any MI attack would decline.
Conversely, we may expect the opportunity for better MI performance for smaller training sets.
Indeed, in accordance with this observation, we see that even the omniscient attack sees accuracy inversely proportional to the dataset size ( Figure 5).
We find that this pattern persists for real-world datasets as well.
Figure 7 shows the accuracy of our attacks on models trained on subsets of various sizes of the Adult dataset (the dataset containing the most records as compared to the number of parameters in the respective model).
We observe that as more data becomes available for training, the advantage of the attack diminishes, becoming quite small (< 4%) on the entire dataset (48,841 records).
This may suggest that the Adult dataset is sufficiently large to preclude any significant information leakage via a modestly-sized MLP model obtained through standard training.
Figure 8 shows the accuracy of our attacks on each of the datasets used in our evaluation, plotted against the size of the respective dataset.
We see to some extent the same downwards trend as dataset size increases, though there is more noise, and some of the image datasets (especially CIFAR10 and CIFAR100) provide notable exceptions.
This is likely due to the variation in the number of features, the network capacity, and the generalization error across datasets.
29th USENIX Security Symposium 1615 For deep models in particular, we want to be able to use information from each layer in our attack.
In Section 4.2, we describe a meta attack that combines the outputs of an individual attack on each layer.
Figure 9 shows the accuracy of the bayes-wb attack on each individual layer and of the meta attack on each LeNet target model.
In every instance, the meta attack is able to substantially outperform any individual attack, indicating that the information it receives from each layer is not entirely redundant.
Moreover, this suggests that information leakage occurs in the representations learned by layers throughout the model-that is, each layer plays some role in the leakage of information about the training data.
A possible consequence of this that we hypothesize in Section 6 is that models trained with transfer learning may leak less information about the training data used to tune the model.Remarkably, for MNIST, the advantage of the meta attack is greater than that of all the individual layers combined.
Finally, we compare our approach to previous work, namely, shadow-bb [38].
In particular, we compare (1) performance in terms of accuracy, precision, and recall; and (2) the reliability of the attack confidence when used to calibrate for higher precision.
In short, our results show that both bayes-wb and general-wb outperform shadow-bb, and can be more reliably calibrated to achieve confident inferences for the attacker.
Furthermore, even on some well-generalized models, on which shadow-bb and naive fare poorly, our attacks can be calibrated to make confident inferences, and sometimes also achieve non-trivial advantage.
Finally, we find that there is often little advantage to shadow-bb over naive, both because shadow-bb often performs comparably to naive, and because shadow-bb does not always produce calibrated confidence scores.Performance.
Figure 10 shows the accuracy, precision, and recall of naive, bayes-wb, general-wb, and shadow-bb.
The precision shown is before calibration attack (calibration results are shown in Figure 11).
We see that both bayes-wb and general-wb are consistently more accurate and precise than naive and shadow-bb.
At least one of bayes-wb or general-wb obtains the highest accuracy of the four methods on each target except Adult, and both outperform the other two methods in terms of precision in all cases.
In some cases, the improvement in accuracy of at least one of our attacks over prior work is by as much as seven percentage points, though in others our accuracy is only modestly better; however, in terms of precision, the difference is more pronounced in almost every case (typically greater by at least five percentage points).
Typically naive or shadow-bb achieve the highest recall, but we note that both methods do so with lower precision; and at least in the case of naive, this is merely a consequence of the fact that most of the models have a high training accuracy.Our results for the performance of shadow-bb are roughly in line with previously reported results for shadow-bb on the datasets which have been used for evaluation in prior work (Adult, MNIST, LFW, CIFAR10, and CIFAR100) [35,38].
On CIFAR10 and CIFAR100, our results are slightly lower than the results reported for shadow-bb by Shokri et al., however, our target models trained on CIFAR10 and CIFAR100 use dropout and have a lower generalization error than the models in the attacks reported by Shokri et al., which most likely accounts for this small discrepancy.Calibration.
As argued in Sections 1 and 2, one of the key desiderata of a membership inference attack is precision.
In order to calibrate an attack for precision, the confidence outputted by the attack must be informative.
Here, we examine the calibration of the confidence outputs of our attacks compared to shadow-bb (naive does not provide a confidence score with which to calibrate).
We find that increasing the decision threshold of the bayeswb and general-wb attacks has a positive effect on precision.
In particular, using the heuristic defined in Algorithm 3, we are able to consistently improve the precision of our attacks.
Figure 11 shows the precision of our attack as the decision threshold is raised according to Algorithm 3, for α = 0.90, and α = 0.99, compared to the uncalibrated attack.
In each case the precision increases, often by 10 or more percentage points.
Though in practice, an attacker would not be easily able to tune the calibration hyper-parameter, α, the consistency of the results in Figure 11 suggest that values of 0.90 and 0.99 serve as a practical "rule-of-thumb" for reliable calibration.On all convolutional models, general-wb is able to be calibrated to upwards of 75% precision.
Notably, this includes the model trained on MNIST, which has only 1.1% generalization error.
This implies that privacy violations are a threat even to well-generalized models, since our attack is able to confidently (with at least 75% confidence) identify a subset of training set members.On the MLP models, the calibration is slightly less consistent; however, here bayes-wb is able to obtain over 70% precision on the models trained on the Breast Cancer Wisconsin and Hepatitis datasets.In Figure 10, we see that the recall of the uncalibrated attack is frequently over 90%.
When calibrating, the recall drops as precision increases, however, we believe this does not diminish the threat of the attacks because a privacy violation occurs if even a few points are confidently inferred.While Figure 11 demonstrates that applying our calibration heuristic to bayes-wb and general-wb consistently increases the precision, we see that this is not always the case for shadowbb.
In some cases, the precision of shadow-bb is decreased by increasing the decision threshold.
In fact, occasionally, the average confidence on non-members is higher than that of members, leading to a precision slightly less than 50%.
This may be a result of the shadow model overfitting to the hold-out data.
When we are able to increase the precision of shadow-bb using its confidence output, the gains are less impressive, suggesting the probability outputs of shadow-bb are less well-calibrated.
Performance on Well-Generalized Models.
While some of the models we used to evaluate our attacks had a generalization error of 10% or more, we also evaluated on several datasets for which the learned model was far less overfit, including MNIST (1.1% generalization error), Adult (1.2%), Pima Diabetes (3.4%), and Breast Cancer Wisconsin (4.3%).
While on PD and BCW, our attacks only slightly outperform naive, on MNIST and Adult, our attacks do substantially better: on the model trained on Adult, general-wb achieves an advantage 2.6 times greater than the advantage achieved by naive.
Even more impressively, on MNIST, general-wb and bayes-wb achieves an advantage 3.5 and 12.5 times greater than the advantage achieved by naive, respectively.
On the other hand, shadow-bb fares poorly on all of these datasets except for Adult, typically achieving less than 2% advantage.
Finally, we note that the bayes-wb attack on the synthetic data model (Section 5.2) achieves a non-trivial 60% accuracy (20% advantage), despite the fact that the model has zero generalization error.In addition to the cases where our attacks achieve relatively high advantage against well-generalized models, we find that when calibrated, our attacks achieve as high as 75% precision on MNIST, and 70% precision on Breast Cancer Wisconsin, again underscoring the threat of privacy violations for well-generalized models.While it is clear that a greater degree of overfitting makes it easier for an adversary to mount any attack, the relative success of our attacks over naive on well-generalized models suggests that the white-box information is useful even when the model does not leak information through incorrect predictions on the test set.Similarity of shadow-bb and naive Results.
Figure 10 reveals that often, shadow-bb has performance comparable or even worse than naive, particularly on well-generalized target models.
This is likely a product of the attack model overfitting to idiosyncrasies in the shadow model's output that are unrelated to the target model.
On deep models with significant overfitting, shadow-bb performs slightly better than naive, however, we found that its behavior was not significantly different from that of naive; for example, on LFW, naive recovers 88% of the exact correct predictions made by shadow-bb.
This supports the intuition that the features used by the shadow model approach (i.e., the softmax outputs) are not fundamentally more well-suited to membership inference than those used by the naive method (i.e., the correctness of the predictions).
This is perhaps unsurprising, as the softmax outputs are likely to coincide largely with the correctness of the prediction-for correct predictions, the softmax will likely have high confidence on the correct class, regardless of whether the point was a member or not; and similarly for incorrect predictions, the softmax will likely have more entropy.
Concerns about privacy, underscored by concrete threats such as the attacks developed in this paper, have also motivated research to provide adequate defenses against such threats.
In this section we explore the ability of some of the commonly-proposed mitigation techniques to defend against our attack.
In particular, we focus on differential privacy [8] and regularization.
We find that, while both are useful to a degree, neither dropout nor ε-differentially private training with a large ε, are necessarily sufficient for mitigating the privacy risk posed by our attack.Differential Privacy.
Differential privacy (DP) [8] is often seen as the gold standard for private models, as models trained with differential privacy have provable guarantees against membership inference.
Namely, Yeom et al. [47] showed that, given an ε-differentially private learning algorithm, an adversary can achieve an advantage of at most e ε − 1.
Differential privacy has been applied to many areas of machine learning, including logistic regression [5], SVMs [34], and more recently, deep learning [1,37].
However, current methods for ensuring differential privacy are typically costly with respect to the accuracy of the model, particularly for small values of ε, which give a better privacy guarantee.
For this reason, in practice, ε is often chosen to be quite large; for example, in 2017, Apple was found to use an effective epsilon as high as 16 in some of its routines [43].
We used the Tensorflow Privacy library [29], an implementation of the moments accountant method [1], which guarantees (ε, δ)-differential privacy, to study the practical efficacy of our attack on protected models.
This method utilizes several hyperparameters from which ε is derived; for uniformity, we modified only the noise multiplier to achieve the desired ε, and used heuristics described in the original paper [1] to select the remaining hyperparameters.
While a different tuning of the hyperparameters may result in a different privacy-utility trade-off, the privacy guarantee depends only on ε and δ, not the hyperparameters directly.
In each case, δ was selected to be smaller than 1/N where N is the size of the dataset.
Figure 12 shows the effectiveness the general-wb attack against models trained with differential privacy for various values of ε on each dataset.
The train and test accuracies of the corresponding differentially-private target models are shown in Figure 13.
First, we note that as expected, when ε decreases the adversary's effectiveness quickly declines.
However, when ε is large (ε = 16), our attack occasionally performs essentially the same on the differentially-private model as on the undefended model.
For example, on BCW, PD, and LFW, 16-DP provided less defense than simple regularization, while harming the accuracy of the model.
Similarly, on Hep, 16-DP reduced the effectiveness of general-wb, but not below the effectiveness of shadow-bb on the corresponding undefended model.
These findings suggest that the practical benefits of large-ε-differential privacy cannot be taken for granted; in general, differential privacy may only be effective for sufficiently small ε.Nevertheless, it is clear that a practical adversary is unlikely to achieve performance that is tight with the theoretical bound.
For both the undefended model and the models trained with DP for ε > ln2 ≈ 0.69, the theoretical bound on the adversary's accuracy is 100%, which no attack was able to achieve.
On the other hand, for ε = 0.25, the theoretical maximum accuracy of the adversary is 64.2%.
In most such cases, our attack fared far poorer than this, coming closest on LFW, where our attack achieved 53.5% accuracy (25% of the theoretical maximum advantage) on the 0.25-DP model.
Thus, we conclude that because the accuracy of a real adversary is not likely to be tight with the worst-case guarantee, it is indeed pragmatic to select a somewhat large ε.
However, our evaluation shows that ε should not be chosen to be too large, or else the operative benefits of differential privacy may be lost.
Furthermore, the success of a given value of ε appears to vary across different datasets and models.
One must therefore be careful when making a practical selection for ε; to this end, we suggest that our attack may be useful in assessing which values of ε are appropriate for a given application.An apparent drawback of the examined method for obtaining differential privacy, revealed in our evaluation, is the steep cost in performance (Figure 13), which is particularly high for small ε.
Despite the fact that our attack became far less effective for small ε, this cost limits the practicality of the defense, highlighting the need for more research in this area.
The results we find here align with recent work [19], in which Jayaraman and Evans showed that the privacy leakage tends to increase as ε becomes large enough to avoid a significant loss in accuracy.
Indeed, only on the German Credit dataset did 16-DP provide a good defense while nearly maintaining the accuracy of the unprotected model.
In the other cases we evaluated, either our attack performed comparably on the DP and unprotected models, or the accuracy of the private model was significantly lower than that of the unprotected model.
Abadi et al. [1] mitigate the high cost in accuracy by first pretraining on public data, and then fine-tuning only the top layers with differential privacy on the private training set.
While this public transfer learning approach may not always be possible, it has two key benefits, the first being that the resulting model's performance is far less poor.
Second, only the final layers of such a model are trained on the private data, and thus our attack may only be able to effectively target those layers.
Our experiments in Section 5.4 show that our attack is far more effective when all layers are leveraged, and that the earlier layers often account for a sizable portion of the information leakage.
This suggests that, when possible, a transfer learning scheme like that of Abadi et al. could be a practical defense.Regularization.
Given the connection between membership inference and overfitting, regularization, such as dropout [41], which aims to reduce overfitting, has also been proposed to combat membership inference.
Generalization alone is not sufficient to protect against membership inference [47], and in fact, our empirical results (Section 5) show that we can successfully attack even models with negligible generalization error; however, dropout has been shown not only to reduce overfitting, but to strengthen privacy guarantees in neural networks [18].
Figure 12 shows the accuracy of our attack with and without dropout.
We find that dropout does not significantly impact the accuracy of our attack in most cases.
However, as opposed to DP, dropout is typically beneficial to the performance of the model, while providing a modest defense.
In this light, regularization (including dropout) may in fact be the more practical defensive measure, insofar as it improves test accuracy, because better generalization does appear to make membership more difficult, though clearly not impossible, for an attacker.Still, we warn that this may not be universally true of all forms of regularization, even regularization that improves generalization-as we have demonstrated, a model can still leak membership information through its parameters while making correct predictions on unseen points.Defenses in the Black-box Setting.
For membership inference in the black-box setting, Shokri et al. [38] also propose a number of other possible defenses, such as restricting the prediction vector to the top k classes, or increasing the entropy of the prediction vector via increasing the normalization temperature of the softmax.
However, these defenses are easily circumvented in the white-box setting, as the pre-modified outputs are still available to an attacker in this threat model.
Similarly, Salem et al. [35] propose a defense called model stacking, in which two models are trained separately on the training data and a third model makes predictions based on the outputs of the first two.
While Salem et al. found this to be an effective defense against black-box approaches, this defense is likewise circumvented in the white-box setting, as the initial two models are available to the attacker.
There is extensive prior literature on privacy attacks on statistical summaries.
Homer et al. [17] proposed what is considered the first membership inference attack on genomic data in 2008.
Following the work by Homer et al., a number of studies [9,14,36,39,44] have looked into membership attacks on statistics commonly published in genome-wide association studies.
In a similar vein, Komarova et al. [22] looked into partial disclosure scenarios, where an adversary is given fixed statistical estimates from combined public and private sources and attempts to infer the sensitive feature of an individual referenced in those sources.More recently, membership inference attacks have been applied to machine learning models.
Ateniese et al. [2] demonstrated that given access to the parameters of support vector machines (SVMs) or Hidden Markov Models (HMMs), an adversary can extract information about the training data.As deep learning has become more ubiquitous, membership inference attacks have been particularly directed at deep neural networks.
A number of different recent works [27,28,33,35,38,47] have taken different approaches to membership inference against deep networks in a standard supervised learning setting.
Additionally, Hayes et al. [15] have studied membership inference against generative adversarial networks (GANs); and others [16,30,33] have studied membership inference in the context of collaborative, or federated, learning.Black-box attacks.
We study membership inference as it applies to deep networks in classic supervised learning problems.
Most of the prior work in this area [27,28,35,38,47] has used the black-box threat model.
Yeom et al. [47] showed that generalization error necessarily leads to membership vulnerability; a natural consequence of this is that a simple "naive" attack (naive), which predicts a point is a member if and only if it was classified correctly, can be found to be quite effective on models that overfit to a large degree.
Other approaches have leveraged not only the predictions of the model, but the confidence outputs.
A particularly canonical approach, along these lines, is the attack introduced by Shokri et al. [38] (shadow-bb).
In this approach, a shadow model is trained on half of˜Sof˜ of˜S, ˜ S in , and an attack model is trained on the the outputs of the shadow model on its training data, ˜ S in (labeled 1), and the remaining datã S\˜SS\˜ S\˜S in (labeled 0).
Shadow models leverage the disparity in prediction confidences on training instances the target model has overfit to, and have been shown to be successful at membership inference on models that have sufficiently high generalization error.
A few other membership inference approaches [15,35] have made use of this same technique.Despite the fact that shadow model attacks leverage more information than the naive attack, we find in our evaluation (Section 5) that often, the shadow model attack fails to outperform the naive attack.
One potential reason for this finding is that the learned attack model used by this approach to distinguish between the shadow model's outputs on members and non-members may be itself subject to overfitting.
This may be especially true if the attack model picks up on behavior particular to one of the shadow models rather than the true target model.
Furthermore, the confidence and entropy of the target model's softmax output is likely to be closely related to whether the target model's prediction was correct or not, meaning that the softmax outputs may not provide substantially different information from that used by naive.White-box attacks.
In some settings, it may be realistic for an attacker to have white-box access to the target model.
Intuitively, while some information is leaked via the behavior of a model, the details of the structure and the parameters of the model are clear culprits for information leakage.
Few prior approaches have successfully leveraged this extra information.
While Hayes et al. [15] describe a white-box attack in their work on membership inference attacks applied to GANs, the attack uses access only the outputs of the discriminator portion of the GAN, rather than the learned weights of either the discriminator or the generator; thus their approach is not white-box in the same sense.
Meanwhile, Nasr et al. [33] demonstrated that a simple extension of the black-box shadow model approach to utilize internal activations does not result in higher membership inference accuracies than the original black-box approach.
This is perhaps unsurprising, as the internal units of the shadow models are not likely to have any relation to those of the target model (see Section 4).
Recently, Nasr et al. [33] provided a white-box attack that leverages the gradients of the target model's loss function with respect to its weights, which SGD approximately brings to zero on the training points at convergence.
In contrast to our work, Nasr et al. use a further relaxed threat model, in which the attacker has access to as much as half of the target model's training data.
We suggest an approach that is quite different from that of Nasr et al..
Our approach does not require this extra knowledge for the attacker, and thus falls under a more restrictive threat model, in which, to our knowledge, no other effective white-box attacks have been proposed.
Our work is the first to fully leverage white-box information to improve membership inference attacks against deep networks (in the standard threat model where the adversary is assumed not to have any examples of true training points).
In particular, our analysis sheds light on a fundamental mechanism of overfitting that can be leveraged by an adversary to compromise a model's privacy in a concrete way.
We use this analysis of how feature usage can lead to information leakage to construct a new white-box attack, which our evaluation demonstrates improves upon the previous state-of-the-art, particularly because it can be reliably calibrated for high precision, even on some well-generalized models.Subsequently, we used our attack to evaluate commonlyproposed privacy defenses.
Perhaps most interestingly, experiments utilizing our attack reveal a nuanced story regarding differential privacy.
When setting ε to small values, the attack was successfully mitigated but the utility of the resulting model quickly diminished; while when ε was increased sufficiently to mitigate the loss in utility, the attack sometimes achieved close to the same accuracy as on the undefended model.
This suggests that there is still considerable work to be done in developing effective defenses against privacy attacks-we anticipate that the insights gained from our approach will contribute to designing such defenses.
that conditioned on the class, y, the individual features, x j , are independent, we obtain Equation 12.
(11) = ¯ s log ∏ j N (x j | ˆ µ y j , ˆ σ 2 j ) N (x j | µ * y j ,σ * 2 j )(12)We then re-write the log of the product as a sum over the log, and observe that the sum can be written as a dot product as in Equation 13, which gives the parameters of the Bayes-optimal model for m y (x).
Finally, by assumption the variance is the same in S as in the general distribution, i.e., ˆ σ j = σ * j = σ j , for all features, j. Thus, v y from Equation 13 becomes zero, so we are left with a linear model for m y , with weights, w y , and bias, b y , given by Equation 5.
(12) = ¯ s ∑ j (x j −µ * y j ) 2 2σ * 2 j − (x j − ˆ µ y j ) 2 2 ˆ σ 2 j +log σ * j ˆ σ j = ¯ 1We begin with the expression for m y (x) and apply Bayes' rule to obtain Equation 10.
Next, we express Equation 10 as a logistic (or, sigmoid) function, ¯ s(x) := (1 + e x ) −1 .
We assume that Pr [T ] = 1 We begin with the expression for m y (x) and apply Bayes' rule to obtain Equation 10.
Next, we express Equation 10 as a logistic (or, sigmoid) function, ¯ s(x) := (1 + e x ) −1 .
We assume that Pr [T ] = 1
